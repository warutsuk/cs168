---
author: 'Tim Roughgarden & Gregory Valiant'
date: '2016-03-30'
generator: pandoc
title: |
    CS168: The Modern Algorithmic Toolbox Lecture \#2: Approximate Heavy
    Hitters and the Count-Min Sketch
---

<div id="header">

Tim Roughgarden & Gregory Valiant[^1^](#fn1){#fnref1 .footnoteRef} {#tim-roughgarden-gregory-valiant1 .author}
------------------------------------------------------------------

### March 30, 2016 {#march-30-2016 .date}

</div>

The Heavy Hitters Problem
=========================

Finding the Majority Element {#ss:majority}
----------------------------

Let’s begin with a problem that many of you have seen before. It’s a
common question in technical interviews. You’re given as input an array
[\\(A\\)]{.math .inline} of length [\\(n\\)]{.math .inline}, with the
promise that it has a [ *majority element*]{} — a value that is repeated
in strictly more than [\\(n/2\\)]{.math .inline} of the array’s entries.
Your task is to find the majority element.

In algorithm design, the usual “holy grail” is a linear-time algorithm.
For this problem, your post-CS161 toolbox already contains a subroutine
that gives a linear-time solution — just compute the median of
[\\(A\\)]{.math .inline}. (Note: it must be the majority element.) So
let’s be more ambitious: can we compute the majority element with a
single left-to-right pass through the array? If you haven’t seen it
before, here’s the solution:

-   Initialize counter := 0, current := NULL.

    \[current stores the frontrunner for the majority element\]

-   For [\\(i=1\\)]{.math .inline} to [\\(n\\)]{.math .inline}:

    -   If counter == 0:

        \[In this case, there is no frontrunner.\]

        -   current := A\[i\]

        -   counter++

    -   else if A\[i\]==current:

        \[In this case, our confidence in the current frontrunner goes
        up.\]

        -   counter++

    -   else

        \[In this case, our confidence in the current frontrunner goes
        down.\]

        -   counter- -

-   Return current

For example, suppose the input is the array [\\(\\{2,1,1\\}\\)]{.math
.inline}. The first iteration of the algorithm makes “2” the current
guess of the majority element, and sets the counter to 1. The next
element decreases the counter back to 0 (since [\\(1 \\neq 2\\)]{.math
.inline}). The final iteration resets the current guess to “1” (with
counter value 1), which is indeed the majority element.

More generally, the algorithm correctly computes the majority element of
any array that possesses one. We encourage you to formalize a proof of
this statement (e.g., by induction on [\\(n\\)]{.math .inline}). The
intuition is that each entry of [\\(A\\)]{.math .inline} that contains a
non-majority-value can only “cancel out” one copy of the majority value.
Since more than [\\(n/2\\)]{.math .inline} of the entries of
[\\(A\\)]{.math .inline} contain the majority value, there is guaranteed
to be a copy of it left standing at the end of the algorithm.

But so what? It’s a cute algorithm, but isn’t this just a toy problem?
It is, but modest generalizations of the problem are quite close to
problems that people really want to solve in practice.

The Heavy Hitters Problem {#ss:hh}
-------------------------

In the [*heavy hitters*]{} problem, the input is an array
[\\(A\\)]{.math .inline} of length [\\(n\\)]{.math .inline}, and also a
parameter [\\(k\\)]{.math .inline}. You should think of [\\(n\\)]{.math
.inline} as very large (in the hundreds of millions, or billions), and
[\\(k\\)]{.math .inline} as modest (10, 100, or 1000). The goal is to
compute the values that occur in the array at least [\\(n/k\\)]{.math
.inline} times.[^2^](#fn2){#fnref2 .footnoteRef} Note that there can be
at most [\\(k\\)]{.math .inline} such values; and there might be none.
The problem of computing the majority element corresponds to the heavy
hitters problem with [\\(k \\approx 2-\\delta\\)]{.math .inline} for a
small value [\\(\\delta &gt; 0\\)]{.math .inline}, and with the
additional promise that a majority element exists.

The heavy hitters problem has lots of applications, as you can imagine.
We’ll be more specific later when we discuss a concrete solution, but
here are some high-level examples:[^3^](#fn3){#fnref3 .footnoteRef}

1.  Computing popular products. For example, [\\(A\\)]{.math .inline}
    could be all of the page views of products on yesterday. The heavy
    hitters are then the most frequently viewed products.

2.  Computing frequent search queries. For example, [\\(A\\)]{.math
    .inline} could be all of the searches on Google yesterday. The heavy
    hitters are then searches made most often.

3.  Identifying heavy TCP flows. Here, [\\(A\\)]{.math .inline} is a
    list of data packets passing through a network switch, each
    annotated with a source-destination pair of IP addresses. The heavy
    hitters are then the flows that are sending the most traffic. This
    is useful for, among other things, identifying denial-of-service
    attacks.

4.  Identifying volatile stocks. Here, [\\(A\\)]{.math .inline} is a
    list of stock trades.

It’s easy to think of more. Clearly, it would be nice to have a good
algorithm for the heavy hitters problem at your disposal for data
analysis.

The problem is easy to solve efficiently if [\\(A\\)]{.math .inline} is
readily available in main memory — just sort the array and do a linear
scan over the result, outputting a value if and only if it occurs
(consecutively) at least [\\(n/k\\)]{.math .inline} times. After being
spoiled by our slick solution for finding a majority element, we
naturally want to do better. Can we solve the heavy hitters problem with
a single pass over the array? This question isn’t posed quite correctly,
since it allows us to cheat: we could make a single pass over the array,
make a local copy of it in our working memory, and then apply the
sorting-based solution to our local copy. Thus what we mean is: can we
solve the Heavy Hitters problem with a single pass over the array, using
only a small amount of auxiliary space?[^4^](#fn4){#fnref4 .footnoteRef}

An Impossibility Result
-----------------------

The following fact might surprise you.

\[fact:hh\] There is [*no*]{} algorithm that solves the Heavy Hitters
problems in one pass while using a sublinear amount of auxiliary space.

We next explain the intuition behind Fact \[fact:hh\]. We encourage you
to devise a formal proof, which follows the same lines as the intuition.

Set [\\(k = n/2\\)]{.math .inline}, so that our responsibility is to
output any values that occur at least twice in the input array
[\\(A\\)]{.math .inline}.[^5^](#fn5){#fnref5 .footnoteRef} Suppose
[\\(A\\)]{.math .inline} has the form
[\\\[\\underbrace{|x\_1|x\_2|x\_3|\\cdots|x\_{n-1}|}\_{\\text{set \$S\$
of distinct elements}}y|,\\\]]{.math .display} where
[\\(x\_1,\\ldots,x\_{n-1}\\)]{.math .inline} are an arbitrary set
[\\(S\\)]{.math .inline} of distinct elements (in
[\\(\\{1,2,\\ldots,n\^2\\}\\)]{.math .inline}, say) and the final entry
[\\(y\\)]{.math .inline} may or may not be in [\\(S\\)]{.math .inline}.
By definition, we need to output [\\(y\\)]{.math .inline} if and only if
[\\(y \\in S\\)]{.math .inline}. That is, [*answering membership queries
reduces to solving the Heavy Hitters problem.*]{} By the “membership
problem,” we mean the task of preprocessing a set [\\(S\\)]{.math
.inline} to answer queries of the form “is [\\(y \\in S\\)]{.math
.inline}”? (A hash table is the most common solution to this problem.)
It is intuitive that you cannot correctly answer all membership queries
for a set [\\(S\\)]{.math .inline} without storing [\\(S\\)]{.math
.inline} (thereby using linear, rather than constant, space) — if you
throw some of [\\(S\\)]{.math .inline} out, you might get a query asking
about the part you threw out, and you won’t know the answer. It’s not
too hard to make this idea precise using the Pigeonhole
Principle.[^6^](#fn6){#fnref6 .footnoteRef}

The Approximate Heavy Hitters Problem {#ss:epshh}
-------------------------------------

What should we make of Fact \[fact:hh\]? Should we go home with our tail
between our legs? Of course not — the applications that motivate the
heavy hitters problem are not going away, and we still want to come up
with non-trivial algorithms for them. In light of Fact \[fact:hh\], the
best-case scenario would be to find a relaxation of the problem that
remains relevant for the motivating applications and also admits a good
solution.[^7^](#fn7){#fnref7 .footnoteRef}

In the [*[\\(\\eps\\)]{.math .inline}-approximate heavy hitters
([\\(\\eps\\)]{.math .inline}-HH) problem*]{}, the input is an array
[\\(A\\)]{.math .inline} of length [\\(n\\)]{.math .inline} and
user-defined parameters [\\(k\\)]{.math .inline} and [\\(\\eps\\)]{.math
.inline}. The responsibility of an algorithm is to output a list of
values such that:

1.  Every value that occurs at least [\\(\\frac{n}{k}\\)]{.math .inline}
    times in [\\(A\\)]{.math .inline} is in the list.

2.  Every value in the list occurs at least [\\(\\frac{n}{k} -\\eps
    n\\)]{.math .inline} times in [\\(A\\)]{.math .inline}.

What prevents us from taking [\\(\\eps = 0\\)]{.math .inline} and
solving the exact version of the problem? We allow the space used by a
solution to grow as [\\(\\tfrac{1}{\\eps}\\)]{.math .inline}, so as
[\\(\\eps \\downarrow 0\\)]{.math .inline} the space blows up (as is
necessary, by Fact \[fact:hh\]).

For example, suppose we take [\\(\\eps = \\tfrac{1}{2k}\\)]{.math
.inline}. Then, the algorithm outputs every value with frequency count
at least [\\(\\tfrac{n}{k}\\)]{.math .inline}, and only values with
frequency count at least [\\(\\tfrac{n}{2k}\\)]{.math .inline}. Thinking
back to the motivating examples in Section \[ss:hh\], such an
approximate solution is essentially as useful as an exact solution.
Space usage [\\(O(\\tfrac{1}{\\eps}) = O(k)\\)]{.math .inline} is also
totally palatable; after all, the output of the heavy hitters or
[\\(\\eps\\)]{.math .inline}-HH problem already might be as large as
[\\(k\\)]{.math .inline} elements.

The Count-Min Sketch
====================

Discussion
----------

This section presents an elegant small-space data structure, the [
*count-min sketch*]{} []{.citation}, that can be used to solve the
[\\(\\eps\\)]{.math .inline}-HH problem. There are also several other
good solutions to the problem, including some natural “counter-based”
algorithms that extend the algorithm in Section \[ss:majority\] for
computing a majority element []{.citation}. We focus on the count-min
sketch for a number of reasons.

1.  It has been implemented in real systems. For example, AT[&]{}T has
    used it in network switches to perform analyses on network traffic
    using limited memory []{.citation}.[^8^](#fn8){#fnref8 .footnoteRef}
    At Google, a precursor of the count-min sketch (called the “count
    sketch” []{.citation}) has been implemented on top of their
    MapReduce parallel processing infrastructure []{.citation}. One of
    the original motivations for this primitive was log analysis (e.g.,
    of source code check-ins), but presumably it is now used for lots of
    different analyses.

2.  The data structure is based on hashing, and as such fits in well
    with the current course theme.

3.  The data structure introduces a new theme, present in many of the
    next few lectures, of “lossy compression.” The goal here is to throw
    out as much of your data as possible while still being able to make
    accurate inferences about it. What you want to keep depends on the
    type of inference you want to support. For approximately preserving
    frequency counts, the count-min sketch shows that you can throw out
    almost all of your data!

We’ll only discuss how to use the count-min sketch to solve the
approximate heavy hitters problem, but it is also useful for other
related tasks (see []{.citation} for a start). Another reason for its
current popularity is that its computations parallelize easily — as we
discuss its implementation, you might want to think about this point.

A Role Model: The Bloom Filter
------------------------------

This section briefly reviews the bloom filter data structure, which is a
role model for the count-min sketch. No worries if you haven’t seen
bloom filters before; our treatment of the count-min sketch below is
self-contained. There are also review videos covering the details of
bloom filters on the course Web site.

The raison d’être of a bloom filter is to solve the [*membership
problem*]{}. The client can insert elements into the bloom filter and
the data structure is responsible for remembering what’s been inserted.
The bloom filter doesn’t do much, but what it does it does very well.

Hash tables also offer a good solution to the membership problem, so why
bother with a bloom filter? The primary motivation is to save space — a
bloom filter compresses the stored set more than a hash table. In fact,
the compression is so extreme that a bloom filter cannot possibly answer
all membership queries correctly. That’s right, it’s a [*data structure
that makes errors.*]{} Its errors are “one-sided,” with no false
negatives (so if you inserted an element, the bloom filter will always
confirm it) but with some false positives (so there are “phantom
elements” that the data structure claims are present, even though they
were never inserted). For instance, using 8 bits per stored element —
well less than the space required for a pointer, for example — bloom
filters can achieve a false positive probability less than 2%. More
generally, bloom filters offer a smooth trade-off between the space used
and the false positive probability. Both the insertion and lookup
operations are super-fast ([\\(O(1)\\)]{.math .inline} time) in a bloom
filter, and what little work there is can also be parallelized easily.

Bloom filters were invented in 1970 []{.citation}, back when space was
at a premium for everything, even spellcheckers.[^9^](#fn9){#fnref9
.footnoteRef} This century, bloom filters have gone viral in the
computer networking community []{.citation}. Saving space is still a big
win in many networking applications, for example by making better use of
the scarce main memory at a router or by reducing the amount of
communication required to implement a network protocol.

Bloom filters serve as a role model for the count-min sketch in two
senses. First, bloom filters offer a proof of concept that sacrificing a
little correctness can yield significant space savings. Note this is
exactly the trade-off we’re after: Fact \[fact:hh\] states that exactly
solving the heavy hitters problem requires linear space, and we’re
hoping that by relaxing correctness — i.e., solving the
[\\(\\eps\\)]{.math .inline}-HH problem instead — we can use far less
space. Second, at a technical level, if you remember how bloom filters
are implemented, you’ll recognize the count-min sketch implementation as
a bird of the same feather.

Count-Min Sketch: Implementation
--------------------------------

The count-min-sketch supports two operations: ([\\(x\\)]{.math .inline})
and ([\\(x\\)]{.math .inline}).[^10^](#fn10){#fnref10 .footnoteRef} The
operation ([\\(x\\)]{.math .inline}) is supposed to return the
[*frequency count*]{} of [\\(x\\)]{.math .inline}, meaning the number of
times that ([\\(x\\)]{.math .inline}) has been invoked in the past.

The count-min sketch has two parameters, the number of
buckets [\\(b\\)]{.math .inline} and the number of hash functions
[\\(\\ell\\)]{.math .inline}. We’ll figure out how to choose these
parameters in Section \[ss:error\], but for now you might want to think
of [\\(b\\)]{.math .inline} as in the thousands and of
[\\(\\ell\\)]{.math .inline} as 5.[^11^](#fn11){#fnref11 .footnoteRef}
The point of [\\(b\\)]{.math .inline} is to compress the array
[\\(A\\)]{.math .inline} (since [\\(b \\ll n\\)]{.math .inline}). This
compression leads to errors. The point of [\\(\\ell\\)]{.math .inline}
is to implement a few “independent trials,” which allows us to reduce
the error. What’s important, and kind of amazing, is that these
parameters are [*independent*]{} of the length [\\(n\\)]{.math .inline}
of the array that we are processing (recall [\\(n\\)]{.math .inline}
might be in the billions, or even larger).

<div class="figure">

Running ([\\(x\\)]{.math .inline}) on the CMS data structure. Each row
corresponds to a hash function [\\(h\_i\\)]{.math
.inline}.[]{data-label="f:cms"}

</div>

The data structure is just a [\\(\\ell \\times b\\)]{.math .inline} 2-D
array CMS of counters (initially all 0). See Figure \[f:cms\]. After
choosing [\\(\\ell\\)]{.math .inline} hash functions
[\\(h\_1,\\ldots,h\_{\\ell}\\)]{.math .inline}, each mapping the
universe of objects to [\\(\\{1,2,\\ldots,b\\}\\)]{.math .inline}, the
code for ([\\(x\\)]{.math .inline}) is simply:

-   for [\\(i=1,2,\\ldots,\\ell\\)]{.math .inline}:

    -   increment CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math
        .inline}\]

Assuming that every hash function can be evaluated in constant time, the
running time of the operation is clearly [\\(O(\\ell)\\)]{.math
.inline}.

To motivate the implementation of ([\\(x\\)]{.math .inline}), fix a
row [\\(i \\in \\{1,2,\\ldots,\\ell\\}\\)]{.math .inline}. Every time
([\\(x\\)]{.math .inline}) is called, the same counter
CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math .inline}\] in
this row gets incremented. Since counters are never decremented, we
certainly have [\\\[\\label{eq:inc} \\text{CMS\[\$i\$\]\[\$h\_i(x)\$\]}
\\ge f\_x,\\\]]{.math .display} where [\\(f\_x\\)]{.math .inline}
denotes the frequency count of object [\\(x\\)]{.math .inline}. If we’re
lucky, then equality holds in . In general, however, there will be
[*collisions*]{}: objects [\\(y \\neq x\\)]{.math .inline} with
[\\(h\_i(y) = h\_i(x)\\)]{.math .inline}. (Note with [\\(b \\ll
n\\)]{.math .inline}, there will be lots of collisions.) Whenever
([\\(y\\)]{.math .inline}) is called for an object [\\(y\\)]{.math
.inline} that collides with [\\(x\\)]{.math .inline} in
row [\\(i\\)]{.math .inline}, this will also increment the same counter
CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math .inline}\]. So
while CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math .inline}\]
cannot underestimate [\\(f\_x\\)]{.math .inline}, it generally
overestimates [\\(f\_x\\)]{.math .inline}.

The [\\(\\ell\\)]{.math .inline} rows of the count-min sketch give
[\\(\\ell\\)]{.math .inline} different estimates of [\\(f\_x\\)]{.math
.inline}. How should we aggregate these estimates? Later in the course,
we’ll see scenarios where using the mean or the median is a good way to
aggregate. Here, our estimates suffer only one-sided error — all of them
can only be bigger than the number [\\(f\_x\\)]{.math .inline} we want
to estimate, and so it’s a no-brainer which estimate we should pay
attention to. The [*smallest*]{} of the estimates is clearly the best
estimate. Thus, the code for ([\\(x\\)]{.math .inline}) is simply:

-   return [\\(\\min\_{i=1}\^{\\ell}
    \\text{CMS\[\$i\$\]\[\$h\_i(x)\$\]}\\)]{.math .inline}

The running time is again [\\(O(\\ell)\\)]{.math .inline}. By , the data
structure has one-sided error — it only returns overestimates of true
frequency counts, never underestimates. The key question is obviously:
[*how large are typical overestimates?*]{} The answer depends on how we
set the parameters [\\(b\\)]{.math .inline} and [\\(\\ell\\)]{.math
.inline}. As [\\(b\\)]{.math .inline} gets bigger, we’ll have fewer
collisions and hence less error. As [\\(\\ell\\)]{.math .inline} gets
bigger, we’ll take the minimum over more independent estimates,
resulting in tighter estimates. Thus the question is whether or not
modest values of [\\(b\\)]{.math .inline} and [\\(\\ell\\)]{.math
.inline} are sufficient to guarantee that the overestimates are small.
This is a quantitative question that can only be answered with
mathematical analysis; we do this in the next section (and the answer is
yes!).

The implementation details of the count-min sketch are very similar to
those of a bloom filter. The latter structure only uses bits, rather
than integer-valued counters. When an object is inserted into a bloom
filter, [\\(\\ell\\)]{.math .inline} hash functions indicate
[\\(\\ell\\)]{.math .inline} bits that should be set to 1 (whether or
not they were previously 0 or  1). The count-min sketch, which is
responsible for keeping counts rather than just tracking membership,
instead increments [\\(\\ell\\)]{.math .inline} counters. Looking up an
object in a bloom filter just involves checking the [\\(\\ell\\)]{.math
.inline} bits corresponding to that object — if any of them are still 0,
then the object has not been previously inserted. Thus Lookup in a bloom
filter can be thought of as taking the minimum of [\\(\\ell\\)]{.math
.inline} bits, which exactly parallels the  operation of a
count-min-sketch. That the count-min sketch only overestimates frequency
counts corresponds to the bloom filter’s property that it only suffers
from false positives.

Count-Min Sketch: Heuristic Error Analysis {#ss:error_heur}
------------------------------------------

The goal of this section is to analyze how much a count-min sketch
overestimates frequency counts, as a function of the parameters
[\\(b\\)]{.math .inline} and [\\(\\ell\\)]{.math .inline}. Once we
understand the relationship between the error and these parameters, we
can set the parameters to guarantee simultaneously small space and low
error.

Fix an object [\\(x\\)]{.math .inline}. Let’s first think about a single
row [\\(i\\)]{.math .inline} of the count-min sketch; we’ll worry about
taking the minimum over rows later. After a bunch of ([\\(x\\)]{.math
.inline}) operations have been executed, what’s the final value of
CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math .inline}\], row
[\\(i\\)]{.math .inline}’s estimate for the frequency count
of [\\(x\\)]{.math .inline}?

If we’re lucky and no other objects collide with [\\(x\\)]{.math
.inline} in the [\\(i\\)]{.math .inline}th row, then
CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math .inline}\] is
just the true frequency count [\\(f\_x\\)]{.math .inline} of
[\\(x\\)]{.math .inline}. If we’re unlucky and some object
[\\(y\\)]{.math .inline} collides with [\\(x\\)]{.math .inline} in the
[\\(i\\)]{.math .inline}th row, then [\\(y\\)]{.math .inline}
contributes its own frequency count [\\(f\_y\\)]{.math .inline} to
CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math .inline}\]. More
generally, CMS\[[\\(i\\)]{.math .inline}\]\[[\\(h\_i(x)\\)]{.math
.inline}\] is the sum of the contributions to this counter by
[\\(x\\)]{.math .inline} and all other objects that collide with it:
[\\\[\\label{eq:z\_heur} \\text{CMS\[\$i\$\]\[\$h\_i(x)\$\]} = f\_x +
\\sum\_{y \\in S} f\_y,\\\]]{.math .display} where [\\(S = \\{ y \\neq x
\\,:\\, h\_i(y) = h\_i(x) \\}\\)]{.math .inline} denotes the objects
that collide with [\\(x\\)]{.math .inline} in the [\\(i\\)]{.math
.inline}th row. In , [\\(f\_x\\)]{.math .inline} and the
[\\(f\_y\\)]{.math .inline}’s are fixed constants (independent of the
choice of [\\(h\_i\\)]{.math .inline}), while the set [\\(S\\)]{.math
.inline} will be different for different choices of the hash function
[\\(h\_i\\)]{.math .inline}.

Recall that a good hash function spreads out a data set as well as if it
were a random function. With [\\(b\\)]{.math .inline} buckets and a good
hash function [\\(h\_i\\)]{.math .inline}, we expect [\\(x\\)]{.math
.inline} to collide with a roughly [\\(1/b\\)]{.math .inline} fraction
of the other elements [\\(y \\neq x\\)]{.math .inline} under
[\\(h\_i\\)]{.math .inline}. Thus we expect [\\\[\\label{eq:error\_heur}
\\text{CMS\[\$i\$\]\[\$h\_i(x)\$\]} = f\_x + \\frac{1}{b} \\sum\_{y
\\neq x} f\_y \\le f\_x + \\frac{n}{b},\\\]]{.math .display} where in
the inequality we use that the sum of the frequency counts is exactly
the total number [\\(n\\)]{.math .inline} of increments (each increment
adds 1 to exactly one frequency count). See also Section \[ss:error\]
for a formal (non-heuristic) derivation of .

We should be pleased with . Recall the definition of the
[\\(\\eps\\)]{.math .inline}-approximate heavy hitters problem
(Section \[ss:epshh\]): the goal is to identify objects with frequency
count at least [\\(\\tfrac{n}{k}\\)]{.math .inline}, without being
fooled by any objects with frequency count less than [\\(\\tfrac{n}{k}
-\\eps n\\)]{.math .inline}. This means we just need to estimate the
frequency count of an object up to additive one-sided error [\\(\\eps
n\\)]{.math .inline}. If we take the number of buckets [\\(b\\)]{.math
.inline} in the count-min sketch to be equal to
[\\(\\tfrac{1}{\\eps}\\)]{.math .inline}, then  says the expected
overestimate of a given object is at most [\\(\\eps n\\)]{.math
.inline}. Note that the value of [\\(b\\)]{.math .inline}, and hence the
number of counters used by the data structure, is completely independent
of [\\(n\\)]{.math .inline}! If you think of [\\(\\eps = .001\\)]{.math
.inline} and [\\(n\\)]{.math .inline} as in the billions, then this is
pretty great.

So why aren’t we done? We’d like to say that, in addition to the
expected overestimate of a frequency count being small, with very large
probability the overestimate of a frequency count is small. (For a role
model, recall that typical bloom filters guarantee a false positive
probability of 1-2%.) This requires translating our bound on an
expectation to a bound on a probability.

Next, we observe that  implies that the probability that a row’s
overestimate of [\\(x\\)]{.math .inline} is more than
[\\(\\tfrac{2n}{b}\\)]{.math .inline} is less than 50%. (If not, the
expected overestimate would be greater than [\\(\\tfrac{1}{2} \\cdot
\\tfrac{2n}{b} = \\tfrac{n}{b}\\)]{.math .inline}, contradicting .) This
argument is a special case of “Markov’s inequality;” see
Section \[ss:error\] for details.

A possibly confusing point in this heuristic analysis is: in the
observation above, what is the probability over, exactly? I.e., where is
the randomness? There are two morally equivalent interpretations of the
analysis in this section. The first, which is carried out formally and
in detail in Section \[ss:error\], is to assume that the hash function
[\\(h\_i\\)]{.math .inline} is chosen uniformly at random from a
universal family of hash functions (see CS161 for the definition). The
second is to assume that the hash function [\\(h\_i\\)]{.math .inline}
is fixed and that the data is random. If [\\(h\_i\\)]{.math .inline} is
a well-crafted hash function, then your particular data set will almost
always behave like random data.[^12^](#fn12){#fnref12 .footnoteRef}

Remember that everything we’ve done so far is just for a single
row [\\(i\\)]{.math .inline} of the hash table. The output of
([\\(x\\)]{.math .inline}) exceeds [\\(f\_x\\)]{.math .inline} by more
than [\\(\\eps n\\)]{.math .inline} only if [*every*]{} row’s estimate
is too big. Assuming that the hash functions [\\(h\_i\\)]{.math .inline}
are independent,[^13^](#fn13){#fnref13 .footnoteRef} we have
[\\\[{\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\min\_{i=1}\^{\\ell}
\\text{CMS\[\$i\$\]\[\$h\_i(x)\$\]} &gt; f\_x + \\frac{2n}{b}\\right\]}
= \\prod\_{i=1}\^{\\ell} {\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\text{CMS\[\$i\$\]\[\$h\_i(x)\$\]}
&gt; f\_x + \\frac{2n}{b}\\right\]} \\le \\left( \\frac{1}{2}
\\right)\^{\\ell}.\\\]]{.math .display}

To get an overestimate threshold of [\\(\\eps n\\)]{.math .inline}, we
can set [\\(b=\\tfrac{2}{\\eps}\\)]{.math .inline} (so e.g., 200 when
[\\(\\eps = .01\\)]{.math .inline}). To drive the error probability —
that is, the probability of an overestimate larger than this threshold —
down to the user-specified value [\\(\\delta\\)]{.math .inline}, we set
[\\\[\\left( \\frac{1}{2} \\right)\^{\\ell} = \\delta\\\]]{.math
.display} and solve to obtain [\\(\\ell = \\log\_2
\\tfrac{1}{\\delta}\\)]{.math .inline}. (This is between 6 and 7 when
[\\(\\delta = .01\\)]{.math .inline}.) Thus the total number of counters
required when [\\(\\delta = \\eps = .01\\)]{.math .inline} is barely
over a thousand (no matter how long the array is!). See
Section \[ss:reportcard\] for a detailed recap of all of the count-min
sketch’s properties, and Section \[ss:error\] for a rigorous and
optimized version of the heuristic analysis in this section.

Count-Min Sketch: Rigorous Error Analysis {#ss:error}
-----------------------------------------

This section carries out a rigorous version of the heuristic error
analysis in Section \[ss:error\_heur\]. Let [\\(f\_x\\)]{.math .inline}
denote the true frequency count of [\\(x\\)]{.math .inline}, and
[\\(Z\_i\\)]{.math .inline} the (over)estimate CMS\[[\\(i\\)]{.math
.inline}\]\[[\\(h\_i(x)\\)]{.math .inline}\]. [\\(Z\_i\\)]{.math
.inline} is a random variable over the state space equal to the set of
all possible hash functions [\\(h\_i\\)]{.math .inline}. (I.e., given an
[\\(h\_i\\)]{.math .inline}, [\\(Z\_i\\)]{.math .inline} is fully
determined.)

If we’re lucky and no other objects collide with [\\(x\\)]{.math
.inline} in the [\\(i\\)]{.math .inline}th row, then [\\(Z\_i =
f\_x\\)]{.math .inline}. If we’re unlucky and some object
[\\(y\\)]{.math .inline} collides with [\\(x\\)]{.math .inline} in the
[\\(i\\)]{.math .inline}th row, then [\\(y\\)]{.math .inline}
contributes its own frequency count [\\(f\_y\\)]{.math .inline} to
[\\(Z\_i\\)]{.math .inline}. As in , we can write [\\\[\\label{eq:z}
Z\_i = f\_x + \\sum\_{y \\in S} f\_y,\\\]]{.math .display} where [\\(S =
\\{ y \\neq x \\,:\\, h\_i(y) = h\_i(x) \\}\\)]{.math .inline} denotes
the objects that collide with [\\(x\\)]{.math .inline} in the
[\\(i\\)]{.math .inline}th row. In , [\\(f\_x\\)]{.math .inline} and the
[\\(f\_y\\)]{.math .inline}’s are fixed constants (independent of the
choice of [\\(h\_i\\)]{.math .inline}), while the set [\\(S\\)]{.math
.inline} is random (i.e., different for different choices of
[\\(h\_i\\)]{.math .inline}).

To continue the error analysis, we make the following assumption:

-   For every pair [\\(x,y\\)]{.math .inline} of distinct objects,
    [\\({\\text{\\bf
    Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[h\_i(y) =
    h\_i(x)\\right\]} \\le \\tfrac{1}{b}\\)]{.math .inline}.

Assumption (\*) basically says that, after conditioning on the bucket to
which [\\(h\_i\\)]{.math .inline} assigns an object [\\(x\\)]{.math
.inline}, the bucket [\\(h\_i\\)]{.math .inline} assigns to some other
object [\\(y\\)]{.math .inline} is uniformly random. For example, the
assumption would certainly be satisfied if [\\(h\_i\\)]{.math .inline}
is a completely random function. It is also satisfied if
[\\(h\_i\\)]{.math .inline} is chosen uniformly at random from a
universal family — it is precisely the definition of such a family (see
your CS161 notes).

Before using assumption (\*) to analyze , we recall [ *linearity of
expectation*]{}: for any real-valued random variables
[\\(X\_1,\\ldots,X\_m\\)]{.math .inline} defined on the same probability
space, [\\\[\\label{eq:linexp} {\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\sum\_{j=1}\^m
X\_j\\right\]} = \\sum\_{j=1}\^m {\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X\_j\\right\]}.\\\]]{.math
.display} That is, the expectation of a sum is just the sum of the
expectations, [*even if the random variables are not
independent*]{}.[^14^](#fn14){#fnref14 .footnoteRef} The statement is
trivial to prove — just expand the expectations and reverse the order of
summation — and insanely useful.

To put the pieces together, we first rewrite  as [\\\[\\label{eq:z2}
Z\_i = f\_x + \\sum\_{y \\neq x} f\_y\\one\_y,\\\]]{.math .display}
where [\\(\\one\_y\\)]{.math .inline} is the indicator random variable
that indicates whether or not [\\(y\\)]{.math .inline} collides with
[\\(x\\)]{.math .inline} under [\\(h\_i\\)]{.math .inline}:
[\\\[\\one\_y = \\left\\{ \\begin{array}{cl} 1 & \\text{if \$h\_i(y) =
h\_i(x)\$}\\\\ 0 & \\text{otherwise.} \\end{array}\\right.\\\]]{.math
.display} Recalling that [\\(f\_x\\)]{.math .inline} and the
[\\(f\_y\\)]{.math .inline}’s are constants, we can apply linearity of
expectation to  to obtain [\\\[\\label{eq:z3} {\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[Z\_i\\right\]} = f\_x
+ \\sum\_{y \\neq x} f\_y \\cdot {\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\one\_y\\right\]}.\\\]]{.math
.display} As indicator random variables, the [\\(\\one\_y\\)]{.math
.inline}’s have very simple expectations: [\\\[\\label{eq:error}
{\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\one\_y\\right\]} =
1 \\cdot \\underbrace{{\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\one\_y =
1\\right\]}}\_{={\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[h\_i(y)=h\_i(x)\\right\]}}
+ \\underbrace{0 \\cdot {\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\one\_y =
0\\right\]}}\_{=0} = {\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[h\_i(y)=h\_i(x)\\right\]}
\\stackrel{(\*)}{\\le} \\frac{1}{b}.\\\]]{.math .display} Combining 
and  gives [\\\[\\label{eq:error2} {\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[Z\_i\\right\]} \\le
f\_x + \\frac{1}{b} \\sum\_{y \\neq x} f\_y \\le f\_x +
\\frac{n}{b}.\\\]]{.math .display}

Next we translate this bound on an expectation to a bound on a
probability. A simple and standard way to do this is via [*Markov’s
inequality.*]{}

\[prop:markov\] If [\\(X\\)]{.math .inline} is a nonnegative random
variable and [\\(c &gt; 1\\)]{.math .inline} is a constant, then
[\\\[{\\text{\\bf Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X
&gt; c \\cdot {\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X\\right\]}\\right\]}
\\le \\frac{1}{c}.\\\]]{.math .display}

The proof of Markov’s inequality is simple. For example, suppose you
have a nonnegative random variable [\\(X\\)]{.math .inline} with
expected value 10. How frequently could it take on a value greater
than 100? (So [\\(c=10\\)]{.math .inline}.) In principle, it is possible
that [\\(X\\)]{.math .inline} has value exactly 100 10% of the time (if
it has value 0 the rest of the time). But it can’t have value strictly
greater than 100 10% or more of the time — if it did, its expectation
would be strictly greater than 10. An analogous argument applies to
nonnegative random variables with any expectation and for any value of
[\\(c\\)]{.math .inline}.

Let’s return to our error analysis, for a fixed object [\\(x\\)]{.math
.inline} and row [\\(i\\)]{.math .inline}. Define [\\\[X = Z\_i - f\_x
\\ge 0\\\]]{.math .display} as the amount by which the [\\(i\\)]{.math
.inline}th row of the count-min sketch overestimates [\\(x\\)]{.math
.inline}’s frequency count [\\(f\_x\\)]{.math .inline}. By , with [\\(b
= \\tfrac{e}{\\eps}\\)]{.math .inline}, the expected value of
[\\(X\\)]{.math .inline} is at most [\\(\\tfrac{\\eps n}{e}\\)]{.math
.inline}.[^15^](#fn15){#fnref15 .footnoteRef} Since this overestimate is
always nonnegative, we can apply Markov’s inequality
(Proposition \[prop:markov\]) with [\\({\\text{\\bf
E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X\\right\]} =
\\tfrac{\\eps n}{e}\\)]{.math .inline} and [\\(c = e\\)]{.math .inline}
to obtain [\\\[{\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X &gt; e \\cdot
\\tfrac{\\eps n}{e}\\right\]} \\le \\frac{1}{e}\\\]]{.math .display} and
hence [\\\[{\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[Z\_i &gt; f\_x +
\\eps n\\right\]} \\le \\frac{1}{e}.\\\]]{.math .display}

Assuming that the hash functions are chosen independently, we have
[\\\[\\label{eq:error3} {\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[\\min\_{i=1}\^{\\ell}
Z\_i &gt; f\_x + \\eps n\\right\]} = \\prod\_{i=1}\^{\\ell} {\\text{\\bf
Pr}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[Z\_i &gt; f\_x +
\\eps n\\right\]} \\le \\frac{1}{e\^{\\ell}}.\\\]]{.math .display} To
achieve a target error probability of [\\(\\delta\\)]{.math .inline}, we
just solve for [\\(\\ell\\)]{.math .inline} in  and find that [\\(\\ell
\\ge \\ln \\tfrac{1}{\\delta}\\)]{.math .inline} rows are sufficient.
For [\\(\\delta\\)]{.math .inline} around 1%, [\\(\\ell = 5\\)]{.math
.inline} is good enough.

Count-Min Sketch: Final Report Card {#ss:reportcard}
-----------------------------------

-   The space required is that for [\\(\\tfrac{e}{\\eps} \\ln
    \\tfrac{1}{\\delta}\\)]{.math .inline} counters. Recall from
    Section \[ss:epshh\] that for the [\\(\\eps\\)]{.math .inline}-HH
    problem, [\\(\\eps = \\tfrac{1}{2k}\\)]{.math .inline} is a sensible
    choice. For [\\(k = 100\\)]{.math .inline} and [\\(\\delta =
    .01\\)]{.math .inline} this is in the low thousands. For larger
    values of [\\(k\\)]{.math .inline}, the number of counters needed
    scales linearly.[^16^](#fn16){#fnref16 .footnoteRef} In any case,
    the number of counters is totally independent of [\\(n\\)]{.math
    .inline} (which could be in billions)! This is the magic of the
    count-min sketch — you can throw out almost all of your data set and
    still maintain approximate frequency counts. Contrast this with
    bloom filters, and pretty much every other data structure that
    you’ve seen, where the space grows linearly with the number of
    processed elements.[^17^](#fn17){#fnref17 .footnoteRef}

-   Assuming the hash functions take constant time to evaluate, the  and
     operations run in [\\(O(\\ln \\tfrac{1}{\\delta})\\)]{.math
    .inline} time.

-   The count-min sketch guarantees 1-sided error: no matter how the
    hash functions [\\(h\_1,\\ldots,h\_{\\ell}\\)]{.math .inline} are
    chosen, for every object [\\(x\\)]{.math .inline} with frequency
    count [\\(f\_x\\)]{.math .inline}, the count-min sketch returns an
    estimate ([\\(x\\)]{.math .inline}) that is at least
    [\\(f\_x\\)]{.math .inline}.

-   Assuming that each hash function
    [\\(h\_1,\\ldots,h\_{\\ell}\\)]{.math .inline} is chosen uniformly
    from a universal family, for every object [\\(x\\)]{.math .inline}
    with frequency count [\\(f\_x\\)]{.math .inline}, the probability
    that the estimate ([\\(x\\)]{.math .inline}) output by the count-min
    sketch is greater than [\\(f\_x + \\eps n\\)]{.math .inline} is at
    most [\\(\\delta\\)]{.math .inline}. One would expect comparable
    performance for fixed well-crafted hash functions
    [\\(h\_1,\\ldots,h\_{\\ell}\\)]{.math .inline} on pretty much any
    data set that you might encounter.

Solving the [\\(\\eps\\)]{.math .inline}-Heavy Hitters Problem
--------------------------------------------------------------

The count-min sketch can be used to solve the [\\(\\eps\\)]{.math
.inline}-HH problem from Section \[ss:epshh\]. If the total
number [\\(n\\)]{.math .inline} of array elements is known in advance,
this is easy: set [\\(\\eps = \\tfrac{1}{2k}\\)]{.math .inline}, process
the array elements using a count-min sketch in a single pass, and
remember an element once its estimated frequency (according to the
count-min sketch) is at least [\\(\\tfrac{n}{k}\\)]{.math .inline}.

When [\\(n\\)]{.math .inline} is not known a priori, here is one way to
solve the problem. Assume that [\\(\\eps = \\tfrac{1}{2k}\\)]{.math
.inline} and so the number of counters is [\\(O(k \\ln
\\tfrac{1}{\\delta})\\)]{.math .inline}. In a single left-to-right pass
over the array [\\(A\\)]{.math .inline}, maintain the number
[\\(m\\)]{.math .inline} of array entries processed thus far. We store
potential heavy hitters in a heap data structure. When processing the
next object [\\(x\\)]{.math .inline} of the array, we invoke
[\\(\\inc(x)\\)]{.math .inline} followed by [\\(\\cnt(x)\\)]{.math
.inline}. If [\\(\\cnt(x) \\ge \\tfrac{m}{k}\\)]{.math .inline}, then we
store [\\(x\\)]{.math .inline} in the heap, using the key
[\\(\\cnt(x)\\)]{.math .inline}. (If [\\(x\\)]{.math .inline} was
already in the heap, we delete it before re-inserting it with its new
key value.) This requires one insertion and at most one deletion from
the heap. Also, whenever [\\(m\\)]{.math .inline} grows to the point
that some object [\\(x\\)]{.math .inline} stored in the heap has a key
less than [\\(m/k\\)]{.math .inline} (checkable in [\\(O(1)\\)]{.math
.inline} time via Find-Min), we delete [\\(x\\)]{.math .inline} from the
heap (via Extract-Min). After finishing the pass, we output all of the
objects in the heap.

Assume for simplicity that the count-min sketch makes no large errors,
with [\\(\\cnt(x) \\in \[f\_x,f\_x+\\eps n\]\\)]{.math .inline} for all
[\\(x\\)]{.math .inline}. Every object [\\(x\\)]{.math .inline} with
[\\(f\_x \\ge \\tfrac{n}{k}\\)]{.math .inline} is in the heap at the end
of the pass. (To see this, consider what happens the last time that
[\\(x\\)]{.math .inline} occurs.) The “no large errors” assumption
implies an approximate converse: every object [\\(x\\)]{.math .inline}
in the heap has true frequency count at least [\\(\\tfrac{n}{k} - \\eps
n = \\tfrac{n}{2k}\\)]{.math .inline} (other objects would be deleted
from the heap by the end of the pass). These are exactly the two
properties we ask of a solution to the [\\(\\eps\\)]{.math .inline}-HH
problem. If the count-min sketch makes large errors on a few objects,
then these objects might erroneously appear in the final output as well.
Ignoring the objects with large errors, the heap contains at most
[\\(2k\\)]{.math .inline} objects at all times (why?), so maintaining
the heap requires an extra [\\(O(\\log k) = O(\\log
\\tfrac{1}{\\eps})\\)]{.math .inline} amount of work per array entry.

Lecture Take-Aways
==================

1.  Hashing is even cooler and more useful than you had realized.

2.  The key ideas behind Bloom Filters extend to other lightweight data
    structures that are useful for solving other problems (not just the
    membership problem).

3.  The idea of lossy compression. If you only want to approximately
    preserve certain properties, like frequency counts, then sometimes
    you can get away with throwing away almost all of your data. We’ll
    see another example next week: dimension reduction, where the goal
    is to approximately preserve pairwise measures (like similarity)
    between objects.

4.  (Approximate) frequency counts/heavy hitters are exactly what you
    want in many applications — traffic at a network switch, click data
    at a major Web site, streaming data from a telescope or satellite,
    etc. It’s worth knowing that this useful primitive can be solved
    efficiently at a massive scale, with even less computation and space
    than most of the linear-time algorithms that you studied in CS161.

<div class="footnotes">

------------------------------------------------------------------------

1.  <div id="fn1">

    </div>

    ©2016–2017, Tim Roughgarden and Gregory Valiant. Not to be sold,
    published, or distributed without the authors’ consent.[↩](#fnref1)

2.  <div id="fn2">

    </div>

    A similar problem is the “top-[\\(k\\)]{.math .inline} problem,”
    where the goal is to output the [\\(k\\)]{.math .inline} values that
    occur with the highest frequencies. The algorithmic ideas introduced
    in this lecture are also relevant for the top-[\\(k\\)]{.math
    .inline} problem.[↩](#fnref2)

3.  <div id="fn3">

    </div>

    You wouldn’t expect there to be a majority element in any of these
    applications, but you might expect a non-empty set of heavy hitters
    when [\\(k\\)]{.math .inline} is 100, 1000, or 10000.[↩](#fnref3)

4.  <div id="fn4">

    </div>

    Rather than thinking of the array [\\(A\\)]{.math .inline} as an
    input fully specified in advance, we can alternatively think of the
    elements of [\\(A\\)]{.math .inline} as a “data stream,” which are
    fed to a “streaming algorithm” one element at a time. One-pass
    algorithms that use small auxiliary space translate to streaming
    algorithms that need only small working memory. One use case for
    streaming algorithms is when data arrives at such a fast rate that
    explicitly storing it is absurd. For example, this is often the
    reality in the motivating example of data packets traveling through
    a network switch. A second use case is when, even though data can be
    stored in its entirety and fully analyzed (perhaps as an overnight
    job), it’s still useful to perform lightweight analysis on the
    arriving data in real time. The first two applications (popular
    transactions or search queries) are examples of this.[↩](#fnref4)

5.  <div id="fn5">

    </div>

    A simple modification of this argument extends the impossibility
    result to all interesting values of [\\(k\\)]{.math .inline} — can
    you figure it out?[↩](#fnref5)

6.  <div id="fn6">

    </div>

    Somewhat more detail: if you always use sublinear space to store the
    set [\\(S\\)]{.math .inline}, then you need to reuse exactly the
    same memory contents for two different sets [\\(S\_1\\)]{.math
    .inline} and [\\(S\_2\\)]{.math .inline}. Your membership query
    answers will be the same in both cases, and in one of these cases
    some of your answers will be wrong.[↩](#fnref6)

7.  <div id="fn7">

    </div>

    This impossibility result (Fact \[fact:hh\]) and our response to it
    (the [\\(\\eps\\)]{.math .inline}-HH problem) serve as reminders
    that the skilled algorithm designer is respectful of but undaunted
    by impossibility results that limit what algorithms can do. For
    another example, recall your study in CS161 of methods for coping
    with [\\(NP\\)]{.math .inline}-complete problems.[↩](#fnref7)

8.  <div id="fn8">

    </div>

    There is a long tradition in the Internet of designing routers that
    are “fast and dumb,” and many of them have far less memory than a
    typical smartphone.[↩](#fnref8)

9.  <div id="fn9">

    </div>

    The proposal was to insert all correctly spelled words into a bloom
    filter. A false positive is then a misspelled word that the
    spellchecker doesn’t catch.[↩](#fnref9)

10. <div id="fn10">

    </div>

    The same data structure supports weighted increments, of the form
    ([\\(x\\)]{.math .inline},[\\(\\Delta\\)]{.math .inline}) for
    [\\(\\Delta \\ge 0\\)]{.math .inline}, in exactly the same way. With
    minor modifications, the data structure can even support deletions.
    We focus on the special case of incrementing by 1 for simplicity,
    and because it is sufficient for many of the motivating
    applications.[↩](#fnref10)

11. <div id="fn11">

    </div>

    Where do we get [\\(\\ell\\)]{.math .inline} hash functions from?
    The same way we get a single hash function. If we’re thinking of
    hash functions as being drawn at random from a universal family, we
    just make [\\(\\ell\\)]{.math .inline} independent draws from the
    family. If we’re thinking about deterministic but well-crafted hash
    functions, in practice it’s usually sufficient to take two good hash
    functions [\\(h\_1,h\_2\\)]{.math .inline} and use
    [\\(\\ell\\)]{.math .inline} linear combinations of them (e.g.
    [\\(h\_1\\)]{.math .inline}, [\\(h\_1+h\_2\\)]{.math .inline},
    [\\(h\_1 + 2h\_2\\)]{.math .inline}, …, [\\(h\_1 +
    (\\ell-1)h\_2\\)]{.math .inline}). Another common hack, which you’ll
    implement on Mini-Project \#1, is to derive each [\\(h\_i\\)]{.math
    .inline} from a single well-crafted hash function [\\(h\\)]{.math
    .inline} by defining [\\(h\_i(x)\\)]{.math .inline} as something
    like the hash (using [\\(h\\)]{.math .inline}) of the string formed
    by [\\(x\\)]{.math .inline} with “[\\(i\\)]{.math .inline}” appended
    to it.[↩](#fnref11)

12. <div id="fn12">

    </div>

    In an implementation that chooses [\\(h\_i\\)]{.math .inline}
    deterministically as a well-crafted hash function, the error
    analysis below does not actually hold for an arbitrary data set.
    (Recall that for every fixed hash function there is a pathological
    data set where everything collides.) So instead we say that the
    analysis is “heuristic” in this case, meaning that while not
    literally true, we nevertheless expect reality to conform to its
    predictions (because we expect the data to be non-pathological).
    Whenever you do a heuristic analysis to predict the performance of
    an implementation, you should always measure the implementation’s
    performance to double-check that it’s working as expected. (Of
    course, you should do this even when you’ve proved performance
    bounds rigorously — there can always be unmodeled effects (cache
    performance, etc.) that cause reality to diverge from your
    theoretical predictions for it.)[↩](#fnref12)

13. <div id="fn13">

    </div>

    Don’t forget that probabilities factor only for independent events.
    There are again two interpretations of this step: in the first, we
    assume that each [\\(h\_i\\)]{.math .inline} is chosen independently
    and randomly from a universal family of hash functions; in the
    second, we assume that the [\\(h\_i\\)]{.math .inline}’s are
    sufficiently well crafted that they almost always behave as if they
    were independent on real data.[↩](#fnref13)

14. <div id="fn14">

    </div>

    Note the analogous statement for products is false if the
    [\\(X\_j\\)]{.math .inline}’s are not independent. For example,
    suppose [\\(X\_1 \\in \\{0,1\\}\\)]{.math .inline} is uniform while
    [\\(X\_2 = 1-X\_1\\)]{.math .inline}. Then [\\({\\text{\\bf
    E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X\_1 \\cdot
    X\_2\\right\]} = 0\\)]{.math .inline} while [\\({\\text{\\bf
    E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X\_1\\right\]}
    \\cdot {\\text{\\bf
    E}\\ifthenelse{\\not\\equal{}{}}{\_{}}{}\\!\\left\[X\_2\\right\]} =
    \\tfrac{1}{4}\\)]{.math .inline}.[↩](#fnref14)

15. <div id="fn15">

    </div>

    Here [\\(e = 2.718\\ldots\\)]{.math .inline}, which gives slightly
    better constant factors than the choice of 2 in
    Section \[ss:error\_heur\].[↩](#fnref15)

16. <div id="fn16">

    </div>

    Note that the challenging case, and the case that often occurs in
    our motivating applications, is an array [\\(A\\)]{.math .inline}
    that simultaneously has lots of different elements but also a few
    elements that occur many times. If there are few distinct elements,
    one can maintain the frequency counts exactly using one counter per
    distinct element. If no elements occur frequently, then there’s
    nothing to do.[↩](#fnref16)

17. <div id="fn17">

    </div>

    How is this possible? Intuitively, with an error of [\\(\\eps
    n\\)]{.math .inline} allowed, only the elements with with large
    ([\\(&gt; \\eps n\\)]{.math .inline}) frequency counts matter, and
    there can be at most [\\(\\tfrac{1}{\\eps}\\)]{.math .inline} such
    elements (why?). Thus it is plausible that space proportional to
    [\\(\\tfrac{1}{\\eps}\\)]{.math .inline} might be enough. Of course,
    there’s still the issue of not knowing in advance which [\\(\\approx
    \\tfrac{1}{\\eps}\\)]{.math .inline} elements are the important
    ones![↩](#fnref17)

</div>
