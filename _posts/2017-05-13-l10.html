---
layout: post
title: "Lecture 10 : Tensors, and Low-Rank Tensor Recovery"
author: Tim
---
{% raw %}
<p>Last lecture discussed singular value decomposition (SVD), and we saw how such decompositions reveal structure about the matrix in question, allowing us to possibly de-noise the matrix, compress the matrix, fill in missing entries, etc. This lecture is about tensors, and we will see an analog of this sort of decomposition that, in a specific sense, can be much stronger than the matrix analog.</p>
<h1 id="introduction-to-tensors">Introduction to Tensors</h1>
<p>Tensor methods are relatively new, and are at the forefront of research in machine learning and data sciences, and, I suspect, will be increasingly viewed as a central tool for extracting features and structure from a dataset.</p>
<p>A tensor is just like a matrix, but with more dimensions:</p>
<p>A <span class="math inline">\(n_1 \times n_2 \times \ldots \times n_k\)</span> <span class="math inline">\(k\)</span>-tensor is a set of <span class="math inline">\(n_1 \cdot n_2 \cdot \ldots \cdot n_k\)</span> numbers, which one interprets as being arranged in a <span class="math inline">\(k\)</span>-dimensional hypercube. Given such a <span class="math inline">\(k\)</span>-tensor, <span class="math inline">\(A\)</span>, we can refer to a specific element via <span class="math inline">\(A_{i_1,i_2,\ldots, i_k}\)</span>.</p>
<p>A <span class="math inline">\(2\)</span>-tensor is simply a matrix, with <span class="math inline">\(A_{i,j}\)</span> referring to the <span class="math inline">\(i,j\)</span>th entry. You should think of a <span class="math inline">\(n_1\times n_2 \times n_3\)</span> <span class="math inline">\(3\)</span>-tensor as simply a stack of <span class="math inline">\(n_3\)</span> matrices, where each matrix has size <span class="math inline">\(n_1 \times n_2\)</span>. The entry <span class="math inline">\(A_{i,j,k}\)</span> of such a <span class="math inline">\(3\)</span>-tensor will refer to the <span class="math inline">\(i,j\)</span>th entry of the <span class="math inline">\(k\)</span>th matrix in the stack.</p>
<p>For our purposes (and in most computer science applications involving data), the above definition of tensors suffices. Tensors are very useful in physics, in which case they are viewed as more geometric objects, and are endowed with some geometric notion of what it means to change the coordinate system. We won’t worry about this, though be aware that you might come across a significantly more confusing definition of a tensor at some point….</p>
<h2 id="examples-of-tensors">Examples of Tensors</h2>
<p>We have all seen plenty of <span class="math inline">\(2\)</span>-tensors (i.e. matrices). Below we list a few examples of higher order tensors that you might encounter.</p>
<p>Given a body of text, and some ordering of the set of words (for example, just alphabetical ordering, <span class="math inline">\(w_1,\ldots,w_n\)</span>, we can associate a <span class="math inline">\(k\)</span>-tensor, <span class="math inline">\(A\)</span>, defined by setting entry <span class="math inline">\(A_{i_1,\ldots,i_k}\)</span> equal to the number of times the sequence of words <span class="math inline">\(w_{i_1},w_{i_2},\ldots,w_{i_k}\)</span> occurs in the text. For example, if we have <span class="math inline">\(n\)</span> distinct words, we could extract an <span class="math inline">\(n\times n \times n\)</span> <span class="math inline">\(3\)</span>-tensor from a corpus of text, where <span class="math inline">\(A_{i,j,k}\)</span> represents the number of times in the corpus that the <span class="math inline">\(i\)</span>th, <span class="math inline">\(j\)</span>th, and <span class="math inline">\(k\)</span>th words occur in sequence.</p>
<p>Suppose we have some data <span class="math inline">\(s_1,s_2,\ldots,s_n\)</span> representing independent draws from some high-dimensional distribution in <span class="math inline">\({\mathbb{R}}^d\)</span>. That is, each <span class="math inline">\(s_i \in {\mathbb{R}}^d.\)</span> The mean of this data is simply a vector of length <span class="math inline">\(d\)</span>. The covariance matrix of this data is represented by a <span class="math inline">\(d \times d\)</span> matrix, whose <span class="math inline">\(i,j\)</span>th entry is the empirical estimate of <span class="math inline">\({\mathbb{E}}[(X_i-{\mathbb{E}}[X_i])(X_j-{\mathbb{E}}[X_j])]\)</span>, where <span class="math inline">\(X_i\)</span> denotes the <span class="math inline">\(ith\)</span> coordinate of a sample from the distribution. We can also consider higher moments: the <span class="math inline">\(d\times d \times d\)</span> <span class="math inline">\(3\)</span>-tensor representing the third order moments, has entries <span class="math inline">\(A_{i,j,k}\)</span> representing the empirical estimate of <span class="math inline">\({\mathbb{E}}\left[(X_i-{\mathbb{E}}[X_i])(X_j-{\mathbb{E}}[X_j])[(X_k-{\mathbb{E}}[X_k])\right]\)</span>. This is simply given by the following expression in terms of the data <span class="math inline">\(s_1,\ldots\)</span>: letting <span class="math inline">\(m_i,m_j,m_k\)</span> denote the average value of the <span class="math inline">\(i\)</span>th, <span class="math inline">\(j\)</span>th, and <span class="math inline">\(k\)</span>th components of the datapoints <span class="math inline">\(s_1,\ldots,\)</span>, <span class="math display">\[M_{i,j,k} = \frac{1}{n} \sum_{\ell=1}^n (s_{\ell_i} - m_i)(s_{\ell_j} - m_j)(s_{\ell_k} - m_k),\]</span> where <span class="math inline">\(s_{\ell_i}\)</span> denotes the value in the <span class="math inline">\(i\)</span>th dimension of datapoint <span class="math inline">\(s_\ell.\)</span> We can define higher order tensors analogously, corresponding to higher order moments.</p>
<h2 id="the-rank-of-a-tensor">The Rank of a Tensor</h2>
<p>The rank of a tensor is defined analogously to the rank of a matrix. Recall that a matrix <span class="math inline">\(M\)</span> has rank <span class="math inline">\(r\)</span> if it can be written as <span class="math inline">\(M = U V^t\)</span>, where <span class="math inline">\(U\)</span> has <span class="math inline">\(r\)</span> columns, and <span class="math inline">\(V\)</span> has <span class="math inline">\(r\)</span> columns. Letting <span class="math inline">\(u_1,\ldots,u_r\)</span> and <span class="math inline">\(v_1,\ldots,v_r\)</span> denote these columns, note that <span class="math display">\[M = \sum_{i=1}^r u_i v_i^t.\]</span> Note that this is the <em>outer-product</em> of these vectors, and this expression represents <span class="math inline">\(M\)</span> as a sum of <span class="math inline">\(r\)</span> rank 1 matrices, where the <span class="math inline">\(i\)</span>th matrix <span class="math inline">\(B_i = u_i v_i^t\)</span> has entries <span class="math inline">\(B_{j,k}= u_i(j) v_i(k)\)</span>, namely the product of the <span class="math inline">\(j\)</span>th entry of vector <span class="math inline">\(u_i\)</span> and the <span class="math inline">\(k\)</span>th entry of vector <span class="math inline">\(v_i\)</span>. Another way of thinking about this, is that a matrix is rank 1 if all the rows are multiples of each other, and that a rank <span class="math inline">\(k\)</span> matrix is just a sum of <span class="math inline">\(k\)</span> rank 1 matrices.</p>
<p>Tensor rank is defined analogously. A tensor has rank 1 if all the rows of all the matrices in the tensor are multiples of each other, and a tensor is rank <span class="math inline">\(k\)</span> if it can be written as a sum of <span class="math inline">\(k\)</span> rank 1 tensors. We first define the notation we use to represent this, via the definition of the vector outer-product.</p>
<p>Given vectors <span class="math inline">\(u,v,w\)</span>, of lengths <span class="math inline">\(n, m,\)</span> and <span class="math inline">\(\ell\)</span>, respectively, their <em>tensor product</em> or <em>outer product</em> is the <span class="math inline">\(n \times m \times \ell\)</span> rank 1 3-tensor denoted <span class="math inline">\(A=u \otimes v \otimes w\)</span> <span class="math inline">\(A\)</span> with entries <span class="math inline">\(A_{i,j,k} = u_i v_j w_k.\)</span></p>
<p>The above definition extends naturally to higher dimensional tensors:</p>
<p>Given vectors <span class="math inline">\(v_1,v_2,\ldots,v_k\)</span>, of lengths <span class="math inline">\(n_1,n_2,\ldots,n_k\)</span>, the <em>tensor product</em> is denoted <span class="math inline">\(v_1 \otimes v_2 \otimes \ldots \otimes v_k\)</span> is the <span class="math inline">\(n_1 \times n_2 \times \ldots \times n_k\)</span> <span class="math inline">\(k\)</span>-tensor <span class="math inline">\(A\)</span> with entry <span class="math inline">\(A_{i_1,i_2,\ldots,i_k} = v_1(i_1)\cdot v_2(i_2)\cdot \ldots \cdot v_k(i_k).\)</span></p>
<p>For example, given <span class="math display">\[v_1= \left( \begin{array}{c} 1 \\ 2\\ 3 \end{array}\right),  v_2= \left(\begin{array}{c} -1 \\ 1 \end{array}\right), v_3= \left( \begin{array}{c} 10 \\ 20 \end{array}\right), .\]</span> <span class="math inline">\(v_1 \otimes v_2 \otimes v_3\)</span> is a <span class="math inline">\(3 \times 2 \times 2\)</span> <span class="math inline">\(3\)</span>-tensor, that can be thought of as a stack of two <span class="math inline">\(3 \times 2\)</span> matrices <span class="math display">\[M_1= \left( \begin{array}{cc} -10 &amp; 10 \\ -20 &amp; 20\\ -30 &amp; 30 \end{array}\right),  M_2= \left( \begin{array}{cc} -20 &amp; 20 \\ -40 &amp; 40\\ -60 &amp; 60 \end{array}\right).\]</span></p>
<p>We are now ready to define the rank of a tensor, which will correspond to our definition of the rank of a matrix in the case that we are referring to a 2-tensor:</p>
<p>A <span class="math inline">\(3\)</span>-tensor <span class="math inline">\(A\)</span> has rank <span class="math inline">\(r\)</span> if there exists <span class="math inline">\(3\)</span> sets of <span class="math inline">\(r\)</span> vectors, <span class="math inline">\(u_1,\ldots,u_r,\)</span> <span class="math inline">\(v_1,\ldots,v_r\)</span> and <span class="math inline">\(w_1,\ldots,w_r\)</span> such that <span class="math display">\[A = \sum_{i=1}^r u_i \otimes v_i \otimes w_i.\]</span> The definition of rank for general <span class="math inline">\(k\)</span>-tensors is analogous.</p>
<h1 id="differences-between-matrices-and-tensors">Differences between Matrices and Tensors</h1>
<p>In general, most of what you know about linear algebra for matrices does NOT apply to <span class="math inline">\(k\)</span>-tensors for <span class="math inline">\(k \ge 3\)</span>. Below is a brief list of notable differences between tensors and matrices:</p>
<ol>
<li><p>For matrices, the best rank-<span class="math inline">\(k\)</span> approximation can be found by iteratively finding the best rank-1 approximation, and then subtracting it off. In other words, for a matrix <span class="math inline">\(M\)</span>, the best rank <span class="math inline">\(1\)</span> approximation of <span class="math inline">\(M\)</span> is the same as the best rank <span class="math inline">\(1\)</span> approximation of the matrix <span class="math inline">\(M_2\)</span> defined as the best rank <span class="math inline">\(2\)</span> approximation of <span class="math inline">\(M\)</span>. Because of this, if <span class="math inline">\(u v^t\)</span> is the best rank 1 approximation of <span class="math inline">\(M\)</span>, then <span class="math inline">\(rank(M-uv^t)  = rank(M - 1).\)</span></p>
<p>For <span class="math inline">\(k\)</span>-tensors with <span class="math inline">\(k \ge 3\)</span>, this is not always the case. If <span class="math inline">\(u \otimes v \otimes w\)</span> is the best rank 1 approximation of 3-tensor <span class="math inline">\(A\)</span>, it is possible that <span class="math inline">\(rank(A-u \otimes v \otimes w) &gt; rank(A).\)</span></p></li>
<li><p>For matrices with entries in <span class="math inline">\({\mathbb{R}}\)</span>, there is no point in looking for a low-rank decomposition that involves complex numbers, because <span class="math inline">\(rank_{\mathbb{R}}(M)=rank_{\mathbb{C}}(M).\)</span> For <span class="math inline">\(k\)</span>-tensors, with <span class="math inline">\(k \ge 3,\)</span> this is not always the case, it can be that the rank over complex vectors is smaller than the rank over real vectors, even if the entries in the tensor are real-valued.</p></li>
<li><p>We don’t really know how to argue about the rank of <span class="math inline">\(3\)</span>-tensors: for example, with probability 1, if you pick the entries of an <span class="math inline">\(n\times n \times n\)</span> <span class="math inline">\(3\)</span>-tensor independently at random from the interval <span class="math inline">\([0,1]\)</span>, the rank will be on the order of <span class="math inline">\(n^2\)</span>, however we don’t know how to describe any explicit construction of <span class="math inline">\(n\times n \times n\)</span> tensors whose rank is greater than <span class="math inline">\(n^{1.1}\)</span>, for all <span class="math inline">\(n\)</span>.</p></li>
<li><p>Computing the rank of matrices is easy (e.g. via SVD). Computing the rank of <span class="math inline">\(3\)</span>-tensors is NP-hard.</p></li>
<li><p>As we will explore in the following section, despite the above point, if the rank of a <span class="math inline">\(3\)</span>-tensor is sufficiently small, then its rank can be efficiently computed, its low-rank representation is <em>unique</em>, and can be efficiently recovered.</p></li>
</ol>
<h1 id="low-rank-tensors">Low-Rank Tensors</h1>
<p>Recall that a low-rank representation of a matrix <span class="math inline">\(M\)</span>, is <em>not</em> unique. For <span class="math inline">\(M= U V^t\)</span>, where both <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> have <span class="math inline">\(r\)</span> columns, for any <span class="math inline">\(r \times r\)</span> invertible matrix <span class="math inline">\(C\)</span>, we have <span class="math inline">\(M = U C C^{-1} V^t= (U C) (C^{-1} V^t),\)</span> and hence the columns of <span class="math inline">\(UC\)</span>, and the rows of <span class="math inline">\(C^{-1} V^t\)</span> form a different rank <span class="math inline">\(r\)</span> representation of <span class="math inline">\(M\)</span>. This lack of uniqueness of low-rank representations is frustrating if we hope to interpret the various factors.</p>
<p>One of the earlier pioneers of low-rank approximation of matrices was the British physcologist/statistician Charles Spearman. One of his early experiments was to give a number of academic tests to a number of students, and form the matrix <span class="math inline">\(M\)</span> in which entry <span class="math inline">\(M_{i,j}\)</span> represented the performance of the <span class="math inline">\(i\)</span>th student on the <span class="math inline">\(j\)</span>th test. He realized that <span class="math inline">\(M\)</span> was very close to a rank <span class="math inline">\(2\)</span> matrix, and went on to conjecture that this might arise via the following explanation: suppose the <span class="math inline">\(ith\)</span> student has two number <span class="math inline">\(m_i,v_i\)</span> representing their mathematical ability and verbal ability. Suppose further that the <span class="math inline">\(j\)</span>th test can basically be represented as two number, <span class="math inline">\(t_j,q_j\)</span> representing that tests’ mathematical and verbal components. If this model were correct, then <span class="math inline">\(M_{i,j} \approx m_it_j + v_i q_j\)</span>, and hence <span class="math inline">\(M\)</span> would be close to the rank 2 matrix <span class="math inline">\(UV^t\)</span>, where the two columns of <span class="math inline">\(U\)</span> represent the students’ math/verbal abilities, and the two columns of <span class="math inline">\(V\)</span> represent the tests’ math/verbal components. Unfortunately, the rank-2 representation is not unique, as mentioned in the previous paragraph, and hence even if this model of the world were true, the rank 2 representation recovered would not correspond to this model.</p>
<p>Amazingly, once one goes from to <span class="math inline">\(3\)</span>-tensors, low-rank decompositions end up being essentially unique!</p>
<p>[thm:1] Given a <span class="math inline">\(3\)</span>-tensor <span class="math inline">\(A\)</span> of rank <span class="math inline">\(k\)</span> s.t. there exists three sets of linearly independent vectors, <span class="math inline">\((u_1,\ldots,u_k), (v_1,\ldots,v_k),(w_1,\ldots,w_k),\)</span> s.t. <span class="math display">\[A = \sum_{i=1}^k u_i \otimes v_i \otimes w_i,\]</span> then this rank <span class="math inline">\(k\)</span> decomposition is unique (up to scaling the vectors by a constant), and these factors can be efficiently recovered.</p>
<p>To give a simple illustration of the above theorem, suppose we conducted Spearman’s experiment, except we added an extra dimension—suppose we administered each test to each student in 1 of three different settings (i.e. a setting in which classical music is playing, a setting with distracting video playing, and a control setting). Let <span class="math inline">\(M\)</span> denote the corresponding <span class="math inline">\(3\)</span>-tensor, with <span class="math inline">\(M_{i,j,k}\)</span> denoting the performance of student <span class="math inline">\(i\)</span> on test <span class="math inline">\(j\)</span> in setting <span class="math inline">\(k\)</span>. Suppose the true model of the world is as follows: as above, for every student there are two numbers representing their math/verbal ability, and every test can be regarded as having a math/verbal component; additionally, for each setting, there is some scaling of the math performance resulting from that setting, and a scaling of the verbal performance resulting from that setting. Hence <span class="math inline">\(M_{i,j,k}\)</span> can be approximated by multiplying the math ability of the student with the math component of the test and the math boost-factor of the setting, and then adding the corresponding product from the verbal components. Theorem [thm:1] asserts that, provided the vector of student math abilities is not identical (up to a constant rescaling) to the vector of verbal abilities, and the 2 vectors of math/verbal test components are not identical up to rescaling, and the 2 vectors of math/verbal setting boosts are not identical up to rescaling, then this is the unique factorization of this tensor, and we will be able to recover these exact factors.</p>
<h2 id="quick-discussion">Quick Discussion</h2>
<p>As mentioned in previous lectures, one can often interpret the top two or three singular vectors of a dataset. Perhaps the main reason that 4th, 5th, etc. singular vectors cannot be interpreted, is that—because they must be orthogonal to the first components—they end up not representing the clean/interpretable phenomena that one might hope. The beauty of Theorem [thm:1] is that the factors do not need to be orthogonal; as long as they are linearly independent, then they can be recovered uniquely. More broadly, tensor methods offer one hope for enabling useful features to be extracted from data in an unsupervised setting.</p>
<h2 id="the-algorithm">The Algorithm</h2>
<p>We now describe the algorithm alluded to in Theorem [thm:1]. This algorithm was originally proposed in the 1970’s but has been reinvented several times, in several different contexts, and is often referred to as “Jenrich’s Algorithm”. Do not worry too much about the details—the main step that might be unfamiliar is the computation of an eigen-decomposition of a matrix <span class="math inline">\(M = Q S Q^{-1}\)</span> where <span class="math inline">\(S\)</span> is the diagonal matrix of eigenvalues, and the columns of <span class="math inline">\(Q\)</span> are eigenvectors. Note that in Lecture 7 and 8, we only looked at eigenvectors of a symmetric matrix <span class="math inline">\(X X^t\)</span>, in which case <span class="math inline">\(Q^t = Q^{-1}\)</span>.</p>
<p><span> </span></p>
<p>Before analyzing the above algorithm, we note that if the original tensor <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times m \times p\)</span>, rather than <span class="math inline">\(n \times n \times n\)</span>, then the above algorithm continues to work, provided we compute the eigen-decomposition of the matrices <span class="math inline">\(A_x A_y^+\)</span> and <span class="math inline">\(A_x^+ A_y,\)</span> where <span class="math inline">\(M^+\)</span> denotes the “pseudo-inverse” of <span class="math inline">\(M\)</span>, which is the analog of inverses for non-square matrices.</p>
<p>To analyze the above algorithm, we first argue that <span class="math inline">\(A_x = \sum_{i=1}^k \langle w_i, x\rangle u_i v_i^t,\)</span> and, similarly, <span class="math inline">\(A_y = \sum_{i=1}^k \langle w_i, y\rangle u_i v_i^t.\)</span></p>
<p>For <span class="math inline">\(A_x\)</span> and <span class="math inline">\(A_y\)</span> as defined in the algorithm, <span class="math display">\[A_x = \sum_{i=1}^k \langle w_i, x\rangle u_i v_i^t, \text{  and  } A_y = \sum_{i=1}^k \langle w_i, y\rangle u_i v_i^t..\]</span></p>
<p>First consider the case where <span class="math inline">\(k=1\)</span>. Hence <span class="math inline">\(A = u_1 \otimes v_1 \otimes w_1\)</span>, and the <span class="math inline">\(i\)</span>th matrix in the stack corresponding to <span class="math inline">\(A\)</span> is <span class="math inline">\(w_1(i) \cdot u_1 v_1^t\)</span>. Hence the contribution of this matrix to the matrix <span class="math inline">\(A_x\)</span> is defined to be <span class="math inline">\(x_i\cdot w_1(i) \cdot u_1 v_1^t,\)</span> and hence the matrix <span class="math inline">\(A_x\)</span> is simply <span class="math inline">\( u_1 v_1^t \sum_i x_i\cdot w(i) = \langle w_1, x\rangle u_1 v_1^t\)</span>. Since <span class="math inline">\(A\)</span> is simply a sum of these rank 1 factors, <span class="math inline">\(A_x\)</span> and <span class="math inline">\(A_y\)</span> are simply the sum of the corresponding rank 1 components, weighted appropriately.</p>
<p>Given the above lemma, <span class="math inline">\(A_x = U D V^t\)</span> where the columns of <span class="math inline">\(U\)</span> are the vectors <span class="math inline">\(u_i\)</span>, and the columns of <span class="math inline">\(V\)</span> are the vectors <span class="math inline">\(v_i\)</span>, and <span class="math inline">\(D\)</span> is a diagonal matrix with <span class="math inline">\(i\)</span>th entry <span class="math inline">\(\langle w_i,x\rangle.\)</span> Similarly, <span class="math inline">\(A_y =  U E V^t,\)</span> where the <span class="math inline">\(i\)</span>th diagonal entry of <span class="math inline">\(E\)</span> is <span class="math inline">\(\langle w_i,y\rangle.\)</span> Given this, <span class="math display">\[A_x A_y^{-1} = U D V^t (V^t)^{-1} E^{-1} U^{-1} = U (DE^{-1}) U^{-1},\]</span> and similarly <span class="math display">\[A_x^{-1} A_y = (V^t)^{-1}D^{-1}U^{-1} U E V^t=(V^t)^{-1}D^{-1}E V^t.\]</span> The correctness of the algorithm now follows from the uniqueness of the eigen-decomposition in the case that the eigenvalues are distinct. Without belaboring the details, for a random choice of vectors <span class="math inline">\(x,y\)</span>, provided the <span class="math inline">\(u\)</span>’s, <span class="math inline">\(v\)</span>’s and <span class="math inline">\(w\)</span>’s are linearly independent, with probability <span class="math inline">\(1\)</span> the eigenvalues of the above matrices will be distinct. Hence we can recover the list of <span class="math inline">\(u&#39;s\)</span> and the list of <span class="math inline">\(v&#39;s\)</span>.</p>
<p>How do we match up the right <span class="math inline">\(u\)</span> with the corresponding <span class="math inline">\(v\)</span>? That is, we want to ensure that we know that <span class="math inline">\(u_1\)</span> belongs to the same factor as <span class="math inline">\(v_1\)</span>, rather than, say, grouping <span class="math inline">\(u_1\)</span> with <span class="math inline">\(v_3\)</span>. This is easy, as the eigenvalues of <span class="math inline">\(A_x A_y^{-1} \)</span> are given by the diagonal matrix <span class="math inline">\(DE^{-1}\)</span> and hence are reciprocals of the eigenvalues of <span class="math inline">\(A_x^{-1} A_y\)</span>, so if <span class="math inline">\(u_1\)</span> is the eigenvector of <span class="math inline">\(A_x A_y^{-1}\)</span> with largest eigenvalue <span class="math inline">\(\lambda_1\)</span>, then <span class="math inline">\(v_1\)</span> will be the eigenvector of <span class="math inline">\(A_x^{-1} A_y\)</span> with smallest eigenvalue which will be equal to <span class="math inline">\(1/\lambda_1\)</span>.</p>


{% endraw %}

