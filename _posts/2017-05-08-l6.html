---
layout: post
title: Lecture 6
author: Tim
---
{% raw %}

<h1 id="context">Context</h1>
<p>Last lecture we covered the basics of gradient descent, with an emphasis on the intuition behind and geometry underlying the method, plus a concrete instantiation of it for the problem of linear regression (fitting the best hyperplane to a set of data points). This basic method is already interesting and useful in its own right (see Homework #3).</p>
<p>This lecture we’ll cover two extensions that, while simple, will bring your knowledge a step closer to the state-of-the-art in modern machine learning. The two extensions have different characters. The first concerns how to actually solve (computationally) a given unconstrained minimization problem, and gives a modification of basic gradient descent — “stochastic gradient descent” — that scales to much larger data sets. The second extension concerns problem formulation rather than implementation, namely the choice of the unconstrained optimization problem to solve (i.e., the objective function <span class="math inline">\(f\)</span>). Here, we introduce the idea of “regularization,” with the goal of avoiding overfitting the function learned to the data set at hand, even for very high-dimensional data.</p>
<h1 id="s:recap">Recap</h1>
<p>Recall that an unconstrained minimization problem is defined by a function <span class="math inline">\(f:\RR^n \rightarrow \RR\)</span>, and the goal is to compute the point <span class="math inline">\(\w \in \RR^n\)</span> that minimizes this function. Recall the basic gradient descent method:</p>
<p>initialize <span class="math inline">\(\w := \w_0\)</span></p>
<p>Recall that the parameter <span class="math inline">\(\alpha\)</span> is called the <span><em>step size</em></span> or <span><em>learning rate</em></span>. An alternative stopping rule (as seen in Homework #3) is to just run gradient descent for a fixed number of iterations and then return the final point.</p>
<p>In , both <span class="math inline">\(\w\)</span> and <span class="math inline">\(\grad f(\w)\)</span> are <span class="math inline">\(n\)</span>-vectors, while <span class="math inline">\(\alpha\)</span> is a scalar. It’s also worth zooming in to see what this update rule looks like in some coordinate, say the <span class="math inline">\(j\)</span>th one: <span class="math display">\[\label{eq:update2}
w_j := w_j - \alpha \cdot \frac{\del f}{\del w_j}(\w).\]</span> The update  can be thought of as <span class="math inline">\(n\)</span> updates of the form  being done in parallel (one per coordinate <span class="math inline">\(j\)</span>).<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Recall the intuition behind the method. Gradient descent enters a contract with basic calculus. Calculus says that a function <span class="math inline">\(f\)</span> differentiable at a point <span class="math inline">\(\w\)</span> can be locally well approximated by a linear function, namely <span class="math inline">\(f(\w+\z) \approx f(\w) + \z^T\grad
f(\w)\)</span> for <span class="math inline">\(\z \in \RR^n\)</span>. This is analogous to drawing a tangent line to the graph of a univariate function (the <span class="math inline">\(n=1\)</span> case). It’s intuitively clear that the tangent line approximates the function well near the point of approximation, but not generally for faraway points. But a linear function <span class="math inline">\(\c^T\w\)</span> is easy to minimize — just move in the direction <span class="math inline">\(-\c\)</span>, for a rate of decrease of <span class="math inline">\({
{\| {\c} \|}
}_2\)</span>. Combining these two facts motivates moving in the direction <span class="math inline">\(-\grad f(\w)\)</span>, of “steepest descent,” for a rate of decrease of <span class="math inline">\({
{\| {\grad f(\w)} \|}
}\)</span>. (The latter point explains the stopping rule — stop once the rate of improvement is too small to bother with.) Gradient descent’s part of the contract is to only take a small step (as controlled by the parameter <span class="math inline">\(\alpha\)</span>), so that the guiding linear approximation is approximately accurate.</p>
<p>Under mild assumptions, gradient descent converges to a local minimum, which may or may not be a global minimum. If <span class="math inline">\(f\)</span> is convex — meaning all chords lie above its graph — then gradient descent converges to a global minimum (under mild assumptions). Some important problems are convex (like the regression problems discussed today), while others are not (like the QWOP problem on Homework #3).</p>
<p>For the linear regression problem, the dimension <span class="math inline">\(n\)</span> is the number of (real-valued) features or attributes of each data point <span class="math inline">\(\x^1,\ldots,\x^m \in \RR^n\)</span>. Also given are real-valued labels <span class="math inline">\(y^1,\ldots,y^m \in \RR\)</span>. We associate each vector <span class="math inline">\(\w \in \RR^n\)</span> with the linear function <span class="math inline">\(\x
\mapsto \w^T\x\)</span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> We took the objective function <span class="math inline">\(f\)</span> equal to the mean-squared error (MSE) achieved by a linear function, so <span class="math display">\[\label{eq:mse}
f(\w) = \frac{1}{m} \sum_{i=1}^m E_i(\w)^2,\]</span> where <span class="math display">\[E_i(\w) = \underbrace{\left( \sum_{j=1}^n w_jx^{(i)}_j
  \right)}_{\text{$\w$&#39;s ``prediction&#39;&#39; of $\x^{(i)}$&#39;s label}} -
\underbrace{y^{(i)}}_{\text{$i$&#39;s label}}\]</span> is the prediction error made by the function <span class="math inline">\(\w\)</span> for the label of the data point <span class="math inline">\(\x^{(i)}\)</span>. The gradient of this <span class="math inline">\(f\)</span> is <span class="math display">\[\label{eq:msegrad}
\grad f(\w) = \frac{1}{m} \sum_{i=1}^m 2E_i(\w) \cdot \x^{(i)},\]</span> and so gradient descent moves in the opposite direction of this. Recall the interpretation: each data point <span class="math inline">\(\x^{(i)}\)</span> “votes” to change the coefficients <span class="math inline">\(\w\)</span> in the direction that would improve the prediction of its label as rapidly as possible (the direction <span class="math inline">\(\x^{(i)}\)</span> if <span class="math inline">\(\w\)</span> underestimates <span class="math inline">\(y^{(i)}\)</span>, or <span class="math inline">\(-\x^{(i)}\)</span> if <span class="math inline">\(\w\)</span> overestimates <span class="math inline">\(y^{(i)}\)</span>). Each vote is weighted according to the current error of <span class="math inline">\(\w\)</span> on that data point, and then these weighted votes are averaged to obtain the direction in which to move.</p>
<p>Finally, recall that each iteration of gradient descent takes <span class="math inline">\(O(mn)\)</span> time for linear regression — <span class="math inline">\(O(n)\)</span> time per data point. Observe that the work involved is unusually easy to parallelize — one can just distribute the data points across however many cores or machines are available, independently compute the summands in the gradient , and then aggregate the results.</p>
<h1 id="how-big-are-m-and-n">How Big Are <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>?</h1>
<p>How should we feel about a running time of <span class="math inline">\(O(mn)\)</span> per iteration of gradient descent? The answer clearly depends on how big <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are. The values of <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> vary greatly across different data sets, and the choice of computational method to use depends on their values.</p>
<h2 id="different-data-sets">Different Data Sets</h2>
<p>At one extreme, we have the case of small data sets. Almost all data sets collected by hand qualify as “small,” which covers the bulk of the data sets that pre-date the 21st century. Classical statistics was developed with such manageable data sets in mind. One famous example is the Iris data set, which was collected by the botanist Edger Anderson and then statistically analyzed by Ronald Fisher (in 1936) <span class="citation"></span>. Anderson measured the length and width of the petal and the sepal of each iris, so each data point is a 4-tuple (<span class="math inline">\(n=4\)</span>). The number of data points is also small (<span class="math inline">\(m=150\)</span>) but this is a less important point.</p>
<p>These days, technology enables the collection of very large data sets (e.g., through Web crawling). For example, consider a collection of documents, each represented as a “bag of words.” Recall this means representing a document as a (sparse) <span class="math inline">\(n\)</span>-vector, where each coordinate counts the frequency of a given word in the document. The number <span class="math inline">\(n\)</span> of dimensions is equal to the number of words you’re keeping tracking of, which is often in the tens of thousands. The number <span class="math inline">\(m\)</span> of documents in a given data set varies, but in general you want to use the biggest data sets that you can get your hands on. For example, there are roughly 5 million articles on Wikipedia; in this case, <span class="math inline">\(mn\)</span> would be in the tens or hundreds of billions (i.e., pretty big).</p>
<p>The story is similar with images. For example, representing a 100-by-100-pixel image with a vector of pixel intensities (grayscale, say) results in <span class="math inline">\(n=10^4\)</span>. Image data set sizes vary, but there are some very big ones around (e.g., the Tiny Images dataset has roughly 80 million 32-by-32 images <span class="citation"></span>). So <span class="math inline">\(mn\)</span> is again in the tens or hundreds of billions.</p>
<h2 id="the-normal-equations-for-small-data-sets">The Normal Equations for Small Data Sets</h2>
<p>For small data sets, there is no need to run gradient descent to perform linear regression. The problem of minimizing the mean-squared error is so nice that it admits a closed-form solution. Specifically, the solution is <span class="math display">\[\label{eq:normal}
\w = \left(\X^T\X\right)^{-1}\X^T\y,\]</span> where <span class="math display">\[\X =
\left[
  \begin{array}{ccc}
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \x^{(1)} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \x^{(2)} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    &amp; \vdots &amp; \\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \x^{(m)} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\end{array}
\right]\]</span> is the <span class="math inline">\(m \times n\)</span> matrix with <span class="math inline">\(\x^{(i)}\)</span>’s for rows (with the dummy coordinate <span class="math inline">\(x^{(i)}_1 = 1\)</span> for all <span class="math inline">\(i\)</span>), and <span class="math display">\[\y =  
\left[
\begin{array}{c}
y^1\\
y^2\\
\vdots\\
y^n
\end{array}
\right]\]</span> denotes the <span class="math inline">\(m\)</span>-vector of <span class="math inline">\(y^{(i)}\)</span>’s. The <span class="math inline">\(n\)</span> equations in  are called the <span><em>normal equations</em></span>;<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> the derivation is straightforward calculus (setting the gradient of MSE(<span class="math inline">\(\w\)</span>) to <span class="math inline">\(\zero\)</span> and solving).</p>
<p>Solving the equations in  requires <span class="math inline">\(O(n^3 + mn^2)\)</span> time — <span class="math inline">\(O(n^3)\)</span> to invert the <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span> matrix <span class="math inline">\(\X^T\X\)</span> and <span class="math inline">\(O(mn^2)\)</span> for the matrix multiplications. There is no reason not to use the normal equations to perform linear regression when <span class="math inline">\(n\)</span> is reasonably small (in the 100s, say) and <span class="math inline">\(m\)</span> is not extremely big.</p>
<h1 id="stochastic-gradient-descent-the-case-of-large-m">Stochastic Gradient Descent: The Case of Large <span class="math inline">\(m\)</span></h1>
<p>Once the number <span class="math inline">\(n\)</span> of dimensions is moderately large (as is the case with documents or images), the matrix operations needed for solving the normal equations require a prohibitive amount of computation. If <span class="math inline">\(mn\)</span> is not extremely large, then the per-iteration cost of <span class="math inline">\(O(mn)\)</span> required by basic gradient descent is fine.</p>
<p>In general you want to use as much data as possible, since more data allows you to learn richer and more accurate models. For very large <span class="math inline">\(m\)</span>, it can be problematic to spend <span class="math inline">\(O(mn)\)</span> time computing the gradient  in every single iteration of gradient descent.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> For concreteness, imagine that <span class="math inline">\(m\)</span> is so large that computing the gradient requires one day of wall-clock time on the best computing platform available to you. One way to think about the competing methods of gradient descent and solving the normal equations is as a trade-off between the number of iterations and the computation required per iteration. At one extreme, solving the normal equations can be regarded as a single-iteration algorithm, with a fair amount of work done in that iteration (<span class="math inline">\(O(n^3+mn^2)\)</span>). Gradient descent requires multiple iterations to converge to a solution — the number depends on many factors, but for now think of it as in the dozens or hundreds — but does only <span class="math inline">\(O(mn)\)</span> work per iteration. Can we take this idea further, doing still less work per iteration, at the cost of a small number of extra iterations? <span><em>Stochastic gradient descent</em></span> provides an affirmative answer to this question.</p>
<p>Stochastic gradient descent is defined for objective functions <span class="math inline">\(f\)</span> that can be expressed as the average of simpler functions: <span class="math display">\[f(\w) = \frac{1}{m} \sum_{i=1}^m f_i(\w).\]</span> Mean-squared error  is one example, and there are many others, especially in machine learning contexts. (For a non-example, see the QWOP problem on Homework #3.) For such a function, by linearity of derivatives, we have <span class="math display">\[\grad f(\w) = \frac{1}{m} \sum_{i=1}^m \grad f_i(\w).\]</span></p>
<p>The update rule in stochastic gradient descent is <span class="math display">\[\label{eq:sgd}
\w := \w - \alpha \cdot \grad f_i(\w),\]</span> where <span class="math inline">\(i \in \{1,2,\ldots,m\}\)</span>. For example, when minimizing MSE, this update rule is <span class="math display">\[\w := \w - \alpha \cdot 2E_i(\w) \cdot \x^{(i)}.\]</span> This is, instead of asking all data points for their votes and averaging as in , we only ask for the opinion of a <span> <em>single</em></span> randomly chosen data point — the dictator for this iteration.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>How is <span class="math inline">\(i\)</span> chosen? For intuition, imagine that we choose <span class="math inline">\(i\)</span> uniformly at random from <span class="math inline">\(\{1,2,\ldots,m\}\)</span>. Then the expected value of the new value of <span class="math inline">\(\w\)</span> is <span class="math display">\[\begin{aligned}
{\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\text{new \w}\right]} &amp; = \underbrace{\frac{1}{m}}_{{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\text{$i$
      chosen}\right]}} \sum_{i=1}^m \underbrace{\left( \w - \alpha \cdot
    \grad f_i(\w) \right)}_{\text{new $\w$ if $i$ chosen}}\\
&amp; = \w - \alpha \cdot \grad f,\end{aligned}\]</span> where the expectation is over the random choice of <span class="math inline">\(i\)</span>. That is, the expected effect of one iteration of stochastic gradient descent is exactly the same as the (deterministic) effect of one iteration of basic gradient descent!</p>
<p>Rather than independently choosing an index <span class="math inline">\(i\)</span> each iteration, standard practice is to order the data points in some way and perform one or more passes over them in this order. The order <span class="math inline">\(i=1,2,\ldots,m\)</span> is often used, or if you want to be safe you can randomly order the points. Summarizing:</p>
<p>initialize <span class="math inline">\(\w := \w_0\)</span></p>
<p>Each iteration of the main loop (i.e., each pass over the coordinates) is called an <span><em>epoch</em></span>. The number of epochs can be as small as one, or more can be used, computation time permitting.</p>
<p>The primary benefit of stochastic gradient descent is clear: since only one data point is used, each iteration takes only <span class="math inline">\(O(n)\)</span> time. Thus, an entire epoch of stochastic gradient descent takes roughly the same amount of time as just one iteration of gradient descent. When there is a limited amount of computation available, the question then is: which performs better, <span class="math inline">\(k\)</span> epochs of stochastic gradient descent, or <span class="math inline">\(k\)</span> iterations of gradient descent? (Here <span class="math inline">\(k\)</span> is however large you can get away with the available computational resources and time constraints.) Very commonly, the answer is the former, and stochastic gradient descent winds up being a big win. Stochastic gradient descent (and optimized variants of it) is pretty much the dominant paradigm in modern large-scale machine learning.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p>A couple of implementation notes. Because of the variance involved in stochastic gradient descent — in a given iteration, a poor choice of a data point can lead you in the completely incorrect direction — the step size <span class="math inline">\(\alpha\)</span> is often set to a smaller value than in gradient descent. If the step size is too large, then stochastic gradient descent can fail to converge. Second, in practice one often interpolates between the extreme cases of batch gradient descent (where all <span class="math inline">\(m\)</span> data points are used every iteration) and stochastic gradient descent (where only one data point is used per iteration) using “mini-batches.” This just means that, each iteration, the gradient terms are computed for a small number (e.g., 128) of data points, with the average of these terms used to compute the next point. With a properly vectorized implementation, this can be a nearly costless way to decrease the variance in the direction moved each iteration.</p>
<h1 id="increasing-the-number-of-dimensions">Increasing the Number of Dimensions</h1>
<h2 id="ss:logistic">Solving the Right Problem</h2>
<p>So far we’ve discussed computational issues — how to actually implement gradient descent so that it scales to very large data sets. We’ve been taking the function <span class="math inline">\(f\)</span> as given (e.g., as MSE). How do we know that we’re minimizing the “right” <span class="math inline">\(f\)</span>?</p>
<p>One reason that minimizing MSE might be the wrong optimization problem is because of simple typechecking errors. The point of linear regression is to predict real-valued labels for data points. But what if we don’t want to predict a real value? For example, what if we want to predict a binary value, like whether or not a given email is spam? There turn out to be analogs of linear regression and the mean-squared error objective for several other prediction tasks. For example, for binary classification, the most common approach is called “logistic regression,” where the linear prediction functions we’ve been studying are replaced by functions bounded between 0 and 1,<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> and the analog of mean-squared error is called “log loss.” You can learn much more about other prediction tasks in a machine learning course like CS229, and we won’t duplicate that material here. The template for modeling and implementing these other prediction tasks follows the exact same one used here for linear regression, so you’re now well-positioned for a deeper study.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>Even if your goal is to predict real-valued labels for points, linear regression might not be good enough. To understand the issue, consider Figure [f:quadratic], which illustrates an example with <span class="math inline">\(n=1\)</span>. There is no line that is a good fit for these data points. So while linear regression will indeed result in the linear function with the minimum-possible MSE, this optimal MSE will be large. On the other hand, it’s clear that there is a quadratic function that does fit these data points quite closely. This motivates computing the best predictor from a class of functions that includes quadratic functions and possibly higher-order polynomials.</p>
<div class="figure">
<embed src="quadratic.pdf" />
<p class="caption">A point set where there is no linear prediction function with small MSE, but there is a quadratic prediction function with small MSE.<span data-label="f:quadratic"></span></p>
</div>
<h2 id="encoding-nonlinear-relationships-with-extra-dimensions">Encoding Nonlinear Relationships with Extra Dimensions</h2>
<p>There is a slick way of adapting the tools we’ve developed to the case of nonlinear prediction functions. The idea is to <span><em>increase the number of dimensions</em></span> in the data points.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> For instance, with <span class="math inline">\(n=1\)</span>, consider mapping each data point <span class="math inline">\(x^{(i)}
\in \RR\)</span> to a <span class="math inline">\((d+1)\)</span>-dimensional vector: <span class="math display">\[x \mapsto \hat{\x} = (1,x,x^2,\ldots,x^d).\]</span> For example, if <span class="math inline">\(d=4\)</span>, a point with value 3 is mapped to the 5-tuple <span class="math inline">\((1,3,9,81,243)\)</span>.</p>
<p>Now imagine solving the linear regression problem not with the original data points <span class="math inline">\(x^{(1)},\ldots,x^{(m)}\)</span>, but with their expanded versions <span class="math inline">\(\hat{\x}^{(1)},\ldots,\hat{\x}^{(m)}\)</span>. The result is a linear function in <span class="math inline">\(d+1\)</span> dimensions, specified by coefficients <span class="math inline">\(w_0,\ldots,w_d\)</span>. The prediction of <span class="math inline">\(\w\)</span> for an expanded data point <span class="math inline">\(\hat{\x}^{(i)}\)</span> is <span class="math inline">\(\w^T\hat{\x}^{(i)} = \sum_{j=0}^d
w_j(x^{(i)})^j\)</span>. We can interpret the number <span class="math inline">\(\sum_{j=0}^d
w_j(x^{(i)})^j\)</span> as a nonlinear prediction for the original data point <span class="math inline">\(x^{(i)}\)</span>. For example, the prediction <span class="math inline">\(y = x^2-x+2\)</span> (for the original one-dimensional points) corresponds to the coefficient vector <span class="math inline">\(\w=(2,-1,2,0,0,\ldots,0)\)</span>. Linear regression in the expanded space minimizes the MSE of any linear function of the <span class="math inline">\(\hat{\x}^{(i)}\)</span>, and therefore the MSE of any degree-<span class="math inline">\(d\)</span> polynomial function of the <span class="math inline">\(x^{(i)}\)</span>’s.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> For <span class="math inline">\(n &gt; 1\)</span>, one can analogously preprocess the data points to add one extra coordinate for each monomial of degree at most <span class="math inline">\(d\)</span> (e.g., for <span class="math inline">\(d=2\)</span>, all products of pairs of the coordinates of a data point).<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p>
<p>There are many other ways to take data points in their “natural” representation and expand them with additional dimensions. For example, with documents, in addition to having one dimension for each word (indicating the number of occurrences), one can have one dimension for each ordered pair of words (tracking how many times they occur consecutively) or for each “<span class="math inline">\(n\)</span>-gram” (a sequence of <span class="math inline">\(n\)</span> consecutive characters, which may or may not form a word). Similarly, for images, in addition to having one dimension per pixel, it’s common to add dimensions corresponding to “patches,” meaning small grids of adjacent pixels (e.g., 6-by-6 or 8-by-8 grids). One way to translate a patch into a real-valued attribute is to compute its distance (Euclidean, say) from one or more reference patches (e.g., one of blue sky, or one of an edge separating different objects). For both documents and images, these augmentations can drive the number of dimensions <span class="math inline">\(n\)</span> into the millions or billions.</p>
<h2 id="ss:overfitting">Overfitting</h2>
<p>We mentioned how more data is always better, computational resources permitting. Are more features always better? The benefit is intuitively clear — better accuracy of fit (as in Figure [f:quadratic]). But there is a catch: with many features, there is a strong risk of <span><em>overfitting</em></span> — of learning a prediction function that is an artifact of the specific data set, rather than one that captures something more fundamental. Thus there is a trade-off between expressivity and predictive power.</p>
<p>To understand the issue, let’s return to the case of <span class="math inline">\(n=1\)</span> and the map <span class="math inline">\(x \mapsto \hat{\x} = (1,x,x^2,\ldots,x^d)\)</span>. Suppose we take <span class="math inline">\(d=m\)</span>, so that we are free to use polynomials with degree equal to the number of data points. Now we can get a mean-squared error of <span><em>zero</em></span>! The reason is just polynomial interpolation — given <span class="math inline">\(m\)</span> pairs <span class="math inline">\((x^{(1)},y^{(1)}),\ldots,(x^{(m)},y^{(m)})\)</span>, there is always a degree-<span class="math inline">\(m\)</span> polynomial that passes through all of them (Figure [f:poly]). Is a MSE of zero good? Not necessarily. The polynomial in Figure [f:poly] is quite “squiggly,” and meanwhile there is a line that fits the points quite closely. If the true relationship between the <span class="math inline">\(x^{(i)}\)</span>s and <span class="math inline">\(y^{(i)}\)</span>s is indeed roughly linear (plus some noise), then the line will give much more accurate predictions on new data points <span class="math inline">\(x\)</span> than the squiggly polynomial. In machine learning terminology, the squiggly polynomial fails to “generalize.” Remember that the point of this exercise is to learn something “general,” meaning a prediction function that transcends the particular data set and remains approximately accurate even for data points that you’ve never seen.</p>
<div class="figure">
<embed src="poly.pdf" />
<p class="caption">Using a high-degree polynomial to achieve zero MSE can result in a squiggly polynomial, even when there is a linear function with low MSE.<span data-label="f:poly"></span></p>
</div>
<h1 id="regularization-the-case-of-large-n">Regularization: The Case of Large <span class="math inline">\(n\)</span></h1>
<h2 id="occams-razor">Occam’s Razor</h2>
<p>It is common in modern machine learning to take a “kitchen sink” approach to defining the features of data points — throwing in every conceivably useful feature that you can think of, and taking <span class="math inline">\(n\)</span> as large as possible. With this approach, it is essential to be on guard against overfitting. Philosophically, the solution is <span><em>Occam’s Razor</em></span> — to be biased toward simpler models, on the basis that they are capturing something more fundamental, rather than some artifact of the specific data set.</p>
<p><span><em>Regularization</em></span> is a concrete method for implementing the principle of Occam’s razor. The idea is to add a “penalty term” to the optimization problem, such that more complex models incur a larger penalty. For the case of linear regression, the new optimization problem is to minimize <span class="math display">\[\text{MSE(\w)} + \text{penalty}(\w),\]</span> where <span class="math inline">\(\text{penalty}(\w)\)</span> is increasing with the “complexity” of <span class="math inline">\(\w\)</span>. Thus a complex solution will be chosen over a simple solution only if it leads to a big decrease in the mean-squared error.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<h2 id="l_2-regularization"><span class="math inline">\(L_2\)</span> Regularization</h2>
<p>There are many ways to define the penalty term. We’ll just consider the most widely used one, which has many names: <span><em>ridge regression</em></span>, <span><em><span class="math inline">\(L_2\)</span> regularization</em></span>, or <span><em>Tikhonov regularization</em></span>. This just means that we set <span class="math display">\[\text{penalty}(\w) = \lambda \cdot {
{\| {\w} \|^2}
}_2,\]</span> where <span class="math inline">\(\lambda\)</span> is a positive “hyperparameter,” a knob that allows you to trade-off smaller MSE (preferred for small <span class="math inline">\(\lambda\)</span>) versus smaller model complexity (preferred for large <span class="math inline">\(\lambda\)</span>).<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> That is, we identify “complex functions” with those with large weights.<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> For example, most of the current work in deep learning uses this form of regularization.<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p>
<p>To see how this addresses the overfitting problem discussed in Section [ss:overfitting] (Figure [f:poly]), we note that a degree-<span class="math inline">\(m\)</span> polynomial passing through <span class="math inline">\(m\)</span> points is likely to have all non-zero coefficients, including some large coefficients. A linear function has mostly zero coefficients, and will be preferred over the squiggly polynomial for data points with an approximately linear relationship (unless <span class="math inline">\(\lambda\)</span> is very small).<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p>
<h2 id="applying-gradient-descent">Applying Gradient Descent</h2>
<p>All of the ideas covered in these two lectures extend easily to regularized regression. In our running example of linear regression with <span class="math inline">\(L_2\)</span> regularization, the objective is to minimize <span class="math display">\[f(\w) = \frac{1}{m} \sum_{i=1}^m E_i(\w)^2 + \lambda \sum_{j=1}^n w_j^2.\]</span> Since we’ve only added some new squared terms to the previous (convex) MSE objective function, we again have a convex function with only one local minimum (the global minimum). The gradient is the same as before (Section [s:recap]), except with an extra <span class="math inline">\(2\lambda w_j\)</span> term in each coordinate <span class="math inline">\(j\)</span>: <span class="math display">\[\frac{\del f}{\del w_j}(\w) = \left( \frac{1}{m} \sum_{i=1}^m 2E_i(a)
  \cdot x^{(i)}_j \right) + 2\lambda w_j.\]</span> The interpretation of this new gradient is the same as before (a weighted average of votes by the data points), except that the new term also exerts a force pushing coefficients closer to 0, with greater force applied to the coefficients with larger magnitudes.</p>
<p>Gradient descent requires essentially the same amount of computation as in the unregularized linear regression problem, and remains equally easy to parallelize. For the stochastic gradient descent version, a single data point <span class="math inline">\(\x^{(i)}\)</span> is chosen each iteration and the update used is just <span class="math display">\[w_j := w_j - \alpha \cdot \left( 2E_i(a)
  \cdot x^{(i)}_j + 2\lambda w_j \right)\]</span> for each coordinate <span class="math inline">\(j\)</span>. (The <span class="math inline">\(2\lambda w_j\)</span> term is always included, no matter which data point <span class="math inline">\(\x^{(i)}\)</span> is chosen.)</p>
<h1 id="lecture-take-aways">Lecture Take-Aways</h1>
<ol>
<li><p>For many machine learning problems, replacing the basic gradient descent method by stochastic gradient descent is crucial for accommodating very large data sets. While the former touches every data point every iteration (to average the corresponding gradient terms), the latter uses only one (or a small number) of data points in each iteration. Stochastic gradient descent is the dominant paradigm in modern machine learning (e.g., in most deep learning work).</p></li>
<li><p>More data is always better, as long as you have the computational resources to handle it.</p></li>
<li><p>More features (or dimensions) offer a trade-off, allowing more expressive power at the risk of overfitting the data set. Still, these days the prevailing trend is to include as many features as possible.</p></li>
<li><p>Regularization is key to pushing back on overfitting risk with high-dimensional data. The general idea is to trade off the “complexity” of the learned model (e.g., the magnitudes of weights of a linear prediction function) with its error on the data set. The goal of learning the simplest model with good explanatory power, on the basis that this explanation is the most likely to generalize to unseen points.</p></li>
<li><p>Adding regularization imposes essentially no extra computational demands on (stochastic) gradient descent.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>What if you don’t know the gradient? (E.g., on the QWOP project on Homework #3.) Remember that each coordinate of the gradient of <span class="math inline">\(f\)</span> is the instantaneous rate of change of <span class="math inline">\(f\)</span> as the <span class="math inline">\(i\)</span>th coordinate is varied. So you can just estimate each coordinate <span class="math inline">\(\tfrac{\del f}{\del w_j}(\w)\)</span> by <span class="math inline">\([f(w_1,\ldots,w_{i-1},w_i+\eta,w_{i+1},\ldots,w_n)-f(\w)]/\eta\)</span> for small <span class="math inline">\(\eta\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>This would seem to prohibit the linear function from having an intercept — i.e., it is forced to map <span class="math inline">\(\zero\)</span> to <span class="math inline">\(\zero\)</span>. This is for convenience and without loss of generality: to encode an intercept, preprocess the data points, adding a new first coordinate with <span class="math inline">\(x^{(i)}_1 = 1\)</span> for every data point <span class="math inline">\(\x^{(i)}\)</span> (so coordinates <span class="math inline">\(j=2,3,\ldots,n\)</span> are the “real” ones). Then <span class="math inline">\(w_1\)</span> can encode whatever intercept you want.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Kinda frightening to think about what the “abnormal” equations might look like…<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Thus there are two different bottlenecks to better machine learning: the data bottleneck, and the computation bottleneck. The major advances in applied machine learning over the past 5-10 years are largely attributable to groups that simultaneously have both massive amounts of data and also the massive computational resources required to process it (think Google Brain).<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>In this context, the basic version of gradient descent (Section [s:recap]) is sometimes called <span><em>batch</em></span> gradient descent, as the whole batch of data points is used for every update step.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>For example, when you hear buzzwords like “backpropagation in neural networks,” it’s typically just referring to an efficient implementation of stochastic gradient descent.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Specifically, <span class="math inline">\(\x \mapsto 1/(1+\exp(-\w^T\x))\)</span> rather than <span class="math inline">\(\x \mapsto \w^T\x\)</span>.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>One exception: many of the other tasks, including logistic regression, do not admit a closed-form solution analogous to the normal equations. Thus gradient descent is even more relevant for such tasks than for linear regression.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>We already saw a simple example of this principle, when we showed how linear functions in <span class="math inline">\(n-1\)</span> dimensions with an intercept can be encoded using linear functions in <span class="math inline">\(n\)</span> dimensions with no intercept (by adding a “dummy” coordinate in which points have value 1).<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>How can we do nonlinear computations using a linear function? Because the nonlinear part is carried out in the preprocessing step of mapping each <span class="math inline">\(x\)</span> to <span class="math inline">\(\hat{\x}\)</span>. Given <span class="math inline">\(\hat{\x}\)</span>, the prediction of its label is just a linear function of the coefficient vector <span class="math inline">\(\w\)</span>.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>You might be concerned about the number of additional coordinates that this idea creates as <span class="math inline">\(n\)</span> and <span class="math inline">\(d\)</span> grow large. If you study machine learning more deeply, you’ll learn about the “kernel trick,” which allows you to deduce the result of certain computations (such as a gradient descent update step) on the expanded data points (the <span class="math inline">\(\hat{\x}^{(i)}\)</span>’s) without ever explicitly constructing these expanded representations.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>The same idea can be used for other types of regression problems, like the logistic regression problem mentioned in Section [ss:logistic].<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>“Hyperparameter” meaning a parameter that is chosen outside the learning procedure. (Whereas the <span class="math inline">\(w_j\)</span>’s, which are computed by the learning procedure and define the learned model, are simply “parameters.”) So how is <span class="math inline">\(\lambda\)</span> chosen? Typically, just by trial and error, to find the value that results in the learned model with the most predictive power. (Or, if you want to sound fancy, you can refer to this trial-and-error as “hyperparameter optimization” or “hyperparameter tuning.”)<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>A detail: one generally doesn’t penalize the weight that corresponds to the function’s intercept (the first coordinate in the setup in Section [s:recap]), just the weights corresponding to the “real” coordinates of the data points.<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>The second-most popular choice of penalty term is probably the <span class="math inline">\(\ell_1\)</span> norm of <span class="math inline">\(\w\)</span> (i.e., <span class="math inline">\(\text{penalty}(\w) = \lambda \cdot \|\w\|_1\)</span>). This is naturally called <span><em><span class="math inline">\(L_1\)</span> regression</em></span>, and the objective function is also referred to as “the lasso.”<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>Regularization can also be useful in low-dimensional problems, for example to reduce the sensitivity of the computed prediction function to outliers.<a href="#fnref16">↩</a></p></li>
</ol>
</div>

{% endraw %}
