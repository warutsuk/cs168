---
layout: post
title: Lecture 19
author: Tim
---
{% raw %}

<p>In the first lecture of CS168, we talked about modern techniques in data storage (consistent hashing). Since then, the major course themes have been about the representation, analysis, interpretation, and visualization of data. Today we’ll close the circle by returning to the “nuts and bolts” of managing data, specifically to data transmission (using error-correcting codes).</p>
<h1 id="expanders">Expanders</h1>
<h2 id="definitions-and-examples">Definitions and Examples</h2>
<p>We begin with a question:</p>
<ul>
<li><p>Can a graph be simultaneously “sparse” and “highly connected?”</p></li>
</ul>
<p>The first half of the lecture explains an affirmative answer to this question. The second half connects the question to the design of error-correcting codes.</p>
<p>What exactly do we mean by “sparse” and “highly connected?” The answers are familiar. One common definition of a sparse graph, as seen in CS161, is that the number <span class="math inline">\(m\)</span> of edges is <span class="math inline">\(O(n)\)</span>, where <span class="math inline">\(n\)</span> is the number of vertices. In this lecture we’ll impose a stronger condition, that every vertex has <span class="math inline">\(O(1)\)</span> degree. This implies that there are only <span class="math inline">\(O(n)\)</span> edges (why?), while the converse is not true (e.g., consider a star graph).</p>
<p>Intuitively, we want “highly connected” to translate to “acts like a clique,” since a clique is clearly the most well connected graph. We’d like a definition that differentiates and interpolates between extreme cases like cliques and paths (Figure [f:expanders]). Back in Week 6, we saw two essentially equivalent definitions of “highly connected,” one combinatorial and one algebraic.</p>
<p>To recap, recall that the <span><em>isoperimetric ratio</em></span> of a graph <span class="math inline">\(G=(V,E)\)</span> is <span class="math display">\[\min_{S \sse V \,:\, |S| \le |V|/2} \frac{|\delta(S)|}{|S|}.\]</span> The notation <span class="math inline">\(\delta(S)\)</span> refers to the “boundary” of <span class="math inline">\(S\)</span> — the edges with exactly one endpoint in <span class="math inline">\(S\)</span>. Thus the isoperimetric ratio is large if and only if every subset of at most half the vertices has a large “surface area to volume” ratio. For example, the isoperimetric ratio of the clique is <span class="math inline">\(\approx \tfrac{n}{2}\)</span>, while that of the path is <span class="math inline">\(\tfrac{2}{n}\)</span>.</p>
<p>The second definition involves <span class="math inline">\(\lambda_2\)</span>, the second-smallest eigenvalue of the graph’s Laplacian matrix.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Computations show that <span class="math inline">\(\lambda_2\)</span> is large (like <span class="math inline">\(n\)</span>) for the clique but small (like <span class="math inline">\(\Theta(1/n^2)\)</span>) for the path. Thus both definitions agree that the connectivity measure is going to infinity with <span class="math inline">\(n\)</span> for cliques and going to 0 with <span class="math inline">\(n\)</span> for paths. More generally, in Lecture #12 we noted <span><em>Cheeger’s inequality</em></span>, which implies that one of these quantities is large if and only if the other quantity is large.</p>
<p>For <span class="math inline">\(d\)</span>-regular graphs — where every vertex has degree <span class="math inline">\(d\)</span> — the isoperimetric ratio is at most <span class="math inline">\(d\)</span>. (Since <span class="math inline">\(\delta(S)\)</span> can at most contain all of the <span class="math inline">\(d|S|\)</span> edges incident to vertices in <span class="math inline">\(S\)</span>.) In Week 6 we saw that the same is true for <span class="math inline">\(\lambda_2\)</span>. So when we restrict attention to sparse (constant-degree) graphs, we will not see these quantities tend to infinity with <span class="math inline">\(n\)</span>, as they do for cliques. So a <span class="math inline">\(d\)</span>-regular graph should be considered “highly connected” if these quantities are close to the obvious upper bound of <span class="math inline">\(d\)</span>.</p>
<p>Formally, a family of <span><em>expander graphs</em></span> is one such that the isoperimetric ratio and <span class="math inline">\(\lambda_2\)</span> do not tend to 0 as the number <span class="math inline">\(n\)</span> of vertices tends to infinity — i.e., they stay bounded away from 0 as <span class="math inline">\(n \rightarrow \infty\)</span>. (Recall that this holds for one of the measures if and only if it holds for the other measure.) Thus cliques are expanders while paths are not, as one would hope.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<h2 id="existence-of-expanders">Existence of Expanders</h2>
<p>We know that expanders exist, since cliques are expanders. But do <span><em>sparse</em></span> expanders exist?<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> You should not have any a priori intuition about this question. Certainly 1-regular graphs (i.e., perfect matchings) and 2-regular graphs (collections of cycles) can never be expanders. This makes the following fact all the more amazing.</p>
<p>[fact:expanders] For every <span class="math inline">\(d \ge 3\)</span>, there exists a family of <span class="math inline">\(d\)</span>-regular expander graphs.</p>
<p>In fact, <span><em>almost every</em></span> <span class="math inline">\(d\)</span>-regular graph with <span class="math inline">\(d \ge 3\)</span> is an expander! This is proved using the “probabilistic method,” which is a very simple but very cool idea developed by Paul Erdös (see <span class="citation"></span>). The most straightforward way of proving the existence of an object with desired properties is to simply exhibit the object. In the probabilistic method, one instead defines, as a thought experiment, a random experiment that produces an object (like a graph). Then one proves that the produced object has the desired property (like expansion) with positive probability over the random choices in the experiment. Note that this implies existence — if no objects with the desired property exist, then certainly the probability of randomly selecting an object with the desired property is zero!</p>
<p>For Fact [fact:expanders], then, we want to define a random experiment that produces a <span class="math inline">\(d\)</span>-regular graph, and that generates an expander with positive probability. There are several ways this can be done; for example, with an even number <span class="math inline">\(n\)</span> of vertices, one can choose <span class="math inline">\(d\)</span> perfect matchings (i.e., group all vertices into pairs), independently and uniformly at random, and superimpose them to obtain a <span class="math inline">\(d\)</span>-regular graph. One can prove that, for every <span class="math inline">\(d \ge 3\)</span>, this process has a positive probability of generating an expander.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> And it turns out that this probability is not just bigger than 0, but is actually very close to 1.</p>
<p>What about “explicit” constructions of expander graphs, in the form of a deterministic and computationally efficient algorithm that generates “on demand” an expander graph of a given size? These are much harder to obtain. The first explicit constructions are from the 1980s, where the constructions are simple to state (using just the simplest-possible number theory) but require difficult mathematics to analyze. In the 21st century, there have been several iterative constructions for which the expansion requirement can be verified using elementary linear algebra (see <span class="citation"></span>).<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>What good are expander graphs? In situations where you have the luxury of choosing your own graph, expander graphs often come in handy. They were first considered in the mid-20th century in the context of telecommunication network design — in that context, it’s clear that both sparsity and rich connectivity (i.e., robustness to failures) are desirable properties. We next study an application in which expander graphs are not physically realized as networks, but rather a bipartite variant is used logically to define good error-correcting codes.</p>
<h1 id="expander-codes">Expander Codes</h1>
<h2 id="error-correcting-codes">Error-Correcting Codes</h2>
<p><span><em>Error-correcting codes</em></span> address the problem of encoding information to be robust to errors (i.e., bit flips). We consider only binary codes, so the objects of study are <span class="math inline">\(n\)</span>-bit vectors.</p>
<p>The simplest error-correcting code uses a single parity bit. This involves appending 1 bit to the end of message, a 0/1 as needed to ensure that the resulting string has even parity (i.e., an even number of 1s). So we would encode <span class="math display">\[100 \mapsto 1001\]</span> and <span class="math display">\[101 \mapsto 1010.\]</span> The corresponding <span><em>code</em></span> — that is, the legitimate codewords — is then <span class="math inline">\(C = \{ {\mathbf{x}}\in \{0,1\}^n \,:\, {\mathbf{x}}\text{ has even parity} \}\)</span>.</p>
<p>The key point is that the <span><em>Hamming distance</em></span> — the number of differing coordinates — of two distinct codewords is at least 2: if you flip one bit of a codeword, then you also need to flip a second bit to obtain a string with even parity. This means that the <span><em>distance</em></span> of the code — the minimum Hamming distance between two distinct codewords — is precisely 2.</p>
<p>A code that has distance 2 can detect one error (i.e., a bit flip): if exactly one bit gets flipped, then the receiver will notice that the received (corrupted) message is not a valid codeword, and can therefore ask the sender to retransmit the message. With a parity bit, an odd number of errors will always be detected, while an even number of errors will never be detected (since it results in a new legitimate codeword).</p>
<p>Generalizing, if a code has distance <span class="math inline">\(d\)</span>, up to <span class="math inline">\(d-1\)</span> adversarial errors can be detected. If the number of errors is less than <span class="math inline">\(d/2\)</span>, then no retransmission is required: there is a unique codeword closest (in Hamming distance) to the transmission, and it is the message originally sent by the receiver.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> That is, the corrupted transmission can be <span><em>decoded</em></span> by the receiver.</p>
<p>How can we generalize the parity bit idea to obtain codes with larger distance? To be useful, it is also necessary to have computationally efficient algorithms for encoding messages and decoding corrupted codewords.</p>
<h2 id="from-graphs-to-codes">From Graphs to Codes</h2>
<p>Every bipartite graph <span class="math inline">\(G=(V,C,E)\)</span> can be used to define an error-correcting code (Figure [f:pc](a)). The left-hand side vertices <span class="math inline">\(V\)</span> correspond to the <span class="math inline">\(n\)</span> coordinates or variables of a code word. Every right-hand vertex <span class="math inline">\(j
\in C\)</span> corresponds to a “parity check.” More formally, a vector <span class="math inline">\({\mathbf{x}}\in \{0,1\}^n\)</span> <span><em>satisfies</em></span> the parity check <span class="math inline">\(j \in C\)</span> if <span class="math inline">\({\mathbf{x}}_{N(j)}\)</span> has even parity, where <span class="math inline">\(N(j) \sse V\)</span> denotes the neighbors of vertex <span class="math inline">\(j\)</span> in <span class="math inline">\(G\)</span> and <span class="math inline">\({\mathbf{x}}_I\)</span> denotes the vector <span class="math inline">\({\mathbf{x}}\)</span> projected onto the coordinates of the set <span class="math inline">\(I\)</span>. The code corresponding to <span class="math inline">\(G\)</span> is, by definition, the vectors <span class="math inline">\({\mathbf{x}}\in
\{0,1\}^n\)</span> that satisfy every parity check of <span class="math inline">\(C\)</span>. For instance, the code generated by using a single parity bit corresponds to the graph in which <span class="math inline">\(C\)</span> is a single vertex, connected to all of the variables (both the original bits and the parity bit) — see Figure [f:pc](b). For a simple extension, we could imagine having one parity check for the first half of the coordinates and a second for the second half (Figure [f:pc](c)). With this code, two errors might or might not be detected (depending on whether they occur in different halves or the same half, respectively).</p>
<p>Every bipartite graph defines a code, but some graphs/codes are better than others. The goal of this lecture is to identify conditions on the graph <span class="math inline">\(G\)</span> so that it is possible to correct many errors (a constant fraction, even), and moreover in linear time.</p>
<p>Following <span class="citation"></span>, we propose the following conditions.</p>
<ul>
<li><p>Every left-hand side vertex <span class="math inline">\(i \in V\)</span> has exactly <span class="math inline">\(d\)</span> neighbors, where <span class="math inline">\(d\)</span> is a constant (like 10).</p></li>
<li><p>Every right-hand side vertex <span class="math inline">\(j \in C\)</span> has at most a constant (like 20) number of neighbors.</p></li>
<li><p>There is a constant <span class="math inline">\(\delta &gt; 0\)</span> (like 0.1), independent of <span class="math inline">\(n\)</span>, such that for all subsets <span class="math inline">\(S \sse V\)</span> with size <span class="math inline">\(|S|\)</span> at most <span class="math inline">\(\delta n\)</span>, <span class="math display">\[\label{eq:expand}
|N(S)| \ge \frac{3}{4} d |S|,\]</span> where <span class="math inline">\(N(S) \sse C\)</span> denotes the vertices (i.e., parity checks) of <span class="math inline">\(C\)</span> that have at least one neighbor in <span class="math inline">\(S\)</span>. Intuitively, this expansion condition implies that a bunch of errors (represented by <span class="math inline">\(S\)</span>) must be reflected in tons of different parity checks (corresponding to <span class="math inline">\(N(S)\)</span>). See Figure [f:expand].</p></li>
</ul>
<div class="figure">
<embed src="expand.pdf" />
<p class="caption">The expansion condition (C3). Every small enough subset <span class="math inline">\(S \subset V\)</span> must have lots of distinct neighbors in <span class="math inline">\(C\)</span>.<span data-label="f:expand"></span></p>
</div>
<p>In the inequality , we use <span class="math inline">\(N(S)\)</span> to denote the vertices of <span class="math inline">\(C\)</span> that have at least one neighbor in <span class="math inline">\(S\)</span>. Observe that by (C1), the number of edges sticking out of <span class="math inline">\(S\)</span> is only <span class="math inline">\(d|S|\)</span>, so <span class="math inline">\(|N(S)|\)</span> is certainly at most <span class="math inline">\(d|S|\)</span>. The condition  asserts that the number of distinct neighbors of <span class="math inline">\(S\)</span> is almost as large as the trivial upper bound, and moreover this holds for <span> <em>every</em></span> one of the exponentially many choices for <span class="math inline">\(S\)</span>. This sounds like a strong property, so you would be right to question if there really exist any graphs that satisfy (C3). Again using the probabilistic method, it is not too hard to prove that a random graph that satisfies (C1) and (C2) also satisfies (C3) with high probability. Before addressing encoding and decoding, we establish next the fact that expander codes have large distance (linear in <span class="math inline">\(n\)</span>) and hence can detect/correct codewords where a constant fraction of the bits have been corrupted. The point of going over this proof is to develop intuition for why the expansion condition (C3) leads to good codes.</p>
<p>[t:dist] A code satisfying conditions (C1)–(C3) has distance at least <span class="math inline">\(\delta
n\)</span>, where <span class="math inline">\(\delta &gt; 0\)</span> is the same constant as in condition (C3).</p>
<p>Let <span class="math inline">\({\mathbf{w}}\)</span> be a code word, and let <span class="math inline">\({\mathbf{x}}\in \{0,1\}^n\)</span> be a vector with <span class="math inline">\(d_H({\mathbf{w}},{\mathbf{x}}) &lt; \delta n\)</span>. We need to show that <span class="math inline">\({\mathbf{x}}\)</span> is not a code word.</p>
<p>Let <span class="math inline">\(S\)</span> denote the <span class="math inline">\(d_H({\mathbf{w}},{\mathbf{x}})\)</span> coordinates in which <span class="math inline">\({\mathbf{w}}\)</span> and <span class="math inline">\({\mathbf{x}}\)</span> differ. We claim that there exists a parity check <span class="math inline">\(j \in C\)</span> that involves exactly one coordinate of <span class="math inline">\(S\)</span>. Observe that this claim implies the proposition: since <span class="math inline">\({\mathbf{w}}\)</span> is a code word, it satisfies <span class="math inline">\(j\)</span>, and since exactly one of the coordinates of <span class="math inline">\(j\)</span> is flipped in <span class="math inline">\({\mathbf{x}}\)</span>, relative to <span class="math inline">\({\mathbf{w}}\)</span>, <span class="math inline">\({\mathbf{x}}\)</span> does not satisfy the parity check <span class="math inline">\(j\)</span>. Hence <span class="math inline">\({\mathbf{x}}\)</span> is not a code word.</p>
<p>To prove the claim, note that condition (C1) implies that there are precisely <span class="math inline">\(d|S|\)</span> edges sticking out of the vertices of <span class="math inline">\(G\)</span> that correspond to <span class="math inline">\(S\)</span> (Figure [f:dist]). Since <span class="math inline">\(|S| &lt; \delta n\)</span>, condition (C3) implies that <span class="math inline">\(|N(S)| \ge \tfrac{3}{4}d|S|\)</span>. That is, at least 75% of the edges sticking out of <span class="math inline">\(S\)</span> go to distinct vertices of <span class="math inline">\(C\)</span>. This leaves at most 25% of the edges with the capability of donating a second neighbor of <span class="math inline">\(S\)</span> to a parity check in <span class="math inline">\(N(S)\)</span>. It follows that at least <span class="math display">\[\left( \frac{3}{4} - \frac{1}{4} \right) d|S|\]</span> vertices of <span class="math inline">\(N(S)\)</span> have a unique neighbor in <span class="math inline">\(S\)</span>, which completes the claim and the proof.</p>
<div class="figure">
<img src="dist" alt="Proof of Theorem [t:dist]. Because d|S| edges go to at least \tfrac{3}{4}d|S| different vertices, at least \tfrac{1}{2}d|S| of the vertices in N(S) are connected to exactly one vertex of S." />
<p class="caption">Proof of Theorem [t:dist]. Because <span class="math inline">\(d|S|\)</span> edges go to at least <span class="math inline">\(\tfrac{3}{4}d|S|\)</span> different vertices, at least <span class="math inline">\(\tfrac{1}{2}d|S|\)</span> of the vertices in <span class="math inline">\(N(S)\)</span> are connected to exactly one vertex of <span class="math inline">\(S\)</span>.<span data-label="f:dist"></span></p>
</div>
<p>What role does expansion play in the proof of Theorem [t:dist]? For a graph-based code to have small distance, it must be the case that starting from a codeword and flipping a small number of bits causes, for each parity check, an even number of flips among the variables in that check. The expansion condition asserts that there is minimal overlap between the parity checks in which two different variables participate, and this rules out such a “conspiracy of cancellations.”</p>
<h2 id="encoding">Encoding</h2>
<p>How does one actually use an expander code to do encoding and decoding? Encoding a message <span class="math inline">\({\mathbf{y}}\)</span> is done via matrix-vector multiplication — i.e., the codeword is <span class="math inline">\({\mathbf{Ay}}\)</span>, where <span class="math inline">\({\mathbf{A}}\)</span> is a suitable matrix with more rows than columns. This is the generic encoding procedure for any linear code — i.e., where the sum modulo 2 of two codewords is again a codeword, as is the case for our codes (why?) — and it is not specific to expander codes. You can think of the columns of <span class="math inline">\({\mathbf{A}}\)</span> as a basis (in the linear algebra sense) of the vector space (over <span class="math inline">\(\mathbb{F}_2\)</span>) of codewords; then encoding just means interpreting the message <span class="math inline">\({\mathbf{y}}\)</span> as the coefficients for a linear combination of these basis codewords.</p>
<h2 id="decoding">Decoding</h2>
<p>What’s interesting about expander codes is their decoding properties. Decoding boils down to solving the <span><em>nearest codeword</em></span> problem. The input is a (corrupted) message <span class="math inline">\({\mathbf{x}}\)</span>, with the promise that it has Hamming distance less than <span class="math inline">\(\delta n/2\)</span> from a legitimate codeword <span class="math inline">\({\mathbf{w}}\)</span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> The goal is to output <span class="math inline">\({\mathbf{w}}\)</span>.</p>
<p>We consider the following amazingly simple decoding algorithm.</p>
<p>=.5=1</p>
<p>With our running example of <span class="math inline">\(d=10\)</span>: if at least 6 of the 10 parity checks that include a variable are unsatisfied, then this variable is a candidate to be flipped.</p>
<p>This algorithm is guaranteed to terminate, no matter what the graph <span class="math inline">\(G\)</span> is. Why? When we flip a variable, the parity checks that contain it toggle between being satisfied and unsatisfied. So if pre-flip there were more unsatisfied than satisfied parity checks, post-flip the reverse is true. So every flip strictly increases the number of satisfied parity checks, and the algorithm must terminate within <span class="math inline">\(|C|\)</span> iterations. Additionally, the algorithm can be implemented in linear time.</p>
<p>The bigger issue is: how do we know that the algorithm terminates with the nearest codeword <span class="math inline">\({\mathbf{w}}\)</span>? The concern is that the algorithm terminates with a non-codeword that nevertheless satisfies at least 50% of the parity checks for each variable. And indeed, with an arbitrary choice of the graph <span class="math inline">\(G\)</span>, this failure case can occur.</p>
<p>This brings us to our second result.</p>
<p>[t:decode] If the graph <span class="math inline">\(G\)</span> satisfies conditions (C1) and (C3) and the corrupted code word <span class="math inline">\({\mathbf{x}}\)</span> has Hamming distance at most <span class="math inline">\(\delta n/2\)</span> from its nearest code word <span class="math inline">\({\mathbf{w}}\)</span>, then the decoding algorithm above is guaranteed to terminate at <span class="math inline">\({\mathbf{w}}\)</span>.</p>
<p>The proof is a slightly more elaborate version of the counting arguments we used to prove Theorem [t:dist].<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> The expansion condition again rules out the conspiring cancellations of a bunch of flipped variables that would be required for the algorithm to get stuck somewhere other than the nearest codeword <span class="math inline">\({\mathbf{w}}\)</span>.</p>
<p>When expander codes were first proposed in <span class="citation"></span>, there was interest from several media companies. For example, for encoding CDs and DVDs, the property of linear-time decodability is very attractive. The companies were concerned about how to choose the bipartite graph <span class="math inline">\(G\)</span>. The probabilistic method shows that a random graph would almost always work, but how can you be sure? Explicit constructions of sufficiently good expanders came along only in 2002 <span class="citation"></span>, and the inability to determinisitically find “hay in a haystack” was an initial impediment to the adoption of expander codes. These days, codes inspired by expander codes are indeed used in practice.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Recall this is the <span class="math inline">\(V \times V\)</span> matrix <span class="math inline">\(D - A\)</span>, where <span class="math inline">\(D\)</span> is the diagonal matrix with entries equal to vertex degrees, and <span class="math inline">\(A\)</span> is the graph’s adjacency matrix.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Among friends one might talk about “an expander graph,” but strictly speaking the definition of an expander only makes sense for an infinite family of graphs. Note that when we say “cliques” or “paths” we’re already talking about an infinite family, since there is one clique and path graph for each value of <span class="math inline">\(n\)</span>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Often when one talks about expanders it’s a given that you’re restricting to graphs of constant degree.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>For large enough <span class="math inline">\(d\)</span> (like 20), the proof is not hard, and it shows up as a homework problem in CS264 (“Beyond Worst-Case Analysis”).<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>The difficulty of explicit constructions is one of the paradoxes you just have to get used to in the study of theoretical computer science. While it is easy to believe that finding a needle in a haystack (a euphemism for the the <span class="math inline">\(P\)</span> vs. <span class="math inline">\(NP\)</span> problem) is hard, sometimes it’s hard to even find hay in a haystack! (Recall that almost all graphs are expanders…)<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>The Hamming distance satisfies the triangle inequality (why?), so if there are two different codewords with Hamming distance less than <span class="math inline">\(d/2\)</span> to a common string, then they have Hamming distance less than <span class="math inline">\(d\)</span> from each other.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Recall that <span class="math inline">\({\mathbf{w}}\)</span> is unique — if there were two different codewords both with Hamming distance less than <span class="math inline">\(\delta
  n/2\)</span> from <span class="math inline">\({\mathbf{x}}\)</span>, then the distance of the code would be less than <span class="math inline">\(\delta n\)</span>.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>It’s not too hard, and shows up as a homework problem in CS264.<a href="#fnref8">↩</a></p></li>
</ol>
</div>

{% endraw %}
