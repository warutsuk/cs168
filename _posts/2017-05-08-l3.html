---
layout: post
title: "Lecture 3 : Similarity Metrics and kd-Trees"
author: Tim
---
{% raw %}

<h1 id="similarity-search">Similarity Search</h1>
<p>We begin with the basic problem of how to organize/represent a dataset so that similar items can be found quickly. There are two slightly different settings in which one might want to consider this question:</p>
<ol>
<li><p>All the data is present, and one wants to find pairs or sets of similar items from within the dataset.</p></li>
<li><p>There is a reference dataset that one has plenty of time to process cleverly, but when we are given a <em>new</em> datapoint, we want to very quickly return a similar datapoint from the reference dataset. This setting is sometimes referred to as the “nearest neighbor search” setting.</p></li>
</ol>
<p>In general, similar techniques/approaches are used for the above two settings, though it is worth being aware of the the different objectives.</p>
<p>There are many real-world applications of similarity search:</p>
<ul>
<li><p>Similar documents, web pages, genomes, etc. (de-duplication of datasets, plagiarism detection in code/essays, detecting mirror web sites, finding similar genes or sequencing populations of organisms such as in the human gut “microbiome”).</p></li>
<li><p>Collaborative filtering: find similar products (based on whether the same set of people purchased them) or individuals (based on purchase history, demographics, web browsing behavior, etc.).</p></li>
<li><p>Machine learning/classification: If we find two similar datapoints, they might have the same label....</p></li>
<li><p>Combining datasets (e.g. in astronomy—different telescopes take pictures of the same portions of the sky, maybe in different wavelengths, etc.–very useful to automatically aggregate these datasets).</p></li>
<li><p>Super fast labeling: e.g. in CERN (which is back up and running as of yesterday), might want to very quickly figure out which particle traces/trajectories are “interesting” and worth saving, and which trajectories are just boring/common particles.</p></li>
</ul>
<h1 id="measures-of-similarity">Measures of Similarity</h1>
<p>Before talking about algorithms for finding similar objects, we should begin by considering several quantifications of what “similarity” means.</p>
<h2 id="jaccard-similarity">Jaccard Similarity</h2>
<p>Jaccard similarity, which we denote by <span class="math inline">\(J(\cdot,\cdot)\)</span>, is a distance metric between two sets (or two multisets—e.g. sets where elements are allowed to appear more than once) of objects, <span class="math inline">\(S,T\)</span>: <span class="math display">\[J(S,T) = \frac{|S \cap T|}{S \cup T}.\]</span> Equivalently, if we represent our sets (or multisets) <span class="math inline">\(S,T\)</span> as vectors <span class="math inline">\(v_S,v_T\)</span>, with the <span class="math inline">\(i\)</span>th index of the vector <span class="math inline">\(v_S(i)\)</span> equalling the number of times that the <span class="math inline">\(i\)</span>th element is represented in <span class="math inline">\(S\)</span>, the above definition becomes: <span class="math display">\[J(S,T) = J(v_S,v_T) = \frac{\sum_i \min(v_S(i),v_T(i))}{\sum_i \max(v_S(i),v_T(i))}.\]</span> This expression is undefined if <span class="math inline">\(S,T\)</span> are both the empty set, in which case we can define the distance to be 0.</p>
<p>Jaccard similarity works quite well in practice, especially for sparse data. For example, if we represent documents in terms of the multiset of words they contain, then the Jaccard similarity between two documents is often a reasonable measure of their similarity. Similarly, to estimate similarity between individuals, an online marketplace like Amazon, might represent people as multisets of items purchased, etc, or movies reviewed, etc., and use Jaccard similarity.</p>
<h2 id="euclidean-distance-ell_2-distance-and-ell_p-distance">Euclidean Distance/ <span class="math inline">\(\ell_2\)</span> distance, and <span class="math inline">\(\ell_p\)</span> distance</h2>
<p>Given datapoints in <span class="math inline">\({\mathbb{R}}^d\)</span>, the Euclidean distance metric, which we are all familiar with, is simply <span class="math display">\[D_{euclidean}(x,y) = ||x-y||_2 = \sqrt{\sum_{i=1}^d (x(i)-y(i))^2}.\]</span></p>
<p>More generally, we can define other measures of similarity for points in <span class="math inline">\({\mathbb{R}}^d\)</span> that generalize the above form; the <span class="math inline">\(\ell_p\)</span> distance is defined as <span class="math display">\[||x-y||_p = \left(\sum_{i=1}^d |x(i)-y(i)|^p \right)^{1/p}.\]</span> If <span class="math inline">\(p=1\)</span>, we get “manhattan” distance, and for large <span class="math inline">\(p\)</span>, <span class="math inline">\(||x-y||_p\)</span> is more and more dependent on the coordinate with maximal difference, with the <span class="math inline">\(\ell_{\infty}\)</span> distance simply being defined as <span class="math inline">\(\max_i |x(i)-y(i)|.\)</span></p>
<p>Note that <span class="math inline">\(\ell_2\)</span> distance is rotationally invariant, whereas <span class="math inline">\(\ell_p\)</span> for <span class="math inline">\(p \neq 2\)</span> is not invariant to rotations of the space. One interpretation of this fact is that if you are using <span class="math inline">\(\ell_p\)</span> distance with <span class="math inline">\(p \neq 2,\)</span> you should make sure that the coordinates of your space actually mean something—it does not make too much sense to use a distance metric that depends on your choice of coordinate axes, if your choice of coordinate axes are arbitrary.</p>
<h2 id="other-similarity-metrics">Other similarity metrics</h2>
<p>There are many other similarity metrics, including “cosine similarity” which we saw on the homework, and “edit distance” that measures the similarity between strings (documents, genetic sequences, etc.) by asking “how many edits—-i.e. insertions/deletions” does it take to get from one string to the other. There are plenty of other similarity metrics, and it is worth spending some time thinking about what metric is right for a given problem.</p>
<h2 id="the-relationships-between-metrics-and-metric-embeddings">The Relationships between Metrics, and Metric Embeddings</h2>
<p>A very natural mathematical questions is “how different are the different metrics”? A specific practical motivation is that many geometric algorithms are designed for Euclidean distance (in part because of its rotation invariance). Given a set of points, and a distance metric <span class="math inline">\(D\)</span>, is it possible to map the points into a set of point in <span class="math inline">\({\mathbb{R}}^d\)</span>, such that the original distance between points is equal to (or closely approximated by) the Euclidean distance between the images of those points? Formally, given a distance metric <span class="math inline">\(D\)</span>, and a set of points <span class="math inline">\(X=x_1,\ldots,x_n\)</span>, is it possible to map the points to a set <span class="math inline">\(Y= y_1,\ldots,y_n\)</span>, where <span class="math inline">\(y_i \in {\mathbb{R}}^d,\)</span> ideally for a lowish dimension <span class="math inline">\(d\)</span>, s.t. for all <span class="math inline">\(i,j\)</span>, <span class="math display">\[D(x_i,x_j) \approx ||y_i - y_j||_2.\]</span></p>
<p>This is known as a “metric embedding”—in this case, an embedding into <span class="math inline">\({\mathbb{R}}^d\)</span> under Euclidean distance—and there is a whole area of math/geometry/computer science devoted to studying when such embeddings exist.</p>
<h1 id="a-datastructure-for-similarity-search-kd-trees">A Datastructure for Similarity Search: kd-trees</h1>
<p>For the remainder of the lecture, we will focus on Euclidean distance, though it is worth thinking about how one would apply these ideas to other similarity measures.</p>
<p>A <span class="math inline">\(kd\)</span>-tree (originally proposed by Bentley in 1975) is a space partitioning datastructure that allows one to very quickly find the closest point in a dataset to a given “query” point. <span class="math inline">\(kd\)</span>-trees perform extremely well when the dimensionality (or effective dimensionality) of the data is not too large—usually people say that it works well if the dimensionality of the space is less than <span class="math inline">\(20\)</span>, or if the number of points is at least <span class="math inline">\(2^d\)</span>, where <span class="math inline">\(d\)</span> is the dimensionality of the points. There are many variants of <span class="math inline">\(kd\)</span>-trees, and you should think of this as a general framework for designing such a datastructure, though the specifics can be fruitfully tailored to individual datasets and applications.</p>
<p>The high-level idea is to build a binary search tree that partitions space. Edges of the tree will correspond to subsets of space, and each node, <span class="math inline">\(v\)</span>, in the tree will have two data-fields: the index of some dimension <span class="math inline">\(i_v\)</span>, and a value <span class="math inline">\(m_v\)</span>. Let <span class="math inline">\(S_v\)</span> denote the subset of space corresponding to the edge going into a node <span class="math inline">\(v\)</span>, and let <span class="math inline">\(S_{&lt;},S_{&gt;}\)</span> denote the subsets of space corresponding to the two outgoing edges of <span class="math inline">\(v\)</span>. These subsets will be defined as <span class="math inline">\(S_{&lt;} = \{x : x \in S_v, \text{ s.t. } x(i_v) &lt; m_v\}\)</span> and <span class="math inline">\(S_{&gt;} = \{x : x \in S_v, \text{ s.t. } x(i_v) \ge m_v\}\)</span>.</p>
<p>We build this tree as follows:</p>
<p><span> </span></p>
<p>Note that the size of the data-structure is linear in the size of the initial pointset. To add a point, <span class="math inline">\(v\)</span>, to the structure, one simply goes down the tree, querying the indices of <span class="math inline">\(V\)</span>, and comparing them to the various medians until one reaches a leaf, at which point one will then split the leaf into two children. The tree will initially be balanced (because we are using the medians of the coordinate values), and hence will have depth <span class="math inline">\(O(\log n)\)</span>.</p>
<p>Given a node <span class="math inline">\(v\)</span>, if we want to find the closest point in our <span class="math inline">\(kd\)</span>-tree structure to <span class="math inline">\(v\)</span>, we will first go down the tree and find the leaf in which <span class="math inline">\(v\)</span> would end up. We then go back up the tree, at each juncture, asking “is it possible that the closest point to <span class="math inline">\(v\)</span> would have ended up down the other path?”. In low dimensions, the answer to this question will often be “no”, and the search will be efficient. For example, in 1 dimension, the leaf node corresponding to <span class="math inline">\(v\)</span> will <em>always</em> contain the closest point to <span class="math inline">\(v\)</span>. [Think about why this is the case!] In high dimensions, we might end up needing to explore many/all leaves of the tree, which is why kd-trees are ill-suited to very high dimensional data.</p>
<h1 id="the-curse-of-dimensionality">The Curse of Dimensionality</h1>
<p>Why are high-dimensional spaces often hard to deal with? Why do the running times of many algorithms scale exponentially with the dimension? One answer is that high-dimensional spaces, in some sense, can lack geometry. For example, they can have lots and lots of points with the property that all pairs of points have roughly the same distance.</p>
<p>What is the largest number of points that fit in <span class="math inline">\(d\)</span>-dimensional space, with the property that all pairwise distances are in the interval <span class="math inline">\([0.75, 1.25]?\)</span></p>
<ul>
<li><p><span class="math inline">\(d=2\)</span>: At most 2 points have this property...if you try to fit a third point, at least one of the 3 pairwise distances will be off.</p></li>
<li><p><span class="math inline">\(d=3\)</span>: At most 3 points have this property...if you try to fit a fourth point, at least one of the 6 pairwise distances will be off.</p></li>
<li><p><span class="math inline">\(d=100\)</span>: You will be able to fit several thousand points!</p></li>
<li><p>In general, you will be able to fit an exponential number of points (a quick calculation shows that a random set of <span class="math inline">\(exp(\sqrt{d})\)</span> will satisfy this property with high probability).</p></li>
</ul>

{% endraw %}
