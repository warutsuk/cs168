---
layout: post
title: Lecture 13
author: Tim
---
{% raw %}

<p>This week, we will cover tools for making inferences based on random samples drawn from some distribution of interest (e.g. a distribution over voter priorities, customer behavior, ip addresses, etc.). We will also learn how to use sampling techniques to solve hard problems—both problems that inherently involve randomness, as well as those that do not.</p>
<p>As a warmup, to get into the probabilistic mindset, we will see a very cute, and useful algorithm for drawing samples from a datastream.</p>
<h1 id="reservoir-sampling">Reservoir Sampling</h1>
<p>Problem: How can one efficiently sample <span class="math inline">\(k\)</span> uniformly random elements from a datastream of length <span class="math inline">\(N \gg k\)</span>? The two main issues are 1) <span class="math inline">\(N\)</span> might be huge—we certainly don’t want to store a significant fraction of the stream in memory, 2) <span class="math inline">\(N\)</span> might be unknown—we might not know the length of the stream ahead of time, but we still hope that whenever the stream ends, we have a uniformly random sample. (Recall that we are asking this problem in a similar “streaming” setting as that of the Count-Min-Sketch algorithm we saw earlier.)</p>
<p>As a motivating example, suppose a router might want to sample 1,0000 random ip addresses from a given day’s traffic. The router obviously doesnt want to store all of the ip addresses, and the router does not know ahead of time the total amount of traffic that it will see on a given day.</p>
<p><span> </span></p>
<p>We now show that at any time <span class="math inline">\(t \ge k\)</span>, the reservoir <span class="math inline">\(R\)</span> consists of a uniformly random subset of <span class="math inline">\(k\)</span> of the entries of <span class="math inline">\(x_1,\ldots,x_t\)</span>. To do this, let <span class="math inline">\(R_t\)</span> denote the reservoir after the <span class="math inline">\(t\)</span>th datastream element has been seen. It suffices to show that for all <span class="math inline">\(t \ge i\)</span>, <span class="math inline">\(\Pr[x_i \in R] = \frac{k}{t}.\)</span></p>
<p>For all <span class="math inline">\(t \ge i,\)</span> <span class="math inline">\(\Pr[x_i \in R_t] = \frac{k}{t},\)</span> where <span class="math inline">\(R_t\)</span> denotes the reservoir after time <span class="math inline">\(t\)</span>.</p>
<p>We prove this by induction on <span class="math inline">\(t\)</span>. If <span class="math inline">\(i \le k,\)</span> the base case will be when <span class="math inline">\(t=k\)</span> and <span class="math inline">\(\Pr[x_i \in R_k] = 1 = k/t\)</span>. If <span class="math inline">\(i &gt; k\)</span>, the base case will be <span class="math inline">\(t=i\)</span>, and recall that the algorithm specifies that we include <span class="math inline">\(x_i\)</span> in <span class="math inline">\(R\)</span> with probability exactly <span class="math inline">\(k/i = k/t.\)</span> For the induction hypothesis, assume that the statement holds for some fixed value of <span class="math inline">\(t \ge i\)</span>. We now consider <span class="math inline">\(\Pr[x_i \in R_{t+1}]\)</span> and leverage our induction hypothesis and conditional probability: <span class="math display">\[\begin{aligned}
\Pr[x_i \in R_{t+1}] &amp; = &amp; \Pr[x_i \in R_t]\cdot\Pr[x_i \text{ not replaced at time } t+1 | x_i \in R_t] \\
&amp; = &amp; \frac{k}{t} \cdot \Pr[x_i \text{ not replaced at time } t+1 | x_i \in R_t] \qquad \text{by our induction hyp.} \\
&amp; = &amp; \frac{k}{t} \cdot \left(1 - \frac{k}{t+1} \cdot \frac{1}{k}\right) = \frac{k}{t+1}.\end{aligned}\]</span> Note that in the last line, we computed <span class="math inline">\( \Pr[x_i \text{ not replaced at time } t+1 | x_i \in R_t]\)</span> as <span class="math inline">\(1-\Pr[x_{t+1}\)</span> replaces <span class="math inline">\(x_i | x_i \in R_t] = 1-\frac{k}{t+1}\frac{1}{k},\)</span> where the <span class="math inline">\(\frac{k}{t+1}\)</span> term is the probability that <span class="math inline">\(x_{t+1}\)</span> is stored in <span class="math inline">\(R_{t+1},\)</span> and the <span class="math inline">\(1/k\)</span> term is the probability that <span class="math inline">\(x_i\)</span> is chosen as the random element of <span class="math inline">\(R_t\)</span> to remove.</p>
<h1 id="basic-probability-tools-our-good-friends-andrey-markov-and-pafnuty-chebyshev">Basic Probability Tools: Our good friends Andrey [Markov] and Pafnuty [Chebyshev]</h1>
<p>We begin with two of the most basic tools from probability theory that are extremely helpful for designing sampling schemes, and interpreting the significance of results gleaned from random samples.</p>
<p><em>In lecture we discussed a number of applications—the punch line was that data is everywhere, and that there is a huge difference between what is possible with a careful, mathematically principled analysis, versus a sloppy/crude analysis. Just ask Nate Silver, who has essentially made a career out of properly analyzing publicly available political poll data (and, other public data, such as sports data, etc.). I have added a link from the course page to a page on his website explaining his political prediction poll-aggregation system, in case you are interested.</em></p>
<p>Markov’s inequality expresses the basic fact that “at most <span class="math inline">\(10\%\)</span> of the population can have an income that is more than <span class="math inline">\(10\times\)</span> the average income of the population”:</p>
<p>For a real-valued random variable <span class="math inline">\(X\)</span> s.t. <span class="math inline">\(X \ge 0\)</span>, for any <span class="math inline">\(c &gt; 0\)</span>, <span class="math display">\[\Pr\left[X \ge c {\textbf{E}}[X] \right] \le \frac{1}{c}.\]</span></p>
<p>Assume for the point of contradiction that <span class="math inline">\(\Pr\left[X \ge c \cdot {\textbf{E}}[X] \right] &gt; \frac{1}{c},\)</span> then <span class="math inline">\({\textbf{E}}[X] &gt; \frac{1}{c} c \dot {\textbf{E}}[X] = {\textbf{E}}[X],\)</span> which is a contradiction.</p>
<p>Markov’s inequality tells us something very simple about a distribution over real numbers, if all we know about the distribution is its expectation, and that it is non-negative. In many applications, it gives extremely weak bounds (e.g. the probability that a student’s GPA is more that twice the average GPA is at most <span class="math inline">\(1/2\)</span>.....not very surprising). Nevertheless, Markov’s inequality is the key tool that lets us prove more powerful theorems that give sharper characterizations of distributions for which we have more information. For example, it lets us prove Chebyshev’s inequality, which tells us that the probability that a random variable is more than <span class="math inline">\(c\)</span> standard deviations from its expectation, is at most <span class="math inline">\(1/c^2\)</span>.</p>
<p>For a real-valued random variable <span class="math inline">\(X\)</span>, and any <span class="math inline">\(c&gt;0\)</span>, <span class="math display">\[\Pr\left[ |X-{\textbf{E}}[X]| \ge c \sqrt{{\textbf{Var}}[X]}\right] \le \frac{1}{c^2}.\]</span></p>
<p>Chebyshev’s inequality is proved by applying Markov’s inequality to a cleverly chosen random variable. Let <span class="math inline">\(Y = (X-{\textbf{E}}[X])^2\)</span>. Note that <span class="math inline">\(Y\)</span> is a perfectly good random variable, satisfying <span class="math inline">\(Y \ge 0.\)</span> To apply Markov’s inequality, we need to know <span class="math inline">\({\textbf{E}}[Y]\)</span>. First, note that <span class="math inline">\({\textbf{E}}[Y] = {\textbf{E}}\left[(X-{\textbf{E}}[X])^2\right]= {\textbf{Var}}[X].\)</span> <span class="math display">\[\begin{aligned}
\Pr \left[ |X-{\textbf{E}}[X]| \ge c \sqrt{{\textbf{Var}}[X]}\right] &amp; = &amp; \Pr[Y \ge c^2 {\textbf{Var}}[X]] \\
&amp; = &amp; \Pr\left[Y \ge c^2 {\textbf{E}}[Y]\right] \le \frac{1}{c^2} \qquad \text{ (by Markov&#39;s inequality)}\end{aligned}\]</span></p>
<p>What is this good for? Lets see an example:</p>
<p>How many people must we poll to estimate the percentage of people who support candidate C? Specifically, say we want our answer to be accurate to <span class="math inline">\(\pm 1\%\)</span>, with probability at least <span class="math inline">\(3/4\)</span>.</p>
<p>Let <span class="math inline">\(p \in [0,1]\)</span> denote the true probability that a randomly chosen person supports candidate C. Suppose we poll <span class="math inline">\(n\)</span> people, and output the fraction that support the candidate. Let <span class="math inline">\(X_1,\ldots,X_n\)</span> denote the <span class="math inline">\(0/1\)</span> indicator random variables with <span class="math inline">\(X_i\)</span> indicating that the <span class="math inline">\(i\)</span>th person polled supports the candidate. Let <span class="math inline">\(Z = \sum_i X_i\)</span> denote the number of people polled who support the candidate. <span class="math display">\[{\textbf{Var}}[Z] = \sum_i {\textbf{Var}}[X_i] = n\cdot p(1-p).\]</span> In the above calculation, we used 1) the fact that the variance of a sum of independent random variables is the sum of the variances, and 2) a quick calculation showing that if <span class="math inline">\(X\)</span> is <span class="math inline">\(1\)</span> with probability <span class="math inline">\(p\)</span>, and 0 otherwise, <span class="math inline">\({\textbf{Var}}[X] = p(1-p)^2 + (1-p) p^2 = p(1-p).\)</span></p>
<p>An error in our estimate of <span class="math inline">\(1\%\)</span> corresponds to estimating <span class="math inline">\({\textbf{E}}[Z]\)</span> up to an error of <span class="math inline">\(0.01\cdot n\)</span>. From Chebyshev’s inequality, <span class="math display">\[\begin{aligned}
\Pr[|Z-{\textbf{E}}[Z]| \ge 0.01 n]  &amp; = &amp; \Pr\left[|Z-{\textbf{E}}[Z]| \ge \frac{0.01n}{\sqrt{{\textbf{Var}}[Z]}} \sqrt{{\textbf{Var}}[Z]}\right] \\ &amp;= &amp; \Pr\left[|Z-{\textbf{E}}[Z]| \ge \frac{0.01\sqrt{n}}{\sqrt{p(1-p)}} \sqrt{{\textbf{Var}}[Z]}\right]\\ &amp; \le &amp;\frac{p(1-p)}{n/100^2} \qquad \text{from Chebyshev&#39;s inequality}.\end{aligned}\]</span> For any <span class="math inline">\(p \in [0,1]\)</span>, the numerator <span class="math inline">\(p(1-p) \le 1/4,\)</span> and hence if <span class="math inline">\(n \ge 100^2,\)</span> this probability is at most <span class="math inline">\(1/4,\)</span> as desired. Note that if we know that <span class="math inline">\(p\)</span> is very small (i.e. <span class="math inline">\(p&lt; 0.1\)</span>) or very close to <span class="math inline">\(1\)</span>, then the numerator <span class="math inline">\(p(1-p)\)</span> is very small, and the required number of samples is correspondingly smaller.</p>
<p>What we have just shown is that, in general, to estimate the expectation of a <span class="math inline">\(0/1\)</span> random variable to error <span class="math inline">\(\pm \eps\)</span>, one needs roughly <span class="math inline">\(O(1/\eps^2)\)</span> independent samples.</p>
<p>If Chebyshev’s inequality tells us something about a distribution given a bound on its variance, you might suspect that, given information on a distributions’ higher moments (i.e. <span class="math inline">\({\textbf{E}}[X^3],{\textbf{E}}[X^4],\)</span>, etc.) one might be able to obtain even sharper insights into the distribution. This intuition is true, and can be made rigorous via the same approach as the proof of Chebyshev’s: apply Markov’s inequality to a cleverly chosen random variable, corresponding to one of the higher moments of the distribution you care about. This is also how one proves “central limit” style inverse exponential bounds on tail probabilities (i.e. showing that the probability you flip fewer than 400 ’heads’ in 1000 tosses of a fair coin is miniscule.)</p>
<h1 id="importance-sampling">Importance Sampling</h1>
<p>We mentioned “importance sampling” extremely briefly. The basic idea is that we can estimate properties of distribution <span class="math inline">\(A\)</span>, based on samples from distribution <span class="math inline">\(B\)</span>. Sometimes, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are fixed, and sometimes we can design distribution <span class="math inline">\(B\)</span> to let us answer questions about <span class="math inline">\(A\)</span> using fewer samples than we would need if we were to directly sample from <span class="math inline">\(A\)</span>. This savings is achieved by having distribution <span class="math inline">\(B\)</span> place higher weight on the “important” elements of the domain. The following example will clarify this high level intuition.</p>
<p>Suppose we want to estimate the average income of the population. We know that the distribution of incomes has a long tail; i.e. there is a small fraction of people who have very large incomes, and hence have a large effect on the average income. This long, but important tail, is what makes the estimation task hard—if we don’t get any samples from the tail, then our estimate will likely be too low, but we need to take many samples in order to get any representatives from the tail.</p>
<p>We can use importance sampling as follows: Suppose we know that computer scientists have higher income than average, and that computer scientists compose exactly a <span class="math inline">\(0.05\)</span> fraction of the total population. Rather than taking <span class="math inline">\(n\)</span> random samples from the population, suppose we take <span class="math inline">\(0.8n\)</span> samples of non-computer scientists, and <span class="math inline">\(0.2n\)</span> samples of random computer scientists. If <span class="math inline">\(a_1\)</span> is the average salary of the non-computer scientists in our sample, and <span class="math inline">\(a_2\)</span> is the average salary of the sample of computer scientists, we can estimate the population average salary as: <span class="math display">\[\hat{avg} = 0.95 a_1 + 0.05 a_2.\]</span> The benefit of this is that by taking more samples from higher earners, we obtained a better estimate of this important tail. Of course, the final calculation needs to reweight these two samples so that our answer is still an unbiased estimate of the overall population average.</p>
<p>Formally, how does one characterize the improvement that we obtained by over sampling the computer scientists? It does not change the expectation of our answer—our estimate is still the unbiased estimate of the population average. The benefit is that we have reduced the variance of our estimate, by focussing our samples on the important portion of the distribution (important with respect to the property that we care about—in this case, estimating the mean).</p>
<h1 id="knowing-the-unknowns-estimating-the-missing-mass">Knowing the unknowns: Estimating the missing mass</h1>
<p>Thus far we have discussed interpreting random samples drawn from distributions over <span class="math inline">\(0/1\)</span>, or distributions over real numbers (as in estimating average salary). In many applications, particularly in natural language processing, and genetics, one obtains samples from a distribution that is supported on a huge number of incomparable items. For example, a distribution over words, or a distribution over rare genetic variants. In these settings, one might not even know the support of the distribution.</p>
<p>While it is generally not possible to know what elements of the support are missing from a given sample, we can still hope to estimate the fraction of the distribution that is composed of the missing elements. One concrete question is the following:</p>
<blockquote>
<p>Given a set of random samples drawn from a distribution, what is the probability that the next sample we draw is a “new” domain element that we have not seen previously? Equivalently, what is the amount of probability mass in the distribution composed of domain elements that are not represented in our sample (i.e. the “missing mass”)?</p>
</blockquote>
<p>This question was first studied by I.J. Good and Alan Turing, in their work at Bletchley Park, in their work during WWII. (They wanted to estimate the probability that the next enigma machine ciphertext would be a previously unseen ciphertext.) They proposed the following extremely simple estimate of the missing mass, known as the <em>Good–Turing</em> frequency estimation scheme. Given <span class="math inline">\(n\)</span> independent draws from a distribution, they proposed the following estimate: <span class="math display">\[\Pr[\text{next draw is something new}] \approx \frac{\#\text{ elmts seen exactly once}}{n}.\]</span></p>
<p>There is a large literature analyzing and extending this basic estimate—just search for “Good-Turing frequency estimation”.</p>
<p>Assuming radio stations play songs in a random order, if you have been listening to the radio for the past 5 hours, the probability that the next song you here is a song you haven’t heard in the past 5 hours can be estimated as the fraction of songs that you have heard exactly once in the past 5 hours.</p>
<p>An estimate of the probability that the next word you read is a new word that you haven’t seen before, is the number of words that you have seen exactly once, divided by the total number of words that you have seen. Keep in mind that these lecture notes are just an abbozzo.</p>
<p>Here is a quick sketch/derivation of this estimate, which we didn’t see in lecture: Assume that we have <span class="math inline">\(n\)</span> samples from a distribution <span class="math inline">\(p\)</span> supported on some unknown domain <span class="math inline">\(X\)</span>, with <span class="math inline">\(p(x)\)</span> denoting the probability that the distribution assigns to element <span class="math inline">\(x \in X\)</span>. <span class="math display">\[\begin{aligned}
``unseen mass&#39;&#39; &amp; = &amp; \Pr[\text{next draw is new}] = \sum_{x \in X} p(x) \cdot \Pr[x \text{ unseen}] \\
&amp; \approx &amp; \sum_{x \in X} p(x) \cdot \Pr[Binomial(n,p(x)) = 0] \\
&amp; = &amp; \sum_{x \in X} p(x) (1-p(x))^n \\
&amp; = &amp; \frac{1}{n} \sum_{x \in X} n\cdot p(x) (2-p(x)^{n-1} (1-p(x))\\
&amp; \approx &amp; \frac{1}{n} \sum_{x \in X} \Pr[Binomial(n,p(x)) = 1] \approx \frac{\#\text{ elmts seen exactly once}}{n}.\end{aligned}\]</span></p>

{% endraw %}
