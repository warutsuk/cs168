---
layout: post
title: Lecture 1 - Introduction and Consistent Hashing
---

## Consistent Hashing ##

### Meta-Discussion ###

We'll talk about the course in general in Section \ref{s:about}, but
first let's discuss a representative technical topic: *consistent
  hashing*.  This topic is representative in the following respects:

* As you could guess by the word "hashing," the topic builds on
  central algorithmic ideas that you've already learned (e.g., in
  CS161) and
  adapts them to some very real-world applications.

* The topic is "modern," in the sense that it is motivated by
  issues in present-day systems that were not present in the applications
  of yore --- consistent hashing is not in your parents' algorithms
  textbook, because   back then it wasn't needed.  The original idea
  isn't that new anymore (from 1997), but it has been repurposed for
  new technologies several times since.

* Consistent hashing is a "tool" in the sense that it is a
  non-obvious idea but, once you know it, it's general and flexible
  enough to potentially prove useful for other problems.
In this course, we'll be looking for the following trifecta:
(i) ideas that are
  *non-obvious*, even to the well-trained computer scientist, so
  that we're not wasting your time; (ii) *conceptually simple*
  --- realistically, these are the only ideas that you might remember
  a year or more from now, when you're a start-up founder, senior
  software engineer, or PhD student (iii) *fundamental*,
  meaning that there is some chance that the idea will prove useful to you in
  the future.

* The idea has real applications.  Consistent hashing gave
  birth to Akamai, which to this day is a major player in the Internet
  (market cap $$\approx$$ \$10B), managing the Web presence of tons of
  major companies.  More recently, consistent hashing has been
  repurposed to solve basic problems in peer-to-peer networks
  (initially in~\cite{chord}),
  including parts of BitTorrent.  These days, all the cool kids are
  using consistent hashing for distributed storage --- made popular by
  Amazon's Dynamo~\cite{amazon}, the idea is to have a lightweight
  alternative to a database where all the data resides in main memory
  across multiple machines,  rather than on disk.

### Web Caching ###

The original motivation for consistent hashing (in 1997) was Web
caching.  You're familiar with the concept and benefits of
caching.  In the context of the Web, imagine a browser
requesting a URL, like \amazon.  Of course, one could request the page
from the appropriate Web server.  But if the page is being requested
over and over again, it's wasteful to repeatedly download it from the
server.  An obvious idea is to use a Web cache, which stores a local copy
of recently visited pages.  When a URL is requested, one can first
check the local cache for the page.  If the page is in the cache, one
can send it directly to the browser --- no need to contact the
original server.  If the page is not in the cache, then the page is
downloaded from a suitable server as before, and the result is both
sent to the browser and also stored in the local cache for future
re-use.

*Caching is good*.
The most obvious benefit is that the end
user experiences a much faster response time.  But caches also improve
the Internet as a whole: fewer requests to far away servers means less
network traffic, less congestion in the queues at network switches and
Web servers, fewer dropped packets, etc.

So clearly we want Web caches.  Where should they go?  A first idea is
to give each end user their own cache, maintained on their own machine
or device.  So if you request [amazon.com](http://www.amazon.com), and you also requested it in the
recent past, then the page can be served from your local cache.  If not,
you incur a cache miss, and the page is downloaded and stored in your
local cache.

However, we could take the benefit of caching to the next level if we
could implement a Web cache that is *shared by many users*, for
example, all users of Stanford's network.  For
example, if you haven't accessed [amazon.com](http://www.amazon.com) recently but someone
"nearby" has (e.g., someone else in the Stanford network), wouldn't
it be cool if you could just use their local copy?  The benefits should
be clear: by aggregating the recent page requests of a large number of
users, these users will enjoy many more cache hits and consequently
less latency.
Akamai's goal was to take the daydream of a single logical Web cache
shared by tons of users and turn it into a viable technology.

Why isn't this an easy problem?  We focus on one of several
obstacles.  Namely, remembering the recently accessed Web pages of a
large number of users might take a lot of storage.  Unless we want to
resort to a big, slow, and special-purpose machine for this purpose,
this means that the aggregated cache might not fit on a single machine.
Thus, implementing a shared cache at a large scale requires 
  *spreading the cache over multiple machines*.

Now suppose a browser requests [amazon.com](http://www.amazon.com) and you want to know if the Web
page has been cached locally.  Suppose the shared cache is spread over
100 machines.  Where should you look for a cached copy of the Web
page?  You could poll all 100 caches for a copy, but that feels pretty
dumb.  And with lots of users and caches, this solution
crosses the line from
dumb to infeasible.  Wouldn't it be nice if, instead, given a URL
(like [amazon.com](http://www.amazon.com)) we magically knew which cache (like \#23) we should
look to for a copy?

### A Simple Solution Using Hashing ###

Formally, we want a mapping from URLs to caches.
The first thought of a well-trained computer scientist might be to
use a hash function for this purpose.[^1]  Recall that a {\em hash
  function} maps elements of a (usually super-big) universe $U$, like
URLs, to ``buckets,'' such as 32-bit values (Figure~\ref{f:hash}).
A ``good'' hash function $h$ satisfies two properties:

[^1]: We'll assume that
  you've seen hashing before, probably multiple times.  See the course
  site for review videos on the topic.

* It is easy to remember and evaluate.  Ideally, computing the function
  involves just a few arithmetic operations, and maybe a "mod"
  operation.

* For all practical purposes, $$h$$ behaves like a totally random
  function, spreading data out evenly and without noticeable
  correlation across the possible buckets.

Designing good hash functions is not easy --- hopefully you won't need
to do it yourself --- but you can regard it as a solved problem.
A common approach in practice is to use a well-known and well-crafted
hash function like MD5[^2] --- 
it's overwhelmingly likely that this function will "behave randomly"
for whatever data set you're working with.  Theoretical guarantees are
possible only for *families* of hash functions,[^3] which
motivates picking a hash function at random from a `"universal"
family (see CS161 for details).

[^2]: This is built in to most programming
  languages, or you can just copy the code for it from the Web.  Or you might want to use something faster and more lightweight (but still well tested), like from the FarmHash family.
  
[^3]: Assuming
that  the number of buckets $$n$$ is significantly smaller than the universe
  size $$U$$, every fixed hash function has a large pathological data
  set $$S \sse U$$ for   it, with all elements of $$S$$ colliding and
  hashing to the   same bucket.  (Hint: Pigeonhole Principle.)

This is *italic*

This is an equation:
$$x_{1,2}=\frac{3}{b}$$

This is **bold**

This is `code`

> This is blockquoted

```
Code in a block.
```

[This is Google](http://www.google.com)

* This is a list.
* Another item.

1. This is another list.
2. Yay
3. Yay3
    * This is inside

A CNN consists of a number of convolutional and subsampling layers optionally followed by fully connected layers.  The input to a convolutional layer is a $$m \text{ x } m \text{ x } r$$ image where $$m$$ is the height and width of the image and $$r$$ is the number of channels, e.g. an RGB image has $$r=3$$.
