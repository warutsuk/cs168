---
layout: post
title: "Lecture 18 : Linear and Convex Programming, with Applications to Sparse Recovery"
author: Tim
---
{% raw %}

<h1 id="s:recap">The Story So Far</h1>
<p>Recall the setup in compressive sensing. There is an unknown signal <span class="math inline">\(\z \in \RR^n\)</span>, and we can only glean information about <span class="math inline">\(\z\)</span> through linear measurements. We choose <span class="math inline">\(m\)</span> linear measurements <span class="math inline">\(\a_1,\ldots,\a_m \in \RR^n\)</span>. “Nature” then chooses a signal <span class="math inline">\(\z\)</span>, and we receive the results <span class="math inline">\(b_1 = {
{\langle {\a_1} , {\z} \rangle}
},\ldots,b_m = {
{\langle {\a_m} , {\z} \rangle}
}\)</span> of our measurements, when applied to <span class="math inline">\(\z\)</span>. The goal is then to recover <span class="math inline">\(\z\)</span> from <span class="math inline">\(\b\)</span>.</p>
<p>Last lecture culminated in the following sparse recovery guarantee for compressive sensing.</p>
<p>[t:cs2] Fix a signal length <span class="math inline">\(n\)</span> and a sparsity level <span class="math inline">\(k\)</span>. Let <span class="math inline">\(\A\)</span> be an <span class="math inline">\(m
\times n\)</span> matrix with <span class="math inline">\(m = \Theta(k \log \tfrac{n}{k})\)</span> rows, with each of its <span class="math inline">\(mn\)</span> entries chosen independently from the standard Gaussian distribution. With high probability over the choice of <span class="math inline">\(\A\)</span>, for every <span class="math inline">\(k\)</span>-sparse signal <span class="math inline">\(\z\)</span>,<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> the unique optimal solution to the <span class="math inline">\(\ell_1\)</span>-minimization problem <span class="math display">\[\label{eq:lp1}
\min \|\x\|_1\]</span> subject to <span class="math display">\[\label{eq:lp2}
\Ax = \b\]</span> is <span class="math inline">\(\z\)</span>.</p>
<p>Theorem [t:cs2] is really kind of shocking.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> We’re solving the wrong optimization problem — <span class="math inline">\(\ell_1\)</span>-minimization rather than <span class="math inline">\(\ell_0\)</span>-minimization, which is what we really want (but <span class="math inline">\(NP\)</span>-hard) — and yet obtaining the right answer! How would one ever suspect that this could be true? Perhaps unsurprisingly, this unreasonable exact recovery was first observed empirically — by geophysicists in the late 1970s and early 1980s, and again in the early 21st century in the context of medical imaging. Only then, in the last decade or so, was theory developed to explain these empirical observations (beginning with <span class="citation"></span>).</p>
<p>Last lecture, we discussed why minimizing the <span class="math inline">\(\ell_1\)</span> norm of a feasible solution promotes sparse solutions. Geometrically, the <span class="math inline">\(\ell_1\)</span> ball is the “longest” along the standard basis vectors, and hence if we blow up a balloon centered at the origin in the shape of the <span class="math inline">\(\ell_1\)</span> ball, its first point of contact with a subspace of linear system solutions (an affine subspace) tends to be relatively sparse, compared to other norms. Next, we discuss how to solve the <span class="math inline">\(\ell_1\)</span>-minimization problem efficiently using linear programming.</p>
<h1 id="linear-programming">Linear Programming</h1>
<h2 id="context">Context</h2>
<p>The more general a problem, the more computationally difficult it is. For example, sufficient generalization of a polynomial-time solvable problem often yields an <span class="math inline">\(NP\)</span>-hard problem. If you only remember one thing about linear programming, make it this: <span><em>linear programming is a remarkable sweet spot balancing generality and computational tractability</em></span>, arguably more so than any other problem in the entire computational landscape.</p>
<p>Zillions of problems, including <span class="math inline">\(\ell_1\)</span>-minimization, reduce to linear programming. It would take an entire course to cover even just its most famous applications. Some of these applications are conceptually a bit boring but still very important — as early as the 1940s, the military was using linear programming to figure out the most efficient way to ship supplies from factories to where they were needed. Central problems in computer science that reduce to linear programming include maximum flow and bipartite matching. (There are also specialized algorithms for these two problems, see CS261.) Linear programming is also useful for <span class="math inline">\(NP\)</span>-hard problems, for which it serves as a powerful subroutine in the design of heuristics (again, see CS261).</p>
<p>Despite this generality, linear programs can be solved efficiently, both in theory (meaning in worst-case polynomial time) and in practice (with input sizes up into the millions).</p>
<h2 id="using-linear-programming">Using Linear Programming</h2>
<p>You can think of linear programming as a restricted programming language for encoding computational problems. The language is flexible, and sometimes figuring out the right way to use it requires some ingenuity (as we’ll see).</p>
<p>At a high level, the description of a linear program specifies what’s allowed, and what you want. Here are the ingredients:</p>
<ol>
<li><p><span><em>Decision variables.</em></span> These are real-valued variables <span class="math inline">\(x_1,\ldots,x_n \in \RR\)</span>. They are “free,” in the sense that it is the job of the linear programming solver to figure out the best joint values for these variables.</p></li>
<li><p><span><em>Constraints.</em></span> Each constraint should be linear, meaning it should have the form <span class="math display">\[\sum_{j=1}^n a_{ij}x_j \le b_i\]</span> or <span class="math display">\[\sum_{j=1}^n a_{ij}x_j = b_i.\]</span> We didn’t bother including constraints of the form <span class="math inline">\(\sum_{j=1}^n a_{ij}x_j \ge b_i\)</span>, since these are equivalent to <span class="math inline">\(\sum_{j=1}^n (-a_{ij})x_j \le -b_i\)</span>. All of the <span class="math inline">\(a_{ij}\)</span>’s and <span class="math inline">\(b_i\)</span>’s are real-valued constants, meaning specific numbers (1, -5, 10, etc.) that are hard-coded into the linear program.</p></li>
<li><p><span><em>Objective function.</em></span> Again, this should be linear, of the form <span class="math display">\[\min \sum_{j=1}^n c_jx_j.\]</span> It’s fine to maximize instead of minimize: after all, <span class="math inline">\(\max \sum_{j=1}^n c_jx_j\)</span> yields the same result as <span class="math inline">\(\min \sum_{j=1}^n (-c_j)x_j\)</span>.</p></li>
</ol>
<p>So what’s not allowed in a linear program? Terms like <span class="math inline">\(x_j^2\)</span>, <span class="math inline">\(x_jx_k\)</span>, <span class="math inline">\(\log (1 + x_j)\)</span>, etc. So whenever a decision variable appears in an expression, it is alone, possibly multiplied by a constant. These linearity requirements may seem restrictive, but many real-world problems are well approximated by linear programs.</p>
<h2 id="a-simple-example">A Simple Example</h2>
<p>To make linear programs more concrete and develop your intuition about them, let’s look at a simple example. Suppose there are two decision variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> — so we can visualize solutions as points <span class="math inline">\((x_1,x_2)\)</span> in the plane. See Figure [f:lp]. Let’s consider the (linear) objective function of maximizing the sum of the decision variables: <span class="math display">\[\label{eq:obj}
\max\, x_1 + x_2.\]</span> We’ll look at four (linear) constraints: <span class="math display">\[\begin{aligned}
x_1 &amp; \ge &amp; 0\\
x_2 &amp; \ge &amp; 0\\
2x_1 + x_2 &amp; \le &amp; 1\\ \label{eq:constraint}
x_1 + 2x_2 &amp; \le &amp; 1.\end{aligned}\]</span> The first two inequalities restrict feasible solutions to the non-negative quadrant of the plane. The second two inequalities further restrict feasible solutions to lie in the shaded region depicted in Figure [f:lp]. Geometrically, the objective function asks for the feasible point furthest in the direction of the coefficient vector <span class="math inline">\((1,1)\)</span> — the “most northeastern” feasible point. Eyeballing the feasible region, this point is <span class="math inline">\((\tfrac{1}{3},\tfrac{1}{3})\)</span>, for an optimal objective function value of <span class="math inline">\(\tfrac{2}{3}\)</span>.</p>
<div class="figure">
<embed src="lp.pdf" />
<p class="caption">A linear program in 2 dimensions.<span data-label="f:lp"></span></p>
</div>
<h2 id="geometric-intuition">Geometric Intuition</h2>
<p>This geometric picture remains valid for general linear programs, with an arbitrary number of dimensions and constraints: <span><em>the objective function gives the optimization direction, and the goal is to find the feasible point that is furthest in this direction</em></span>. Moreover, the feasible region of a linear program is just a higher-dimensional analog of a polygon.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<h2 id="algorithms-for-linear-programming">Algorithms for Linear Programming</h2>
<p>Linear programs are not difficult to solve in two dimensions — for example, one can just check all of the vertices (i.e., “corners”) of the feasible region. In high dimensions, linear programs are not so easy; the number of vertices can grow exponentially with the number of dimensions (e.g., think about hypercubes), so there’s no time to check them all. Nevertheless, we have the following important fact.</p>
<p>[fact:lp] Linear programs can be solved efficiently.</p>
<p>The theoretical version of Fact [fact:lp] states that there is a polynomial-time algorithm for linear programming.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> The practical version of Fact [fact:lp] is that there are excellent commercial codes available for solving linear programs.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> These codes routinely solve linear programs with millions of variables and constraints. One thing to remember about linear programming is that, for over 60 years, many people with significant resources — ranging from the military to large companies — have had strong incentives to develop good codes for it. This is one of the reasons that the best codes are so fast and robust.</p>
<p>In CS168, we won’t discuss how the various algorithms for linear programming work.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> While the key conceptual ideas are pretty natural, lots of details are required. Most professional researchers and optimizers treat linear programming as a “black box” — a subroutine that can be invoked at will, without knowledge of its inner details. We’ll adopt this perspective as well in lecture and on Homework #9.</p>
<h1 id="linear-programming-and-ell_1-minimization">Linear Programming and <span class="math inline">\(\ell_1\)</span>-Minimization</h1>
<p>We now show that the <span class="math inline">\(\ell_1\)</span>-minimization problem in Theorem [t:cs2] can be solved using linear programming. The only non-trivial issue is the objective function <span class="math display">\[\min \|\x\|_1 = \sum_{j=1}^n |x_j|,\]</span> which, because of the absolute values, is non-linear.</p>
<p>As a warm-up, suppose first that we know that the unknown signal <span class="math inline">\(\z\)</span> is component-wise non-negative (in addition to being <span class="math inline">\(k\)</span>-sparse). Then, the <span class="math inline">\(\ell_1\)</span>-minimization problem is just <span class="math display">\[\min \sum_{j=1} x_j\]</span> subject to <span class="math display">\[\label{eq:b}
\Ax = \b\]</span> and <span class="math display">\[\label{eq:nonneg}
\x \ge 0.\]</span> The objective function is clearly linear. The <span class="math inline">\(n\)</span> non-negativity constraints in  — each of the form <span class="math inline">\(x_j \ge 0\)</span> for some <span class="math inline">\(j\)</span> — are linear. Each of the <span class="math inline">\(m\)</span> equality constraints  has the form <span class="math inline">\(\sum_{j=1}^n a_{ij}x_j = b_i\)</span>, and is therefore linear. Thus, this is a linear program.</p>
<p>We know the unknown signal <span class="math inline">\(\z\)</span> satisfies <span class="math inline">\(\Ax = \b\)</span> (by the definition of <span class="math inline">\(\b\)</span>). We’re also assuming that <span class="math inline">\(\z \ge 0\)</span>. Hence, <span class="math inline">\(\z\)</span> is a feasible solution to the linear program. Since <span class="math inline">\(\x \ge 0\)</span> for every feasible solution, the objective function value <span class="math inline">\(\sum_{j=1}^n x_j\)</span> equals <span class="math inline">\(\|\x\|_1\)</span> for every feasible solution. We conclude that this linear program is a faithful encoding of <span class="math inline">\(\ell_1\)</span>-minimization for non-negative signals.</p>
<p>For the general case of real-valued signals <span class="math inline">\(\z\)</span>, the key trick is to add additional variables that allow us to “linearize” the non-linear objective function in . In addition to the previous decision variables <span class="math inline">\(x_1,\ldots,x_n\)</span>, our linear program will include auxiliary decision variables <span class="math inline">\(y_1,\ldots,y_n\)</span>. The intent is for <span class="math inline">\(y_j\)</span> to represent <span class="math inline">\(|x_j|\)</span>. We use the objective function <span class="math display">\[\label{eq:y1}
\min \sum_{j=1}^n y_j,\]</span> which is clearly linear. We also add <span class="math inline">\(2n\)</span> linear inequalities, of the form <span class="math display">\[\label{eq:y2}
y_j - x_j \ge 0\]</span> and <span class="math display">\[\label{eq:y3}
y_j + x_j \ge 0\]</span> for every <span class="math inline">\(j=1,2,\ldots,n\)</span>. Finally, we have the usual <span class="math inline">\(m\)</span> linear consistency constraints <span class="math display">\[\label{eq:y4}
\Ax = \b.\]</span></p>
<p>Every feasible solution of this linear program satisfies all of the constraints, and in particular  and  imply that <span class="math inline">\(y_j \ge \max \{ x_j,-x_j \} = |x_j|\)</span> for every <span class="math inline">\(j=1,2,\ldots,n\)</span>. Observe further that at an optimal solution, equality must hold for every <span class="math inline">\(j\)</span>: given a feasible solution with <span class="math inline">\(y_j
&gt; x_j\)</span> and <span class="math inline">\(y_j &gt; -x_j\)</span> for some <span class="math inline">\(j\)</span>, one can decrease <span class="math inline">\(y_j\)</span> slightly to produce a new solution that is still feasible and that has slightly better (i.e., smaller) objective function value . It follows that the values of the variables <span class="math inline">\(\x\)</span> in an optimal solution to the linear program given by – is the optimal solution to the <span class="math inline">\(\ell_1\)</span>-minimization problem given in –.</p>
<p>To further showcase the power and flexibility of linear programming, suppose that the results of the linear measurements are corrupted by noise. Concretely, assume that instead of receiving <span class="math inline">\(b_i =
{
{\langle {\a_i} , {\z} \rangle}
}\)</span> for each measurement <span class="math inline">\(i=1,2,\ldots,m\)</span>, we receive a value <span class="math inline">\(b_i \in [{
{\langle {\a_i} , {\z} \rangle}
} - \eps, {
{\langle {\a_i} , {\z} \rangle}
} + \eps]\)</span>, where <span class="math inline">\(\eps &gt; 0\)</span> is a bound on the magnitude of the noise. Now, the linear system <span class="math inline">\(\Ax = \b\)</span> might well be infeasible — <span class="math inline">\(\z\)</span> is now only an approximately feasible solution. The linear program – is easily modified to accommodate noise — just replace the equality constraints  by two sets of inequality constraints, <span class="math display">\[\sum_{j=1}^n a_{ij}x_j \le b_i + \eps\]</span> and <span class="math display">\[\sum_{j=1}^n a_{ij}x_j \ge b_i - \eps\]</span> for each <span class="math inline">\(i=1,2,\ldots,m\)</span>. The guarantee in Theorem [t:cs2] can also be extended, with significant work, to handle noise <span class="citation"></span>.</p>
<p>This concludes our brief discussion of linear programming. While compressive sensing is a convenient excuse to discuss this powerful tool, don’t forget that linear programming is useful for solving or approximating a huge range of applications drawn from many different domains. It’s quite likely that one or more problems arising in your future work will be solvable using linear programming. The classic book <span class="citation"></span> remains an excellent introduction to some of the applications.</p>
<h1 id="s:convex">Beyond Linear Programs: Convexity</h1>
<p>We next discuss a generalization of linear programming that captures still more applications, without sacrificing too much computational efficiency. After describing this generalization, we give in Section [s:mc] a representative application, the matrix completion problem.</p>
<h2 id="convex-sets">Convex Sets</h2>
<p>In Week 3 we mentioned that a good rule of thumb is to equate “convex” with “nice” and “non-convex” with “nasty,” especially when optimization is concerned. Back then, we were studying gradient descent, which has much nicer properties for minimizing convex functions than for non-convex functions (in the latter case, the ending point depends on the starting point and might only be a local minimum). Here, convexity is in large part what’s driving the computational tractability of linear programming.</p>
<p>Convexity is relevant for both sets and for functions. Intuitively, a subset <span class="math inline">\(C \sse \RR^n\)</span> is convex if it is “filled in,” meaning that it contains all line segments between its points. See Figure [f:convex] for examples. Formally, <span class="math inline">\(C\)</span> is <span><em>convex</em></span> if for every <span class="math inline">\(\x,\y \in C\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>, <span class="math inline">\(\lambda \x +
(1-\lambda) \y \in C\)</span>. (As <span class="math inline">\(\lambda\)</span> ranges from 0 to 1, it traces out the line segment from <span class="math inline">\(\y\)</span> to <span class="math inline">\(\x\)</span>.)</p>
<div class="figure">
<embed src="convex.pdf" />
<p class="caption">Examples of convex and non-convex sets.<span data-label="f:convex"></span></p>
</div>
<p>For example, the feasible region of every linear program is convex. To see this, first suppose there is only one constraint, which is an inequality. Then the feasible region is just a half-space, which is clearly convex. The feasible region of a linear program is an intersection of such half-spaces. (Note that an equality constraint is equivalent to the combination of two inequality constraints.) The intersection of convex sets <span class="math inline">\(C_1,C_2\)</span> is again convex — if <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> are in both <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>, then the line segment between <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> lies inside both <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> (since each is convex), so this line segment also lies in their intersection. We conclude that every linear program has a convex feasible region.</p>
<p>For a relevant example that is more general than the finite intersection of half-spaces and subspaces, take <span class="math inline">\(C\)</span> to be the set of <span class="math inline">\(n \times n\)</span> symmetric and positive semidefinite (PSD) matrices, viewed as a subset of <span class="math inline">\(\RR^{n^2}\)</span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> It is clear that the set of symmetric matrices is convex — the average of symmetric matrices is again symmetric. It is true but less obvious that the set remains convex under the extra PSD constraint.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> You’ll work with symmetric PSD matrices in Part 3 of Homework #9.</p>
<h2 id="convex-functions">Convex Functions</h2>
<p>Who had the nerve to use the same word “convex” for two totally different things, sets and functions? The overloaded terminology becomes more forgivable if we define a function <span class="math inline">\(f:\RR^n \rightarrow \RR\)</span> to be <span><em>convex</em></span> if and only if the region above its graph is a convex set. See Figure [f:convex2] for some examples.</p>
<div class="figure">
<embed src="convex2.pdf" />
<p class="caption">Examples of convex and non-convex functions.<span data-label="f:convex2"></span></p>
</div>
<p>This definition is equivalent to the one given in Lecture #5, where we said that a convex function is one where all “chords” of its graph lie above the graph. Mathematically, this translates to <span class="math display">\[f(\lambda \x + (1-\lambda)\y) \le
\lambda f(\x) + (1-\lambda) f(\y)\]</span> for every <span class="math inline">\(\x,\y \in C\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>. That is, for points <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>, if you take the average of <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> and then apply <span class="math inline">\(f\)</span>, you’ll get a smaller number than if you first apply <span class="math inline">\(f\)</span> to <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> and then average the results. It’s not always easy to check whether or not a given function is convex, but there is a mature analytical toolbox for this purpose (taught in EE364, for example).</p>
<h2 id="convex-programs">Convex Programs</h2>
<p><span><em>Convexity leads to computational tractability.</em></span> For example, in theory, it is possible to minimize an essentially arbitrary convex function over an essentially arbitrary convex feasible region. (There’s a bit of fine print, but the conditions are quite mild.) This is fantastic news: in principle, we should be able to develop fast and robust algorithms for all of the convex optimization problems that we want to solve.</p>
<p>Practice is in the process of catching up with what the theory predicts. To oversimplify the current state-of-the-art, there are currently solvers that can handle medium-size and sufficiently nice convex optimization problems. The first piece of good news is that this is already enough to solve many problems that we’re interested in; see Homework #9 for a couple of examples. The second piece of good news is that, as we speak, many smart people are working hard to close the gap in computational efficiency between linear and convex programming solvers — we expect major progress on convex solvers over the next 5 or so years.</p>
<p>Summarizing: convex programming is even more general than linear programming and captures some extra interesting applications. It is relatively computationally tractable, although the biggest instance sizes that can be solved are generally one or two orders of magnitude smaller than with linear programming (e.g., tens of thousands instead of millions).</p>
<p>For intuition about why convexity leads to tractability, consider the case where the feasible region or the objective function is <span><em>not</em></span> convex. With a non-convex feasible region, there can be “locally optimal” feasible points that are not globally optimal, even with a linear objective function (Figure [f:nonconvex](left)). As we saw in Week 3, the same problem arises with a non-convex objective function, even when the feasible region is just the real line (Figure [f:nonconvex](right)). When both the objective function and feasible region are convex, this can’t happen — all local optima are also global optima. This makes optimization much easier.</p>
<div class="figure">
<embed src="nonconvex.pdf" />
<p class="caption">Non-convexity and local optima. (Left) A linear (i.e. convex) objective function with a non-convex feasible region. (Right) A non-convex objective function over a convex feasible region (the real line).<span data-label="f:nonconvex"></span></p>
</div>
<h1 id="s:mc">Application: Matrix Completion</h1>
<h2 id="setup-and-motivation">Setup and Motivation</h2>
<p>We conclude the lecture with a case study of convex programming, in the context of another central problem in sparse recovery with incomplete information: <span><em>matrix completion</em></span>. You saw this problem in Week #5, where we approached it using SVD-based techniques. Here, we’ll obtain better results using convex optimization.</p>
<p>Recall the setup: there is an unknown “ground truth” matrix <span class="math inline">\(\M\)</span>, analogous to the unknown sparse signal <span class="math inline">\(\z\)</span> in compressive sensing. The input is a matrix <span class="math inline">\(\Mh\)</span>, derived from <span class="math inline">\(\M\)</span> by erasing some of its entries — the erased values are unknown, and the remaining values are known. The goal is to recover the matrix <span class="math inline">\(\M\)</span> from <span class="math inline">\(\Mh\)</span>.</p>
<p>An example of matrix completion that received a lot of hype is the “Netflix challenge.” Netflix was interested in the matrix <span class="math inline">\(\M\)</span> where rows are customers, columns are movies, and an entry of the matrix describes how much a customer would like a given movie. If a customer has rated a movie, then that entry is known; otherwise, it is unknown. Thus, most of the entries of <span class="math inline">\(\M\)</span> are missing in <span class="math inline">\(\Mh\)</span>. Recovering <span class="math inline">\(\M\)</span> from <span class="math inline">\(\Mh\)</span>, even approximately, would obviously be very useful to Netflix in designing a recommendation system.</p>
<p>Without any assumptions on the ground truth matrix <span class="math inline">\(\M\)</span>, there is no way to recover its missing entries from <span class="math inline">\(\Mh\)</span> — they could be anything, and an algorithm would have no clue about what they are. A similar issue came up in compressive sensing, when we realized that there was no way to recover arbitrary unknown signals of length <span class="math inline">\(n\)</span> while using fewer than <span class="math inline">\(n\)</span> linear measurements. In compressive sensing, we made progress by assuming that the unknown signal was sparse. So what kind of assumption can play the role of sparsity in matrix completion? We could just assume that <span class="math inline">\(\M\)</span> is mostly zeroes, but then we get a stupid problem — presumably the best guess of <span class="math inline">\(\M\)</span> given <span class="math inline">\(\Mh\)</span> would just fill in all the missing entries with zeroes. This hack is unhelpful for the Netflix application, for example.</p>
<p>The key assumption we’ll make is that the unknown matrix <span class="math inline">\(\M\)</span> has low rank. (One can also extend the following results to the case of matrices that are approximately low-rank.) For an extreme example, imagine that we knew that <span class="math inline">\(\M\)</span> was rank one, with all rows multiples of each other. In this case, as we saw in Lecture #9, we can sometimes recover <span class="math inline">\(\M\)</span> from <span class="math inline">\(\Mh\)</span> even when <span class="math inline">\(\Mh\)</span> has very few known entries.</p>
<h2 id="rank-minimization">Rank Minimization</h2>
<p>Given that all we know about the unknown matrix <span class="math inline">\(\M\)</span> is that it agrees with <span class="math inline">\(\Mh\)</span> on the known entries and that it has low rank, we might try to recover <span class="math inline">\(\M\)</span> from <span class="math inline">\(\Mh\)</span> by solving the following optimization problem: <span class="math display">\[\label{eq:rank1}
\min\, \rank(\M)\]</span> subject to <span class="math display">\[\label{eq:rank2}
\text{$\M$ agrees with $\Mh$ on its known entries.}\]</span> This optimization problem has one real-valued decision variable for each unknown entry in <span class="math inline">\(\Mh\)</span>; the known entries can be treated as constants.</p>
<p>Unfortunately, this rank-minimization problem is <span class="math inline">\(NP\)</span>-hard, and no good general-purpose heuristic algorithms are known.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> We confronted a similar issue in compressive sensing, where directly minimizing the sparsity of a solution to a linear system was an <span class="math inline">\(NP\)</span>-hard problem. Our approach there was to relax the <span class="math inline">\(\ell_0\)</span>-minimization problem to the computationally tractable <span class="math inline">\(\ell_1\)</span>-minimization problem. Is there some way we can view matrix rank-minimization as an <span class="math inline">\(\ell_0\)</span>-minimization problem, and then switch to the <span class="math inline">\(\ell_1\)</span> norm instead?</p>
<p>The singular value decomposition (SVD) provides an affirmative answer. Specifically, suppose the unknown <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\M\)</span> has the SVD <span class="math display">\[\M = \U\S\V^T,\]</span> where <span class="math inline">\(\U\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix, <span class="math inline">\(\S\)</span> is an <span class="math inline">\(m \times
n\)</span> diagonal matrix, and <span class="math inline">\(\V\)</span> is an <span class="math inline">\(n \times
n\)</span> orthogonal matrix. Then, the rank of <span class="math inline">\(\M\)</span> is precisely the number <span class="math inline">\(r\)</span> of non-zero singular values (i.e., entries of <span class="math inline">\(\S\)</span>), with every row of <span class="math inline">\(\M\)</span> a linear combination of its top <span class="math inline">\(r\)</span> right singular vectors, and every column of <span class="math inline">\(\M\)</span> a linear combination of its top <span class="math inline">\(r\)</span> left singular vectors.</p>
<p>Writing <span class="math inline">\(\Sigma(\M)\)</span> for the set of singular values of a matrix <span class="math inline">\(\M\)</span>, we can therefore rephrase the optimization problem – as <span class="math display">\[\label{eq:l01}
\min |\supp(\Sigma(\M))|\quad\quad\text{(a.k.a.\ $\|\Sigma(\M)\|_0$)}\]</span> subject to <span class="math display">\[\label{eq:l02}
\text{$\M$ agrees with $\Mh$ on its known entries,}\]</span> which we can view as a <span class="math inline">\(\ell_0\)</span>-minimization problem.</p>
<h2 id="nuclear-norm-minimization">Nuclear Norm Minimization</h2>
<p>Following in our compressive sensing footsteps, we now consider the analogous <span class="math inline">\(\ell_1\)</span>-minimization problem, where we just change the 0-norm to the 1-norm: <span class="math display">\[\label{eq:l11}
\min \|\Sigma(\M)\|_1\]</span> subject to <span class="math display">\[\label{eq:l12}
\text{$\M$ agrees with $\Mh$ on its known entries.}\]</span> This optimization problem is called <span><em>nuclear norm minimization</em></span>.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> It minimizes the sum of the singular values subject to consistency with the known information. Since <span class="math inline">\(\ell_1\)</span>-minimization promotes sparse solutions, we might hope that solving the problem – leads, under reasonable conditions, to the sparsest (i.e., minimum-rank) solution.</p>
<p>The following fact is non-obvious but can be proved using the convexity toolbox (e.g. from EE364) mentioned earlier.</p>
<p>[fact:nn] The objective function  is convex.</p>
<p>Fact [fact:nn] implies that the optimization problem – is convex and hence can be solved relatively efficiently.</p>
<p>Since 2008 <span class="citation"></span>, there has been significant progress on identifying sufficient conditions on the matrix <span class="math inline">\(\M\)</span> and the number of known entries such that the optimization problem – successfully recovers <span class="math inline">\(\M\)</span>. A typical guarantee is the following.</p>
<p>[t:mc] Assume that:</p>
<ol>
<li><p>The unknown matrix <span class="math inline">\(\M\)</span> has rank <span class="math inline">\(r\)</span>.</p></li>
<li><p>The matrix <span class="math inline">\(\Mh\)</span> includes at least <span class="math inline">\(\Omega(r(m+n) \log^2 (m+n))\)</span> known entries, chosen uniformly at random from <span class="math inline">\(\M\)</span>.</p></li>
<li><p><span class="math inline">\(\M\)</span> is sufficiently dense and non-pathological.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p></li>
</ol>
<p>It is clear that some version of the second condition is needed — with too few known entries, there’s no way to recover <span class="math inline">\(\M\)</span> from <span class="math inline">\(\Mh\)</span>. For low-rank matrices <span class="math inline">\(\M\)</span>, the required number of known entries is impressively small — sublinear in the number <span class="math inline">\(mn\)</span> of <span class="math inline">\(\M\)</span>’s entries. Given the small number of known entries, it is also clear that some version of the third condition is needed. If <span class="math inline">\(\M\)</span> is too sparse, like a diagonal matrix, then the randomly sampled known entries will likely all be zeroes.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Recall <span class="math inline">\(\z\)</span> is <span> <em><span class="math inline">\(k\)</span>-sparse</em></span> if it has at most <span class="math inline">\(k\)</span> non-zeroes.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The proof is not easy, and is beyond the scope of CS168. Our focus is on interpretations, applications, and algorithms.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Called a “polyhedron;” in the common special case where the feasible region is bounded, it is called a “polytope.”<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>The earliest, from 1979, is the “ellipsoid method” <span class="citation"></span>; this was a big enough deal at the time that it made the New York Times <span class="citation"></span>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>The open-source solvers are not as good, unfortunately, but are still useful for solving reasonably large linear programs (see Homework #9).<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Kudos to the reader who is bothered by this: linear programming is a beautiful subject, and we strongly encourage the reader to take a class or read a book (like <span class="citation"></span>) on the subject.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>There are many equivalent definitions of PSD matrices. One of the simplest is as the matrices of the form <span class="math inline">\(\A^T\A\)</span>, like the covariance matrices we were looking at during our PCA discussions in Lectures #7–9.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Another definition of PSD matrices is as the matrices <span class="math inline">\(\A\)</span> for which the corresponding quadratic form <span class="math inline">\(\x^T\A\x\)</span> is nonnegative for every <span class="math inline">\(\x \in \RR^n\)</span>. Using linearity, it is easy to see that the average of two matrices that satisfy this condition yields another matrix that satisfies the condition.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Relatedly, its objective function is non-convex (i.e., “nasty”). For example, the average of rank-1 matrices need not be a rank-1 matrix (why?).<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>One also hears about <span><em>trace minimization</em></span>, a closely related optimization problem.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>The formal term is “incoherent,” which roughly means that the rows of <span class="math inline">\(\M\)</span> are not well-aligned with the standard basis vectors. This is similar to the assumption on the measurement matrix in Theorem [t:cs2].<a href="#fnref11">↩</a></p></li>
</ol>
</div>

{% endraw %}
