---
layout: post
title: "Lecture 17 : Compressive Sensing"
author: Tim
---
{% raw %}

<h1 id="s:sparsity">Sparsity</h1>
<p>Data analysis is only interesting when the data has structure — there’s not much you can do with random noise. Ideally, when data has nice structure, it can be both detected and exploited algorithmically.</p>
<p>“Structure” can mean many things. One major theme in data analysis, implicit in many recent lectures, is <span><em>sparsity</em></span>. In the simplest case, the raw data — vectors in <span class="math inline">\(\RR^n\)</span>, say — are sparse, meaning that the vectors consist mostly of zeroes. For example, in Homework #2, when you looked at terms used in various newsgroups, you learned that rather than explicitly storing all of the zeroes of a sparse vector, it’s more efficient to store just the non-zero values and their coordinates.</p>
<p>In recent lectures, we’ve been thinking about data that becomes sparse after a suitable change of basis, meaning that in the new basis most of the coefficients of most vectors are zero. For example, last week we discussed looking at data in the Fourier basis (or “frequency domain”), and Greg showed how an audio recording of him whistling for 5 seconds was dense in the time domain but sparse (just 1-2 spikes) in the frequency domain. We knew to check the representation in the Fourier domain because of the physics of sound. Similarly, properties of real-world images suggest re-representing them in the wavelet basis (mentioned last lecture), where they are typically sparse.</p>
<p>Data is often only <span><em>approximately</em></span> sparse. Rather than being mostly zeroes, the data (perhaps after a change of basis) consist mostly of relatively small numbers. Such almost sparse data admit good compression and good de-noising — after expressing the data so that they are almost sparse, one can just zero out the small coefficients and remember only the large ones to get a compressed or de-noised version of the data. We’ve already seen a couple of examples of this paradigm.</p>
<p>For example, in principal components analysis (PCA), one doesn’t have advance knowledge of the basis in which the data is spares — the goal of the method is to examine the data set and identify the “best” basis for approximately representing it as linear combinations of small set of <span class="math inline">\(k\)</span> orthonormal vectors. The projection of the original data points onto the top <span class="math inline">\(k\)</span> principal components can be be thought of as a sparse approximation (with only <span class="math inline">\(k\)</span> non-zeros per data point in the new basis) of the original data set. Low-rank matrix approximations — computing the singular value decomposition (SVD) of a matrix and retaining only its top <span class="math inline">\(k\)</span> singular vectors and values — can also be viewed in this way. Here, matrix rank playing the role analogous to vector sparsity (recall rank <span class="math inline">\(k\)</span> means only <span class="math inline">\(k\)</span> non-zero singular values).</p>
<h1 id="s:apps">The Idea and Applications of Compressive Sensing</h1>
<p>The usual approach to compressing approximately sparse data is to first collect the raw data, and then to compress it in software. For example, your smartphone initially captures an image in the standard pixel basis (with red, green, and blue intensities for each pixel). Subsequently, using a standard image compression algorithm (e.g., JPEG), the raw image is compressed, which typically reduces the image size by a large factor (15+). At a high level, such compression algorithms follow the paradigm outlined in Section [s:sparsity] — expressing the image in a suitable basis (like a wavelet basis) so that it is almost sparse, and then retaining only the components that have sufficiently large coefficients. This two-step approach works fine in many applications, but it’s hard not to wonder if the first step can be made more efficient. If most of the raw data is going to be thrown out immediately, before any processing, why did we bother to collect it all in the first place?</p>
<p>The main idea of <span><em>compressive sensing</em></span>, also called “compressed sensing,” is to directly capture data in a compressed form. The theory of compressive sensing is barely a decade old, so the engineering viability of this idea is still being worked out in different application domains. At present, the key ideas seem well on their way to practical use and commercialization.</p>
<p>Here are some potential applications of compressive sensing — of collecting less information in cases where you know that the data is sparse in a suitable basis.</p>
<ol>
<li><p><span><em>Cameras that use less power.</em></span> The power used by your smartphone to capture an image, or a frame of a video, is driven by the number of analog-to-digital conversions that it has to do, which in turn is proportional to the number of pixels it uses. Compressive sensing suggests the possibilities of using fewer pixels without degrading image quality, resulting in qualitatively less battery drain for a given number of photos or videos. There was an early (meaning 2007) proof-of-concept prototype of a “single-pixel” camera built at Rice <span class="citation"></span>; recently, new developments in chip fabrication suggest that commercialization of these ideas could occur in the near future.</p></li>
<li><p><span><em>MRI and tomography.</em></span> In magnetic resonance imaging (MRI), a powerful magnet and radio waves are used to image parts of the human body (especially tendons, cartridge, joints, etc.). The scan time is proportional to the number of measurements (or “slices”) taken, and can easily be 30 minutes or more. Compressive sensing techniques have been tried out in some hospitals, and they have sped up scan times by a constant factor. This may not matter much for an MRI on an athlete’s knee, but when the patient is a child and/or is not allowed to breathe during the scan (e.g., for a lung scan), a constant factor speed-up can be a big deal.</p></li>
<li><p><span><em>Demystifying undersampling.</em></span> There have been examples in different fields where accurate inference is possible with far less information that would seem to be necessary. One example is in geophysics, in the business of drilling for oil. In the late 1970s and 1980s, geophysicists started trying to figure out what layers of rock strata looked like by sending seismic waves into the ground, at various frequencies.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Such seismic waves pass through homogeneous rock without much ado, but when they hit a discontinuity between different layers of rock — also where oil deposits are more likely to be — there is a reflection back to the surface.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Even with very few samples — meaning sending waves at just a few different frequencies — the geophysicists obtained quite accurate reconstructions of the rock layers (which could be confirmed during later drilling). They even came up with the linear programming-based algorithm that we’ll discuss next lecture! The working hypothesis for the efficacy of using few samples was that the structure of the rock layers was typically “simple;” compressive sensing provides a mathematical formulation and proof of this intuition.</p></li>
</ol>
<h1 id="the-setup">The Setup</h1>
<p>We give a mathematical description of the compressive sensing problem. At a high level, the goal is to design a small number of “linear measurements” such that their results enable the unique and efficient reconstruction of an unknown sparse signal. After this description, we’ll relate it back to the applications in Section [s:apps].</p>
<ul>
<li><p><span><em>Step 1:</em></span> Design <span class="math inline">\(m\)</span> “linear measurements” <span class="math inline">\(\a_1,\ldots,\a_m \in \RR^n\)</span>.</p></li>
<li><p><span><em>Step 2:</em></span> “Nature” picks an unknown “signal,” meaning an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\z \in \RR^n\)</span>.</p></li>
<li><p><span><em>Step 3:</em></span> Receive the measurement results <span class="math inline">\(
b_1 = {
{\langle {\a_1} , {\z} \rangle}
}, b_2 = {
{\langle {\a_2} , {\z} \rangle}
}, \ldots b_m = {
{\langle {\a_m} , {\z} \rangle}
}\)</span>.</p></li>
<li><p><span><em>Step 4:</em></span> Recover the <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\z\)</span> from the <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\b\)</span>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p></li>
</ul>
<p>Note that the goal is to design a <span><em>single</em></span> set of measurements <span class="math inline">\(\a_1,\ldots,\a_m\)</span> that “works” for <span><em>all</em></span> signals <span class="math inline">\(\z\)</span>, in the sense that Step 4 should be possible.</p>
<p>For instance, in the camera example, we think of <span class="math inline">\(\z\)</span> as the raw image, in the standard pixel basis. Then, each <span class="math inline">\(\a_i\)</span> specifies a linear combination of the pixels. Various approaches are being explored for implementing such linear combinations in hardware. In Rice’s single-pixel camera, they were implemented using tiny mirrors <span class="citation"></span>. More recently, researchers have devised on-chip hardware tailored for such linear combinations <span class="citation"></span>.</p>
<p>In the tomography example, <span class="math inline">\(\z\)</span> is the “true image” of whatever’s being scanned. Medical imaging devices already report what are essentially linear measurements of <span class="math inline">\(\z\)</span> — this is in the spirit of the linearity of the Fourier transform discussed last week, with a different but analogous transform. (We’ll return to the geophysicists in a bit.)</p>
<h1 id="recovering-sparse-signals-main-result-and-interpretations">Recovering Sparse Signals: Main Result and Interpretations</h1>
<p>In the fourth and final step of the setup, the goal translates to solving a linear system <span class="math inline">\(\Ax = \b\)</span> in the variables <span class="math inline">\(\x\)</span>, where the <span class="math inline">\(m \times n\)</span> constraint matrix <span class="math display">\[\A =
\left[
  \begin{array}{ccc}
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \a_1 &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \a_2 &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    &amp; \vdots &amp; \\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \a_m &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\end{array}
\right].\]</span> was designed up front and the right-hand size <span class="math inline">\(\b = \A\z\)</span> is given. Solving linear systems is a very well-studied problem and you’ve surely seen algorithms for it, for example Gaussian elimination.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p>Can we design <span class="math inline">\(\A\)</span> so that it’s easy to recover <span class="math inline">\(\z\)</span> from <span class="math inline">\(\b\)</span>? Without further restrictions, this is a stupid question, and the answer is obviously yes. Taking <span class="math inline">\(\A = \I\)</span>, the <span class="math inline">\(n \times n\)</span> identity matrix, <span class="math inline">\(\b = \Az\)</span> will simply be <span class="math inline">\(\z\)</span> — the unknown signal is handed to us on a silver platter. In this solution, the <span class="math inline">\(i\)</span>th linear measurement <span class="math inline">\({
{\langle {\a_i} , {\z} \rangle}
}\)</span> returns the <span class="math inline">\(i\)</span>th coordinate of <span class="math inline">\(\z\)</span>, so <span class="math inline">\(n\)</span> linear measurements are used.</p>
<p>Recall that our goal is to minimize the number of measurements used — corresponding to the power drain in a smartphone camera or the scan time of a MRI machine. We’ve seen that <span class="math inline">\(n\)</span> linear measurements are sufficient; can we get away with fewer? The issue with taking <span class="math inline">\(m
&lt; n\)</span> is that the linear system <span class="math inline">\(\Ax = \b\)</span> becomes underdetermined. While <span class="math inline">\(\z\)</span> remains a feasible solution of the linear system (by the definition of <span class="math inline">\(\b\)</span>), every vector <span class="math inline">\(\z + \w\)</span> with <span class="math inline">\(\w\)</span> in the <span class="math inline">\((n-m)\)</span>-dimensional kernel of <span class="math inline">\(\A\)</span> is also a solution.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> This means we can’t reconstruct <span class="math inline">\(\z\)</span>, since we have no idea which of the infinitely many solutions to <span class="math inline">\(\Ax=\b\)</span> is the real unknown signal.</p>
<p>Summarizing, to reconstruct arbitrary unknown signals <span class="math inline">\(\z \in \RR^n\)</span>, <span class="math inline">\(n\)</span> linear measurements are both necessary and sufficient. But recall the discussion in Section [s:sparsity] — many signals of interest are (approximately) sparse in a suitable basis. Perhaps we can get away with fewer than <span class="math inline">\(n\)</span> measurements if we only care about reconstructing sparse signals?</p>
<p>For the rest of lecture, we assume that the unknown signal <span class="math inline">\(\z\)</span> is <span> <em><span class="math inline">\(k\)</span>-sparse</em></span> in the standard basis, meaning that it has at most <span class="math inline">\(k\)</span> non-zero coordinates. For example, <span class="math inline">\(\z\)</span> could be a black and white image, with a black background (of 0-pixels) with lines of white (1) pixels. The parameter <span class="math inline">\(k\)</span> could be anything between 0 and <span class="math inline">\(n\)</span>, but you should think of it as much less than <span class="math inline">\(n\)</span>, for example <span class="math inline">\(\sqrt{n}\)</span> or even <span class="math inline">\(\log_2 n\)</span>. We focus on sparsity in the standard basis for simplicity only. Importantly, everything we’ll say below also applies when <span class="math inline">\(\z\)</span> is <span class="math inline">\(k\)</span>-sparse in a different basis, for example the Fourier basis (as with Greg’s whistle). The only difference is that one needs to throw in an extra change-of-basis matrix into some of the statements below.</p>
<p>The main theorem in compressive sensing, at least for the purposes of this lecture, provides a resounding “yes” to the question of whether or not signal sparsity allows for fewer linear measurements.</p>
<p>[t:cs] Fix a signal length <span class="math inline">\(n\)</span> and a sparsity level <span class="math inline">\(k\)</span>. Let <span class="math inline">\(\A\)</span> be an <span class="math inline">\(m
\times n\)</span> matrix with <span class="math inline">\(m = \Theta(k \log \tfrac{n}{k})\)</span> rows, with each of its <span class="math inline">\(mn\)</span> entries chosen independently from the standard Gaussian distribution.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> With high probability over the choice of <span class="math inline">\(\A\)</span>, every <span class="math inline">\(k\)</span>-sparse signal <span class="math inline">\(\z\)</span> can be efficiently recovered from <span class="math inline">\(\b =
\Az\)</span>.</p>
<p>In Theorem [t:cs], a single matrix <span class="math inline">\(\A\)</span> — a fixed set of <span class="math inline">\(m =
\Theta(k \log \tfrac{n}{k})\)</span> linear measurements — “works” simultaneously for all <span class="math inline">\(k\)</span>-sparse signals <span class="math inline">\(\z\)</span>, allowing efficient recovery of <span class="math inline">\(\z\)</span> from <span class="math inline">\(\b\)</span>. (Even though there’s zillions of different solutions to <span class="math inline">\(\Ax=\b\)</span>!) Note that for all interesting values of <span class="math inline">\(k\)</span>, the number <span class="math inline">\(m\)</span> of measurements used is way less than <span class="math inline">\(n\)</span> — so signal sparsity indeed radically reduces the number measurements needed. For example, if <span class="math inline">\(n\)</span> is a million, with <span class="math inline">\(k = \sqrt{n}\)</span> only tens of thousands of measurements are needed, and with <span class="math inline">\(k = \log_2 n\)</span> only hundreds are needed.</p>
<p>The proof of Theorem [t:cs] is beyond the scope of this course; we’ll focus instead on interpreting it and discussing the algorithms used for recovery. Some important comments:</p>
<ol>
<li><p>Theorem [t:cs] extends to “almost <span class="math inline">\(k\)</span>-sparse” signals <span class="math inline">\(\z\)</span>. This is crucial for practical applications — approximately sparse signals are far more common than exactly sparse ones. By “approximately <span class="math inline">\(k\)</span>-sparse,” we mean that the sum of the magnitudes of the <span class="math inline">\(k\)</span> largest coordinates of <span class="math inline">\(\z\)</span> is much larger than the sum of the magnitudes of the rest of the coordinates of <span class="math inline">\(\z\)</span>. The recovery guarantee changes, as you might expect: if <span class="math inline">\(\z\)</span> is only approximately <span class="math inline">\(k\)</span>-sparse, then the algorithm returns a vector <span class="math inline">\(\z&#39;\)</span> that is only approximately equal to <span class="math inline">\(\z\)</span>. (If you’re lucky, you might even get back a sparser, “de-noised” version of <span class="math inline">\(\z\)</span>.) The guarantee degrades gracefully: for <span class="math inline">\(\z\)</span>’s further and further away from sparse vectors, the guarantee for the returned <span class="math inline">\(\z&#39;\)</span> grows weaker and weaker. In the extreme case, where <span class="math inline">\(\z\)</span> isn’t sparse at all, then since <span class="math inline">\(m &lt; n\)</span> our previous discussion implies that the returned vector <span class="math inline">\(\z&#39;\)</span> might have nothing to do with <span class="math inline">\(\z\)</span>.</p></li>
<li><p>The number <span class="math inline">\(m = \Theta(k \log \tfrac{n}{k})\)</span> of linear measurements used in Theorem [t:cs] is optimal up to a constant factor, at least if one also wants the approximate recovery of approximately sparse vectors.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<p>For some approximate intuition about where the mysterious <span class="math inline">\(\Theta(k
\log \tfrac{n}{k})\)</span> term comes from, recall the proof that every comparison-based sorting algorithm requires <span class="math inline">\(\Omega(n \log n)\)</span> comparisons: a correct algorithm distinguishes between the <span class="math inline">\(n!\)</span> possible relative orderings of the <span class="math inline">\(n\)</span> input elements, and each comparison might be resolved in a way that only reduces the number of remaining possibilities by a factor of 2 (or less). Thus, at least <span class="math inline">\(\log_2 (n!) = \Theta(n \log n)\)</span> comparisons are needed in the worst case. In the present context, there are an infinite number of possibilities for the unknown <span class="math inline">\(k\)</span>-sparse vector <span class="math inline">\(\z\)</span>, but if we bucket these by support (i.e., the set of non-zero coordinates) then there are <span class="math inline">\(\binom{n}{k}\)</span> different possibilities that our linear measurements must differentiate. Thinking of a linear measurement as analogous to a comparison, we might guess that at least <span class="math inline">\(\log_2
\binom{n}{k}\)</span> measurements are required to correctly reconstruct all possible <span class="math inline">\(k\)</span>-sparse signals. (You can use Stirling’s approximation to verify that <span class="math inline">\(\log_2 \binom{n}{k} = \Theta(k \log \tfrac{n}{k})\)</span>.) This analogy is not entirely accurate, as each linear measurement returns a real value, not just a bit. (This is why smaller values of <span class="math inline">\(m\)</span>, even <span class="math inline">\(m=2k\)</span>, are sufficient for recovering exactly sparse vectors.) Still, a non-trivial proof shows that for recovering approximately sparse vectors, <span class="math inline">\(m = \Theta(\log_2 \binom{n}{k})\)</span> is indeed the correct answer.</p></li>
<li><p>The assumption that the entries of <span class="math inline">\(\A\)</span> are i.i.d. Gaussians is not crucial — many matrices <span class="math inline">\(\A\)</span> work, and there is now a reasonable understanding of such matrices. Indeed, if the linear measurements are implemented in hardware, then it may be infeasible to implement Gaussian coefficients. In the camera application, it makes sense to use coefficients with fewer bits, ideally just in <span class="math inline">\(\{-1,0,1\}\)</span>. In the MRI example, one doesn’t even have the luxury of using random linear measurements — instead one is stuck with the measurements performed by the MRI machine, which are similar to the rows of the Fourier matrix <span class="math inline">\(\M_n\)</span> discussed last week (Figure [f:sparse](a)). Fortunately, there are slightly weaker versions of Theorem [t:cs] that also apply to these sorts of linear measurements.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>One type of matrix <span class="math inline">\(\A\)</span> that certainly <span><em>doesn’t</em></span> work is a sparse matrix. For example, imagine that <span class="math inline">\(\A\)</span> is a random subset of not too many rows of the identity matrix <span class="math inline">\(\I_n\)</span> (Figure [f:sparse](b)). <span class="math inline">\(\Az\)</span> is then just the projection onto the coordinates of <span class="math inline">\(\z\)</span> corresponding to the chosen rows of <span class="math inline">\(\I_n\)</span>. Plenty of sparse vectors <span class="math inline">\(\z\)</span> will be 0 in every one of these coordinates, in which case <span class="math inline">\(\b=0\)</span>. There is no hope of reconstructing <span class="math inline">\(\z\)</span> from <span class="math inline">\(\b\)</span> in this case.</p>
<p>Roughly, generalized versions of Theorem [t:cs] say that as long as <span class="math inline">\(\A\)</span> is sufficiently dense and not pathological, it enables the recovery of sparse signals. More generally, if <span class="math inline">\(\z\)</span> is a signal that is sparse in some other basis, and <span class="math inline">\(\A\)</span> is dense and non-pathological after changing to the relevant basis, then <span class="math inline">\(\z\)</span> can be recovered from the linear measurements in <span class="math inline">\(\A\)</span>.</p></li>
</ol>
<p>A related concept is the <span><em>uncertainty principle</em></span>, which states that no signal <span class="math inline">\(\z\)</span> can be “localized” (i.e., made sparse) simultaneously in both the time and frequency domains. We saw a vivid illustration of this last week: Greg’s whistle was sparse in the frequency domain, but dense in the time domain; while the series of percussive noises was sparse in the time domain (with one spike for each noise) but dense in the frequency domain. The uncertainty principle is a rigorous result stating that <span><em>every</em></span> signal is dense in the time domain or in the frequency domain (or both).</p>
<p>Now, back to the geophysicists. Their unknown signal <span class="math inline">\(\z\)</span> was the structure of rock strata, and it was typically sparse in the spatial domain (analogous to percussive noises) — mostly chunks of homogeneous rock, separated from each other by the occasional discontinuity (Figure [f:strata]). Their measurements — seismic waves — were sparse in the frequency domain. By the uncertainty principle, when these measurements are re-represented in the spatial domain — the basis in which the signal is sparse — they are dense (and presumably not pathological). Theorem [t:cs] and its generalizations suggest that relatively few measurements should suffice for recovering the signal, corroborating what was observed empirically.</p>
<div class="figure">
<embed src="strata.pdf" />
<p class="caption">The structure of rock strata is often pretty simple. When a seismic wave hits a discontinuity, there is both a reflection and a refraction.<span data-label="f:strata"></span></p>
</div>
<h1 id="recovery-via-ell_1-minimization">Recovery via <span class="math inline">\(\ell_1\)</span> Minimization</h1>
<h2 id="recovery-via-optimization">Recovery via Optimization</h2>
<p>Next, we study how to recover a <span class="math inline">\(k\)</span>-sparse signal <span class="math inline">\(\z\)</span> from <span class="math inline">\(\b =
\Az\)</span>, where <span class="math inline">\(\A\)</span> is chosen as in Theorem [t:cs]. Since <span class="math inline">\(\A\)</span> has many fewer rows than columns (recall <span class="math inline">\(m = \Theta(k \log
\tfrac{n}{k})\)</span>), <span class="math inline">\(\Ax = \b\)</span> is an underdetermined linear system with tons of solutions (<span class="math inline">\(\z\)</span>, and lots of others). We need some principled way to select one feasible solution out of the many — hopefully it will be <span class="math inline">\(\z\)</span>. A natural and flexible way to do this is via optimization — to posit an objective function defined on the feasible solutions to <span class="math inline">\(\Ax = \b\)</span> and single out the “best” one.</p>
<p>More precisely, for some real-valued function <span class="math inline">\(f:\RR^n \rightarrow
\RR\)</span>, we consider optimization problems of the form <span class="math display">\[\min f(\x)\]</span> subject to <span class="math display">\[\Ax = \b.\]</span> (We could also maximize a function <span class="math inline">\(f\)</span>, but this is equivalent to minimizing <span class="math inline">\(-f\)</span>.) We talked about optimization before, in Week 3 in the context of gradient descent. There, we only considered <span><em>unconstrained</em></span> optimization (like finding regression coefficients), whereas here we need to constrain the recovered signal <span class="math inline">\(\x\)</span> to be consistent with the observed measurements <span class="math inline">\(\b\)</span>.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>What objective function <span class="math inline">\(f\)</span> should we use? All we know about the unknown signal <span class="math inline">\(\z\)</span> is that it satisfies <span class="math inline">\(\Az = \b\)</span> and that <span class="math inline">\(\z\)</span> is <span class="math inline">\(k\)</span>-sparse. This suggests that we want to look for the sparsest solution to <span class="math inline">\(\Ax = \b\)</span>: <span class="math display">\[\min |\supp(\x)|\]</span> subject to <span class="math display">\[\Ax = \b,\]</span> where <span class="math inline">\(\supp(\x)\)</span> denotes the <span><em>support</em></span> of <span class="math inline">\(\x\)</span>, meaning the set of coordinates for which it is non-zero. The quantity <span class="math inline">\(|\supp(\x)|\)</span> is sometimes called the <span class="math inline">\(\ell_0\)</span> norm of <span class="math inline">\(\x\)</span>, so we can call this problem <span><em><span class="math inline">\(\ell_0\)</span>-minimization</em></span>.</p>
<p>Unfortunately, <span class="math inline">\(\ell_0\)</span>-minimization is <span class="math inline">\(NP\)</span>-hard <span class="citation"></span>. So the next idea is to replace the <span class="math inline">\(\ell_0\)</span> norm with a more tractable norm. An important lesson of this lecture is that minimizing the <span class="math inline">\(\ell_1\)</span> norm, <span class="math inline">\(\|\x\|_1 = \sum_{j=1}^n |x_j|\)</span>, has two non-obvious and very useful properties:</p>
<ul>
<li><p>Minimizing the <span class="math inline">\(\ell_1\)</span> norm of a solution promotes sparsity.</p></li>
<li><p>The <span class="math inline">\(\ell_1\)</span> norm can be minimized by computationally efficient algorithms.</p></li>
</ul>
<p>In comparison, the <span class="math inline">\(\ell_0\)</span> norm satisfies property (1) but not (2). Minimizing the <span class="math inline">\(\ell_2\)</span> norm satisfies property (2) — the singular value decomposition (SVD) discussed in Lecture #9 can be used to give a slick solution — but as we’ll see shortly, this does not lead to property (1).</p>
<h2 id="geometry-with-the-ell_1-norm">Geometry with the <span class="math inline">\(\ell_1\)</span> Norm</h2>
<p>We next explain property (1), that <span class="math inline">\(\ell_1\)</span> minimization tends to promote sparse solutions. First we recall an exercise we did way back in Lecture #3, comparing the unit balls under various norms. See Figure [f:balls] for an illustration in 2 dimensions.</p>
<div class="figure">
<embed src="balls.pdf" />
<p class="caption">The unit balls in <span class="math inline">\(\RR^2\)</span> under the <span class="math inline">\(\ell_2\)</span>, <span class="math inline">\(\ell_{\infty}\)</span>, and <span class="math inline">\(\ell_1\)</span> norms.<span data-label="f:balls"></span></p>
</div>
<p>Imagine that you tied a dog to a pole at the origin using a leash with unit <span class="math inline">\(\ell_p\)</span> norm. With <span class="math inline">\(p=2\)</span>, the dog can roam the same amount in every direction. With <span class="math inline">\(p={\infty}\)</span>, the dog can travel the farthest (in <span class="math inline">\(\ell_2\)</span> distance) in the northwest, northeast, southwest, and southeast directions. With <span class="math inline">\(p=1\)</span>, the dog can stray farthest (in <span class="math inline">\(\ell_2\)</span> distance) along the standard basis vectors.</p>
<p>Now consider the set <span class="math inline">\(\{ \x \,:\, \Ax = \b \}\)</span> of feasible solutions to a linear system. In our context, this set is the <span class="math inline">\((n-m)\)</span>-dimensional kernel of <span class="math inline">\(\A\)</span>, shifted by <span class="math inline">\(\z\)</span> (an affine subspace). See Figure [f:balloon]. Let’s examine the optimal solution of the optimization problem <span class="math display">\[\min \|\x\|_p\]</span> subject to <span class="math display">\[\Ax = \b,\]</span> as a function of the choice of norm <span class="math inline">\(p\)</span>. Geometrically, we’ll think about blowing up a balloon centered at the origin, where the shape of the balloon is that of the unit ball in the given norm. If we’ve blown up the balloon to a radius (in <span class="math inline">\(\ell_p\)</span> norm) of <span class="math inline">\(r\)</span>, then the intersection of the balloon and <span class="math inline">\(\{ \x \,:\, \Ax = \b \}\)</span> is exactly the set of norm-<span class="math inline">\(r\)</span> solutions to <span class="math inline">\(\Ax = \b\)</span>. Thus, the minimum-norm solution to <span class="math inline">\(\Ax = \b\)</span> is the first point of contact between the balloon, as we blow it up, and the affine subspace of feasible solutions.</p>
<div class="figure">
<embed src="balloon.pdf" />
<p class="caption">For a given choice of norm, the minimum-norm feasible solution is the first point of contact between a blown-up balloon and the affine subspace of feasible solutions.<span data-label="f:balloon"></span></p>
</div>
<p>For example, consider the case depicted in Figure [f:balloon], where points live in two dimensions and the feasible region is one-dimensional (a line). With <span class="math inline">\(p=2\)</span>, where the balloon is expanding at the same rate in all directions, the minimum-norm feasible solution is the point that forms a line with the origin perpendicular to the feasible region. With <span class="math inline">\(p=\infty\)</span>, the relevant direction in which the balloon is expanding the quickest is northeast, so the optimal point is the intersection of this direction with the feasible region. With <span class="math inline">\(p=1\)</span>, the ball is expanding the quickest along the standard basis vectors, so the first intersection occurs along the <span class="math inline">\(y\)</span>-axis. The optimal solution under the <span class="math inline">\(p=2,\infty\)</span> norms have full support (non-zero <span class="math inline">\(x\)</span>- and <span class="math inline">\(y\)</span>-coordinates), while the optimal solution under the <span class="math inline">\(\ell_1\)</span> norm has a support size of only 1. The reader is encouraged to repeat this thought experiment in three dimensions — where the <span class="math inline">\(\ell_1\)</span> ball is an octahedron — with one- and two-dimensional subspaces of feasible solutions.</p>
<p>Summarizing, because the <span class="math inline">\(\ell_1\)</span> ball expands more quickly in directions with smaller support than in directions with larger support, it makes sense that minimizing the <span class="math inline">\(\ell_1\)</span> norm generally favors sparser solutions. Indeed, <span class="math inline">\(\ell_1\)</span> minimization is precisely the algorithm used for the guarantee in Theorem [t:cs]. We conclude the lecture by sharpening the theorem statement to take this into account.</p>
<p>[t:cs2] Fix a signal length <span class="math inline">\(n\)</span> and a sparsity level <span class="math inline">\(k\)</span>. Let <span class="math inline">\(\A\)</span> be an <span class="math inline">\(m
\times n\)</span> matrix with <span class="math inline">\(m = \Theta(k \log \tfrac{n}{k})\)</span> rows, with each of its <span class="math inline">\(mn\)</span> entries chosen independently from the standard Gaussian distribution. With high probability over the choice of <span class="math inline">\(\A\)</span>, for every <span class="math inline">\(k\)</span>-sparse signal <span class="math inline">\(\z\)</span>, the unique optimal solution to the <span class="math inline">\(\ell_1\)</span>-minimization problem <span class="math display">\[\min \|\x\|_1\]</span> subject to <span class="math display">\[\Ax = \b\]</span> is <span class="math inline">\(\z\)</span>.</p>
<p>Next lecture we’ll explain how to solve the <span class="math inline">\(\ell_1\)</span>-minimization problem efficiently using linear programming. We’ll also consider a different type of sparse recovery problem, that of matrix completion.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For example, by setting off some dynamite!<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Think of light going from air to water — some of it reflects back, some of it refracts in the new medium.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>There is a loose analogy between this setup and that of error-correcting codes. With codes, one designs an encoding function (analogous to our mapping <span class="math inline">\(\z \rightarrow \b\)</span>) such that, given any codeword that hasn’t been corrupted too much, one can uniquely and computationally efficiently decode (for us, map <span class="math inline">\(\b\)</span> back to <span class="math inline">\(\z\)</span>).<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>The singular value decomposition (SVD) also leads to a good algorithm for solving linear systems; see e.g. <span class="citation"></span>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Recall that the kernel of a matrix <span class="math inline">\(\A\)</span> is the set of vectors <span class="math inline">\(\w\)</span> such that <span class="math inline">\(\A\w = \zero\)</span>.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>You’ve seen this matrix before, in a quite different context — the Johnson-Lindenstrauss (JL) Lemma for Euclidean distance-preserving dimensionality reduction (Lecture #4). While compressive sensing and dimensionality reduction seems like quite different goals, there are deep connections between them (see <span class="citation"></span>).<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>For the weaker and less practically relevant goal of reconstructing only exactly <span class="math inline">\(k\)</span>-sparse vectors, it turns out that <span class="math inline">\(m=2k\)</span> measurements are necessary and sufficient (see <span class="citation"></span>)!<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Precisely, randomly choosing <span class="math inline">\(m\)</span> rows of the Fourier matrix (with <span class="math inline">\(m\)</span> a bit larger than <span class="math inline">\(k\)</span>, but much less than <span class="math inline">\(n\)</span>) yields, with high probability, a matrix <span class="math inline">\(\A\)</span> such that every <span class="math inline">\(k\)</span>-sparse vector <span class="math inline">\(\z\)</span> can be recovered from <span class="math inline">\(\b =
  \Az\)</span>.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>There are various ways to turn constrained optimization problems into unconstrained ones (like Lagrangian relaxation), but more direct solutions are preferred for most sparse recovery problems.<a href="#fnref9">↩</a></p></li>
</ol>
</div>

{% endraw %}
