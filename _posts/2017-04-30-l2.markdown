---
layout: post
title: "Lecture 2 : Approximate Heavy Hitters and the Count-Min Sketch"
---

## The Heavy Hitters Problem

### Finding the Majority Element {#sec1-1}

Let's begin with a problem that many of you have seen before.
It's a common question in technical interviews.  You're given as
input an array $$A$$ of length $$n$$, with the promise that it has a *majority element* --- a value that is repeated in strictly more than
$$n/2$$ of the array's entries.  Your task is to find the majority element.

In algorithm design, the usual "holy grail" is a linear-time
algorithm.  For this problem, your post-CS161 toolbox already contains
a subroutine that gives a linear-time solution --- just compute the
median of $$A$$.  (Note: it must be the majority element.)  So let's be
more ambitious: can we compute the majority element with a single
left-to-right pass through the array?
If you haven't seen it before, here's the solution:

* Initialize counter := 0, current := NULL. 
  [current stores the frontrunner for the majority element]

* For $$i=1$$ to $$n$$:

     * If counter == 0:
     
       [In this case, there is no frontrunner.]

          * current := A[i]

          * counter++

     * else if A[i]==current:

       [In this case, our confidence in the current frontrunner goes up.]

          * counter++

     * else

       [In this case, our confidence in the current frontrunner goes down.]

          * counter- -

* Return current

For example, suppose the input is the array $$\{2,1,1\}$$.  The first
iteration of the algorithm makes "2" the current guess of the majority
element, and sets the counter to 1.  The next element decreases the
counter back to 0 (since $$1 \neq 2$$).  The final iteration resets the
current guess to "1" (with counter value 1), which is indeed the
majority element.

More generally, the algorithm correctly computes the majority element
of any array that possesses one.  We encourage you to formalize a
proof of this statement (e.g., by induction on $$n$$).  The intuition is
that each entry of $$A$$ that
contains a non-majority-value can only "cancel out" one copy of the
majority value.  Since more than $$n/2$$ of the entries of $$A$$
contain the majority value, there is guaranteed to be a copy of it
left standing at the end of the algorithm.

But so what?  It's a cute algorithm, but isn't this just a toy
problem?  It is, but modest generalizations of the problem are quite
close to problems that people really want to solve in practice.

### The Heavy Hitters Problem {#sec1-2}

In the *heavy hitters* problem, the input is an array $$A$$ of
length $$n$$, and also a parameter $$k$$.  You should think of $$n$$ as very
large (in the hundreds of millions, or billions), and $$k$$ as modest
(10, 100, or 1000). The goal is to compute the values that occur in the array at
least $$n/k$$ times.[^1] Note that there can be at most $$k$$ such values; and there might be
none. The problem of computing the majority element corresponds to the heavy hitters
problem with $$k \approx 2-\delta$$ for a small value $$\delta > 0$$, and
with the additional promise that a majority element exists.

[^1]: A similar problem is the "top-$$k$$ problem," where the goal is to output the $$k$$ values that occur with the highest frequencies.  The algorithmic ideas introduced in this lecture are also relevant for the top-$$k$$ problem.

The heavy hitters problem has lots of applications, as you can
imagine.  We'll be more specific later when we discuss a concrete
solution, but here are some high-level examples: [^2]

[^2]: You wouldn't expect there to be a majority element in any of these applications, but you might expect a non-empty set of heavy hitters when $$k$$ is 100, 1000, or 10000.

1. Computing popular products.  For example, $$A$$ could be all of
  the page views of products on [amazon](http://www.amazon.com) yesterday. The heavy
  hitters are then the most frequently viewed products.

2. Computing frequent search queries.  For example, $$A$$ could be
  all of the searches on Google yesterday.  The heavy
  hitters are then searches made most often.

3. Identifying heavy TCP flows.  Here, $$A$$ is a list of data packets passing through a network switch, each annotated with a source-destination pair of IP addresses.  The heavy hitters are then the flows that are sending the most traffic.  This is useful for, among other things, identifying denial-of-service attacks.

4. Identifying volatile stocks.  Here, $$A$$ is a list of stock
  trades.

It's easy to think of more.  Clearly, it would be nice to have a
good algorithm for the heavy hitters problem at your disposal for data
analysis.

The problem is easy to solve efficiently if $$A$$ is readily available
in main memory --- just sort the array and do a linear scan over the result,
outputting a value if and only if it occurs (consecutively) at least
$$n/k$$ times.
After being spoiled by our slick solution for finding a majority element, we
naturally want to do better.  Can we solve the heavy hitters problem
with a single pass over the array?
This question isn't posed quite
correctly, since it allows us to cheat: we could make a single pass
over the array, make a local copy of it in our working memory, and
then apply the sorting-based solution to our local copy.  Thus what we
mean is: can we solve the Heavy Hitters problem with a single pass
over the array, using only a small amount of auxiliary
space?[^3]

[^3]: Rather than thinking of the array $$A$$ as an input fully specified in advance, we can alternatively think of the elements of $$A$$ as a "data stream," which are fed to a "streaming algorithm" one element at a time. One-pass algorithms that use small auxiliary space translate to streaming algorithms that need only small working memory. One use case for streaming algorithms is when data arrives at such a fast rate that explicitly storing it is absurd. For example, this  is often the reality in the motivating example of data packets traveling through a network switch. A second use case is when, even though data can be stored in its entirety and fully analyzed (perhaps as an overnight job), it's still useful to perform lightweight analysis on the arriving data in real time. The first two applications (popular transactions or search queries) are examples of this.

###  An Impossibility Result

The following fact might surprise you.

**Fact 1.1** *There is* no *algorithm that solves the Heavy Hitters problems in one pass
while using a sublinear amount of auxiliary space.*
{: #fact1}

We next explain the intuition behind [Fact 1.1](#fact1).  We encourage
you to devise a formal proof, which follows the same lines as the
intuition.

Set $$k = n/2$$, so that our responsibility is to output any values that
occur at least twice in the input array $$A$$.[^4]

Suppose $$A$$ has the form

$$ \underbrace{|x_1|x_2|x_3|\cdots|x_{n-1}|}_{\text{set } S \text{ of distinct elements}}y|, $$

[^4]:A simple modification of this argument extends the impossibility result to all interesting values of $$k$$ --- can you figure it out?

where $$x_1,\ldots,x_{n-1} $$ are an arbitrary set $$S$$ of distinct
elements (in $$\{1,2,\ldots,n^2\}$$, say) and the
final entry $$y$$ may or may not be in $$S$$.  By definition,
we need to output $$y$$ if and only if $$y \in S$$.
That is, *answering membership queries reduces
to solving the Heavy Hitters problem.*

By the "membership problem", we mean the task of preprocessing a set
$$S$$ to answer queries of the form "is $$y \in S$$"? (A hash table is the most common solution to this problem.)
It is intuitive that you cannot correctly answer all membership queries for a set $$S$$
without storing $$S$$ (thereby using linear, rather than constant, space)
--- if you throw some of $$S$$ out, you might get a query asking about
the part you threw out, and you won't know the answer.  It's not too
hard to make this idea precise using the Pigeonhole Principle.[^5]


[^5]: Somewhat more detail: if you always use sublinear space to store the set $$S$$, then you need to reuse exactly the same memory contents for two different sets $$S_1$$ and $$S_2$$. Your membership query answers will be the same in both cases, and in one of these cases some of your answers will be wrong.

###  The Approximate Heavy Hitters Problem

What should we make of [Fact 1.1](#fact1)?  Should we go home with our
tail between our legs?  Of course not --- the applications that
motivate the heavy hitters problem are not going away, and we
still want to come up with non-trivial algorithms for them.
In light of [Fact 1.1](#fact1), the best-case scenario would be to
find a relaxation of the problem that remains relevant for the
motivating applications and also admits a good
solution.[^6]

[^6]: This impossibility result ([Fact 1.1](#fact1)) and our response to it (the $$\epsilon$$-HH problem) serve as reminders that the skilled  algorithm designer is respectful of but undaunted by impossibility results that limit what algorithms can do. For another example, recall your study in CS161 of methods for coping with $$NP$$-complete problems.

In the *$$\epsilon$$-approximate heavy hitters ($$\epsilon$$-HH) problem}*,
the input is an array $$A$$ of length $$n$$ and user-defined parameters $$k$$ and $$\epsilon$$. The responsibility of an algorithm is to output a list of values such that:
\begin{enumerate}

1. Every value that occurs at least $$\frac{n}{k}$$ times in
   $$A$$ is in the list.

2. Every value in the list occurs at least $$\frac{n}{k} -\epsilon n$$
   times in $$A$$.


What prevents us from taking $$\epsilon = 0$$ and solving the exact version
of the problem? We allow the space used by a solution to grow
as $$\tfrac{1}{\epsilon}$$, so as $$\epsilon \downarrow 0$$ the space blows
up (as is necessary, by [Fact 1.1](#fact1)).

For example, suppose we take $$\epsilon = \tfrac{1}{2k}$$.  Then, the
algorithm outputs every value with frequency count at least $$\tfrac{n}{k}$$,
and only values with frequency count at least $$\tfrac{n}{2k}$$.  Thinking
back to the motivating examples in [Section 1.2](#sec1-2) , such an
approximate solution is essentially as useful as an exact solution.
Space usage $$O(\tfrac{1}{\epsilon}) = O(k)$$ is also totally palatable;
after all, the output of the heavy hitters or $$\epsilon$$-HH problem
already might be as large as $$k$$ elements.

## The Count-Min Sketch

### Discussion

This section presents an elegant small-space data structure, the *count-min sketch* [\[5\]](#ref5), that can
be used to solve the $$\epsilon$$-HH problem.  There are also several other
good solutions to the problem, including some natural
"counter-based" algorithms that extend the algorithm
in [Section 1.1](#sec1-1) for computing a
majority element [\[6\]](#ref6)[\[7\]](#ref7).
We focus on the count-min sketch
for a number of reasons.


1. It has been implemented in real systems.  For example, AT{\&}T has
  used it in network switches to perform analyses on network traffic using
  limited memory[\[4\]](#ref4). [^7] At Google, a precursor of the count-min sketch (called the "count
  sketch" [\[3\]](#ref3)) has been implemented on top of their MapReduce
  parallel processing infrastructure [\[8\]](#ref8).  One of the original
 motivations for this primitive was log analysis (e.g., of source code
 check-ins), but presumably it is now used for lots of different analyses.
[^7]:There is a long tradition in the Internet of designing routers that are "fast and dumb," and many of them have far less memory than a 
typical smartphone.

2. The data structure is based on hashing, and as such fits in well
  with the current course theme.

3. The data structure introduces a new theme, present in many of
  the next few lectures, of "lossy compression."  The goal here is
  to throw out as much of your data as possible while still being able
  to make accurate inferences about it.  What you want to keep depends
  on the type of inference you want to support.  For approximately
  preserving frequency counts, the count-min sketch shows that you can
  throw out almost all of your data!


We'll only discuss how to use the count-min sketch to solve the
approximate heavy hitters problem, but it is also useful for other
related tasks (see [\[5\]](#ref5) for a start).
Another reason for its current popularity is that its computations
parallelize easily --- as we discuss its implementation, you might
want to think about this point.





### References
B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications
of the ACM, 13(7):422–426, 1970.
{: #ref1}

A. Broder and M. Mitzenmacher. Network applications of. bloom filters: A survey.
Internet Mathematics, 1(4):485–509, 2005.
{: #ref2}

M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent items in data streams.
Theoretical Computer Science, 312(1):3–15, 2004.
{: #ref3}

G. Cormode, T. Johnson, F. Korn, S. Muthukrishnan, O. Spatscheck, and D. Srivastava.
Holistic UDAFs at streaming speeds. In Proceedings of the 2004 ACM SIGMOD
International Conference on Management of Data, pages 35–46, 2004.
{: #ref4}

G. Cormode and S. Muthukrishnan. An improved data stream summary: The count-min
sketch and its applications. Journal of Algorithms, 55(1):58–75, 2005.
{: #ref5}

A. Metwally, D. Agrawal, and A. El Abbadi. Efficient computation of frequent and
top-k elements in data streams. In Proceedings of the 10th International Conference on
Database Theory (ICDT), pages 398–412, 2005.
{: #ref6}

J. Misra and D. Gries. Finding repeated elements. Science of Computer Programming,
2:143–152, 1982.
{: #ref7}

R. Pike, S. Dorward an R. Griesemer, and S. Quinlan. Interpreting the data: Parallel
analysis with Sawzall. Dynamic Grids and Worldwide Computing, 13(4):277–298, 2005.
{: #ref8}


