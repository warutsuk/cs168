---
layout: post
title: Lecture 4
author: Tim
---
{% raw %}

<h1 id="the-curse-of-dimensionality-in-the-nearest-neighbor-problem">The Curse of Dimensionality in the Nearest Neighbor Problem</h1>
<p>Lectures #1 and #2 discussed “unstructured data”, where the only information we used about two objects was whether or not they were equal. Last lecture, we started talking about “structured data”. For now, we consider structure expressed as a (dis)similarity measure between pairs of objects. There are many such measures; last lecture we mentioned Jaccard similarity (for sets), <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> distance (for points in <span class="math inline">\(\R^k\)</span>, when coordinates do or do not have meaning, respectively), edit distance (for strings), etc.</p>
<p>How can such structure be leveraged to understand the data? We’re currently focusing on the canonical <span><em>nearest neighbor</em></span> problem, where the goal is to find the closest point of a point set to a given point (either another point of the point set or a user-supplied query). Last lecture also covered a solution to the problem, the <span class="math inline">\(k\)</span>-d tree. This is a good “first cut” solution to the problem when the number of dimensions is not too big — less than logarithmic in the size of the point set.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> When the number <span class="math inline">\(k\)</span> of dimensions is at most 20 or 25, a <span class="math inline">\(k\)</span>-d tree is likely work well. Why do we want the number of dimensions to be small? Because of the <span><em>curse of dimensionality</em></span>. Recall that to compute the nearest neighbor of a query <span class="math inline">\(q\)</span> using a <span class="math inline">\(k\)</span>-d tree, one first does a downward traversal through the tree to identify the smallest region of space that contains <span class="math inline">\(q\)</span>. (Nodes of the <span class="math inline">\(k\)</span>-d tree correspond to regions of space, with the regions corresponding to the children <span class="math inline">\(y,z\)</span> of a node <span class="math inline">\(x\)</span>.) Then one does an upward traversal of the tree, checking other cells that could conceivably contain <span class="math inline">\(q\)</span>’s nearest neighbor. The number of cells that have to be checked can scale exponentially with the dimension <span class="math inline">\(k\)</span>.</p>
<p>The curse of dimensionality appears to be fundamental to the nearest neighbor problem (and many other geometric problems), and is not an artifact of the specific solution of the <span class="math inline">\(k\)</span>-d tree. For further intuition, consider the nearest neighbor problem in one dimension, where the point set <span class="math inline">\(P\)</span> and query <span class="math inline">\(q\)</span> are simply points on the line. The natural way to preprocess <span class="math inline">\(P\)</span> is to sort the points by value; searching for the nearest neighbor is just binary search to find the interval in which <span class="math inline">\(q\)</span> lies, and then computing <span class="math inline">\(q\)</span>’s distance to the points immediately to the left and right of <span class="math inline">\(q\)</span>. What about two dimensions? It’s no longer clear what “sorting the point set” means, but let’s assume that we figure that out. (The <span class="math inline">\(k\)</span>-d tree offers one approach.) Intuitively, there are now <span><em>four</em></span> directions that we have to check for a possible nearest neighbor. In three dimensions, there are eight directions to check, and in general the number of relevant directions is scaling exponentially with <span class="math inline">\(k\)</span>. There is no known way to overcome the curse of dimensionality in the nearest neighbor problem, without resorting to approximation (as we do below): every known solution that uses a reasonable amount of space uses time that scales exponentially in <span class="math inline">\(k\)</span> or linearly in the number <span class="math inline">\(n\)</span> of points.</p>
<h1 id="point-of-lecture">Point of Lecture</h1>
<p>Why is the curse of dimensionality a problem? The issue is that the natural representation of data is often high-dimensional. Recall our motivating examples for representing data points in real space. With documents and the bag-of-words model, the number of coordinates equals the number of words in the dictionary — often in the tens of thousands. Images are often represented as real vectors, with at least one dimension per pixel (recording pixel intensities) — here again, the number of dimensions is typically in the tens of thousands. The same holds for points representing the purchase or page view history of an Amazon customer, or of the movies watched by a Netflix subscriber.</p>
<p>The friction between the large number of dimensions we want to use to represent data and the small number of dimensions required for computational tractability motivates <span><em>dimensionality reduction</em></span>. The goal is to re-represent points in high-dimensional space as points in low-dimensional space, preserving interpoint distances as much as possible. We can think of dimensionality reduction as a form of lossy compression, tailored to approximately preserve distances. For an analogy, the count-min sketch (Lecture #2) is a form a lossy compression tailored to the approximate preservation of frequency counts.</p>
<p>Dimensionality reduction enables the following high-level approach to the nearest neighbor problem:</p>
<ol>
<li><p>Represent the data and queries using a large number <span class="math inline">\(k\)</span> of dimensions (tens of thousands, say).</p></li>
<li><p>Use dimensionality reduction to map the data and queries down to a small number <span class="math inline">\(d\)</span> of dimensions (in the hundreds, say).</p></li>
<li><p>Compute answers to nearest-neighbor queries in the low-dimensional space.</p></li>
</ol>
<p>Provided the dimensionality reduction subroutine approximately preserves all interpoint distances, the answer to the nearest-neighbor query in low dimensions is an approximately correct answer to the original high-dimensional query. Even if the reduced number <span class="math inline">\(d\)</span> of dimensions is still too big to use <span class="math inline">\(k\)</span>-d trees, reducing the dimension still speeds up algorithms significantly. For example, on Mini-Project #2, you will use dimensionality reduction to improve the running time of a brute-force search algorithm, where the running time has linear dependence on the dimension.</p>
<p>The three-step paradigm above is relevant for any computation that only cares about interpoint distances between the points, not just the nearest-neighbor problem. Distance-based clustering is another example.</p>
<p>It should now be clear that we would love to have subroutines for dimensionality reduction in our algorithmic toolbox. The rest of this lecture gives a few examples of such subroutines, and a unified way to think about them.</p>
<h1 id="s:finger">Role Model: Fingerprints</h1>
<p>The best way to understand the new concepts in this lecture is via analogy to a hashing-based trick that you already know cold. We review the key idea here; our other examples of dimensionality reduction follow the same template.</p>
<p>Let’s return to a world of unstructured abstract data, with no notion of distance between objects other than “equal” vs. “non-equal.” The analog of dimensionality reduction is then: how can we re-represent all objects, using fewer bits than before, so that the “distinctness” relation is approximately preserved? This goal is, of course, right in the wheelhouse of hashing.</p>
<p>To maximize overlap with our later dimensionality reduction subroutines, we solve the problem in two steps. Suppose we have <span class="math inline">\(n\)</span> objects from a universe <span class="math inline">\(U\)</span>. Each object requires <span class="math inline">\(\log_2 U\)</span> bits to describe. In the first step, we choose a function <span class="math inline">\(h\)</span> — for example, uniformly at random from a universal family, with range equal to all 32-bit values — and map each object <span class="math inline">\(x\)</span> to <span class="math display">\[\label{eq:finger}
f(x) = h(x) \bmod 2.\]</span> This associates each object with a single bit, 0 or 1 — certainly this is much compressed compared to the original <span class="math inline">\(\log_2 U\)</span>-bit representation! The properties of this mapping are:</p>
<ol>
<li><p>If <span class="math inline">\(x=y\)</span>, then <span class="math inline">\(f(x)=f(y)\)</span>. That is, the property of equality is preserved.</p></li>
<li><p>If <span class="math inline">\(x \neq y\)</span> and <span class="math inline">\(h\)</span> is a good hash function, then <span class="math inline">\({\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[f(x)
  = f(y)\right]} \le \half\)</span>. That is, the property of distinctness is preserved with probability at least 50%.</p></li>
</ol>
<p>Achieving error 50% doesn’t sound too impressive, but it’s easy to reduce it in a second step, via the “magic of independent trials.” Repeating the experiment above <span class="math inline">\(\ell\)</span> times — choosing <span class="math inline">\(\ell\)</span> different hash functions <span class="math inline">\(h_1,\ldots,h_{\ell}\)</span> and labeling each object <span class="math inline">\(x\)</span> with <span class="math inline">\(\ell\)</span> bits <span class="math inline">\(f_1(x),\ldots,f_{\ell}(x)\)</span> — the properties become:</p>
<ol>
<li><p>If <span class="math inline">\(x=y\)</span>, then <span class="math inline">\(f_i(x)=f_i(y)\)</span> for all <span class="math inline">\(i=1,2,\ldots,\ell\)</span>.</p></li>
<li><p>If <span class="math inline">\(x \neq y\)</span> and the <span class="math inline">\(h_i\)</span>’s are good and independent hash functions, then <span class="math inline">\({\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[f(x) 
  = f(y)\right]} \le 2^{-k}\)</span>.</p></li>
</ol>
<p>For example, to achieve a user-specified error of <span class="math inline">\(\delta &gt; 0\)</span>, we only need to use <span class="math inline">\(\lceil \log_2 \tfrac{1}{\delta} \rceil\)</span> bits to represent each object. For all but the tiniest values of <span class="math inline">\(\delta\)</span>, this representation is much smaller than the original <span class="math inline">\(\log_2 U\)</span>-bit one.</p>
<h1 id="s:l2"><span class="math inline">\(L_2\)</span> Distance and Random Projections</h1>
<p>The “fingerprinting” subroutine of Section [s:finger] approximately preserves a 0-1 function on object pairs (“not equal vs. equal”). What if we want to preserve approximately the distances between object pairs? For example, if we want to preserve the <span class="math inline">\(L_2\)</span> (a.k.a. Euclidean) distance <span class="math display">\[\sqrt{\sum_{i=1}^k (x_i-y_i)^2}\]</span> between points in <span class="math inline">\(\R^k\)</span>, what’s the analog of a hash function? This section proposes <span><em>random projection</em></span> as the answer. This idea results in a very neat primitive, the <span> <em>Johnson-Lindenstrauss (JL) transform</em></span>, which says that if all we care about are the Euclidean distances between points, then we can assume (conceptually and computationally) that the number of dimensions is not overly huge (in the hundreds, at most).</p>
<h2 id="the-high-level-idea">The High-Level Idea</h2>
<p>Assume that the <span class="math inline">\(n\)</span> objects of interest are points <span class="math inline">\(\x_1,\ldots,\x_n\)</span> in <span class="math inline">\(k\)</span>-dimensional Euclidean space <span class="math inline">\(\R^k\)</span> (where <span class="math inline">\(k\)</span> can be very large). Suppose we choose a “random vector” <span class="math inline">\(\r = (r_1,\ldots,r_k) \in
\R^k\)</span>. (See Section [ss:step1] for details on how the <span class="math inline">\(r_i\)</span>’s are chosen.) Define a corresponding real-valued function <span class="math inline">\(f_{\r}:\R^k \mapsto \R\)</span> by taking the inner product of its argument with the randomly chosen coefficients <span class="math inline">\(\r\)</span>: <span class="math display">\[\label{eq:fr}
f_{\r}(\x) = {
{\langle {\x} , {\r} \rangle}
} = \sum_{j=1}^k x_jr_j.\]</span> Thus, <span class="math inline">\(f_{\r}(\x)\)</span> is a random linear combination of the components of <span class="math inline">\(\x\)</span>. This function will play a role analogous to the single-bit function defined in  in the previous section. The function in  compresses a <span class="math inline">\(\log_2 U\)</span>-bit object description to a single bit; the random projection in  replaces a vector of <span class="math inline">\(k\)</span> real numbers with a single real number.</p>
<p>Figure [f:ip] recalls the geometry of the inner product, as the projection of one vector onto the line spanned by another, which should be familiar from your high school training.</p>
<div class="figure">
<img src="fig1" alt="The inner product of two vectors is the projection of one onto the line spanned by the other." />
<p class="caption">The inner product of two vectors is the projection of one onto the line spanned by the other.<span data-label="f:ip"></span></p>
</div>
<p>If we want to use this idea to approximately preserve the Euclidean distances between points, how should we pick the <span class="math inline">\(r_j\)</span>’s? Inspired by our two-step approach in Section [s:finger], we first try to preserve distances only in a weak sense. We then use independent trials to reduce the error.</p>
<h2 id="ss:gaussian">Review: Gaussian Distributions</h2>
<div class="figure">
<img src="fig2" alt="The probability density function for the standard Gaussian distribution (with mean 0 and variance 1)." />
<p class="caption">The probability density function for the standard Gaussian distribution (with mean 0 and variance 1).<span data-label="f:gaussian"></span></p>
</div>
<p>We briefly recall some properties of Gaussian (a.k.a. normal) distributions that are useful for us. Recall the basic shape of the distribution (Figure [f:gaussian]). The distribution is symmetric around its mean <span class="math inline">\(\mu\)</span>. Roughly 68% of its mass is assigned to points that are within one standard deviation <span class="math inline">\(\sigma\)</span> of its mean. Recall that, for any distribution, the square <span class="math inline">\(\sigma^2\)</span> of the standard deviation is the variance. A Gaussian distribution is completely and uniquely defined by the values of its mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The <span><em>standard</em></span> Gaussian is the special case where <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma = \sigma^2 = 1\)</span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>We’re designing our own dimensionality reduction subroutine, and can choose the random coefficients <span class="math inline">\(r_i\)</span> however we want. Why use Gaussians? The nice property we’ll exploit here is their closure under addition. Formally, suppose <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variables with normal distributions <span class="math inline">\(N(\mu_1,\sigma_1^2)\)</span> and <span class="math inline">\(N(\mu_2,\sigma_2^2)\)</span>, respectively. Then, the random variable <span class="math inline">\(X_1 + X_2\)</span> has the normal distribution <span class="math inline">\(N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)\)</span>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>Note that you shouldn’t be impressed that the mean of <span class="math inline">\(X_1 + X_2\)</span> equals the sum of the means of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> — by linearity of expectation, this is true for <span><em>any</em></span> pair of random variables, even non-independent ones. Similarly, it’s not interesting that the variance of <span class="math inline">\(X_1+X_2\)</span> is the sum of the variances of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> — this holds for any pair of independent random variables.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> What’s remarkable is that the distribution of <span class="math inline">\(X_1+X_2\)</span> is a Gaussian (with the only mean and variance that it could possibly have). Adding two distributions from a family generally gives a distribution outside that family. For example, the sum of two random variables that are uniform on <span class="math inline">\([0,1]\)</span> certainly isn’t uniformly distributed on <span class="math inline">\([0,2]\)</span> — there’s more mass in the middle then on the ends.</p>
<h2 id="ss:step1">Step 1: Unbiased Estimator of Squared <span class="math inline">\(L_2\)</span> Distance</h2>
<p>We now return to the random projection function <span class="math inline">\(f_{\r}\)</span> defined in , with the random coefficients <span class="math inline">\(r_1,\ldots,r_k\)</span> chosen independently from a standard Gaussian distribution. We next derive the remarkable fact that, for every pair <span class="math inline">\(\x,\y\)</span> of points in <span class="math inline">\(\R^k\)</span>, the square of <span class="math inline">\(f_{\r}(\x) - f_{\r}(\y)\)</span> is an unbiased estimator of the squared Euclidean distance between <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>.</p>
<p>Fix <span class="math inline">\(\x,\y \in \R^k\)</span>. The <span class="math inline">\(L_2\)</span> distance between <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>, denoted <span class="math inline">\({
{\| {\x-\y} \|}
}_2\)</span>, is <span class="math inline">\(\sqrt{\sum_{j=1}^k (x_j-y_j)^2}\)</span>. By the definition of <span class="math inline">\(f_{\r}\)</span>, we have <span class="math display">\[\label{eq:euclid1}
f_{\r}(\x) - f_{\r}(\y)
= \sum_{j=1}^k x_jr_j - \sum_{j=1}^k y_jr_j
= \sum_{j=1}^k (x_j-y_j)r_j.\]</span> Here’s where the nice properties of Gaussians come in. Recall that the <span class="math inline">\(x_j\)</span>’s and <span class="math inline">\(y_j\)</span>’s are fixed (i.e., constants), while the <span class="math inline">\(r_j\)</span>’s are random. For each <span class="math inline">\(j=1,2,\ldots,k\)</span>, since <span class="math inline">\(r_j\)</span> is a Gaussian with mean zero and variance 1, <span class="math inline">\((x_j-y_j)r_j\)</span> is a Gaussian with mean zero and variance <span class="math inline">\((x_j-y_j)^2\)</span>. (Multiplying a random variable by a scalar <span class="math inline">\(\lambda\)</span> scales the standard deviation by <span class="math inline">\(\lambda\)</span> and hence the variance by <span class="math inline">\(\lambda^2\)</span>.) Since Gaussians add, the right-hand side of  is a Gaussian with mean 0 and variance <span class="math display">\[\sum_{j=1}^k (x_j-y_j)^2 = {
{\| {\x-\y} \|}
}^2_2.\]</span> Whoa — this is an unexpected connection between the output of random projection and the (square of the) quantity that we want to preserve. How can we exploit it? Recalling the definition <span class="math inline">\(\Var(X) = {\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[(X
  - {\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X\right]})^2\right]}\)</span> of variance as the expected squared deviation of a random variable from its mean, we see that for a random variable <span class="math inline">\(X\)</span> with mean 0, <span class="math inline">\(\Var(X)\)</span> is simply <span class="math inline">\({\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X^2\right]}\)</span>. Taking <span class="math inline">\(X\)</span> to be the random variable in , we have <span class="math display">\[\label{eq:euclid2}
{\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[(f_{\r}(\x) - f_{\r}(\y))^2\right]} = {
{\| {\x-\y} \|}
}^2_2.\]</span> That is, the random variable <span class="math inline">\((f_{\r}(\x) - f_{\r}(\y))^2\)</span> is an unbiased estimator of the squared Euclidean distance between <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>.</p>
<h2 id="ss:step2">Step 2: The Magic of Independent Trials</h2>
<p>We’ve showed that random projection reduces the number of dimensions from <span class="math inline">\(k\)</span> to just one (replacing each <span class="math inline">\(\x\)</span> by <span class="math inline">\(f_{\r}(\x)\)</span>), while preserving squared distances in expectation. Two issues are: we care about preserving distances, not squares of distances;<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> and we want to almost always preserve distances very closely (not just in expectation). We’ll solve both these problems in one fell swoop, via the magic of independent trials.</p>
<p>Suppose instead of picking a single vector <span class="math inline">\(\r\)</span>, we pick <span class="math inline">\(d\)</span> vectors <span class="math inline">\(\r_1,\ldots,\r_d\)</span>. Each component of each vector is drawn i.i.d. from a standard Gaussian. For a given pair <span class="math inline">\(\x,\y\)</span> of points, we get <span class="math inline">\(d\)</span> independent unbiased estimates of <span class="math inline">\({
{\| {\x-\y} \|}
}^2_2\)</span> (via ). Averaging independent unbiased estimates yields an unbiased estimate with less error.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> Because our estimates in  are (squares of) Gaussians, which are very well-understood distributions, one can figure out exactly how large <span class="math inline">\(d\)</span> needs to be to achieve a target approximation (for details, see <span class="citation"></span>). The bottom line is: for a set of <span class="math inline">\(n\)</span> points in <span class="math inline">\(k\)</span> dimensions, to preserve all <span class="math inline">\(\binom{n}{2}\)</span> interpoint Euclidean distances up to a <span class="math inline">\(1
\pm \eps\)</span> factor, one should set <span class="math inline">\(d = \Theta(\eps^{-2} \log
n)\)</span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<h2 id="the-johnson-lindenstrauss-transform">The Johnson-Lindenstrauss Transform</h2>
<p>Rephrasing and repackaging what we’ve already done yields the <span> <em>Johnson-Lindenstrauss (JL) transform</em></span>. This was originally a result in pure functional analysis <span class="citation"></span>, and was ported over to the algorithmic toolbox in the mid-90s <span class="citation"></span>. The JL transform is an influential result, and in the 21st century, many variations and improvements have been proposed (see the end of the section).</p>
<div class="figure">
<img src="fig3" alt="The Johnson-Lindenstrauss tranform A for dimension reduction." />
<p class="caption">The Johnson-Lindenstrauss tranform <span class="math inline">\(A\)</span> for dimension reduction.<span data-label="f:jl"></span></p>
</div>
<p>The JL transform, for domain and range dimensions <span class="math inline">\(k\)</span> and <span class="math inline">\(d\)</span>, is defined using a <span class="math inline">\(d \times k\)</span> matrix <span class="math inline">\(\A\)</span> in which each of the <span class="math inline">\(kd\)</span> entries is chosen i.i.d. from a standard Gaussian distribution. See Figure [f:jl]. This matrix defines a mapping from <span class="math inline">\(k\)</span>-vectors to <span class="math inline">\(d\)</span>-vectors via <span class="math display">\[\x \mapsto \tfrac{1}{\sqrt{d}}\Ax,\]</span> where the <span class="math inline">\(1/\sqrt{d}\)</span> scaling factor corresponds to the average over independent trials discussed in Section [ss:step2].</p>
<p>To see how this mapping <span class="math inline">\(f_{\A}\)</span> corresponds to our derivation in Section [ss:step2], note that for a fixed pair <span class="math inline">\(\x,\y\)</span> of <span class="math inline">\(k\)</span>-vectors, we have <span class="math display">\[\begin{aligned}
\label{eq:jl1}
\left\| f_{\A}(\x) - f_{\A}(\y) \right\|^2_2
&amp; = &amp; 
\left\| \frac{1}{\sqrt{d}} \Ax - \frac{1}{\sqrt{d}} \Ay \right\|^2_2\\
\label{eq:jl2}
&amp; = &amp; 
\frac{1}{d} \left\| \A(\x-\y) \right\|^2_2,\\
\label{eq:jl3}
&amp; = &amp; 
\frac{1}{d} \sum_{i=1}^d \left( a_i^T(\x-\y) \right)^2,\end{aligned}\]</span> with <span class="math inline">\(a_i^T\)</span> denotes the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\A\)</span>. Since each row <span class="math inline">\(a_i^T\)</span> is just a <span class="math inline">\(k\)</span>-vector with entries chosen i.i.d. from a standard Gaussian, each term <span class="math display">\[\left( a_i^T(\x-\y) \right)^2 = \left( \sum_{j=1}^k a_{ij}(x_j-y_j)
\right)^2\]</span> is precisely the unbiased estimator of <span class="math inline">\({
{\| {\x-\y} \|}
}^2_2\)</span> described in  and . Thus – is the average of <span class="math inline">\(d\)</span> unbiased estimators. Provided <span class="math inline">\(d\)</span> is sufficiently large, with probability close to 1, all of the low-dimensional interpoint squared distances <span class="math inline">\({
{\| {f_{\A}(\x) - f_{\A}(\y)} \|}
}^2_2\)</span> are very good approximations of the original squared distances <span class="math inline">\({
{\| {\x-\y} \|}
}_2^2\)</span>. This implies that, with equally large probability, all interpoint Euclidean distances are approximately preserved by the mapping <span class="math inline">\(f_{\A}\)</span> down to <span class="math inline">\(d\)</span> dimensions.</p>
<p>Thus, for any point set <span class="math inline">\(\x_1,\ldots,\x_n\)</span> in <span class="math inline">\(k\)</span>-dimensional space, and any computation that cares only about interpoint Euclidean distances, there is little loss in doing the computation on the <span class="math inline">\(d\)</span>-dimensional <span class="math inline">\(f_{\A}(\x_i)\)</span>’s rather than on the <span class="math inline">\(k\)</span>-dimension <span class="math inline">\(\x_i\)</span>’s.</p>
<p>The JL transform is not usually implemented exactly the way we described it. One simplification is to use <span class="math inline">\(\pm 1\)</span> entries rather than Gaussian entries; this idea has been justified both empirically and theoretically (see <span class="citation"></span>). Another line of improvement is add structure to the matrix so that the matrix-vector product <span class="math inline">\(\Ax\)</span> can be computed particularly quickly a la the Fast Fourier Transform — this is known as the “fast JL transform” <span class="citation"></span>. Finally, since the JL transform can often only bring the dimension down into the hundreds without overly distorting interpoint distances, additional tricks are often needed. One of these is “locality sensitive hashing (LSH),” touched on briefly in Section [s:lsh].</p>
<h1 id="jaccard-similarity-and-minhash">Jaccard Similarity and MinHash</h1>
<p>Section [s:l2] makes the case for using random projections for preserving Euclidean distances. What about other notions of similarity? Several analogs of random projection for other measures are known; we next discuss a representative one.</p>
<h2 id="ss:alta">The High-Level Idea</h2>
<p>A long time ago (mid/late 90s, post-Web but pre-Google) in a galaxy far, far away, there was a Web search engine called Alta Vista. When designing a search engine, an immediate problem is to filter search results to remove near-duplicate documents — for example, two different Web pages that differ only in their timestamps. To turn this into a well-defined problem, one needs a similarity measure between documents. Alta Vista decided to use <span><em>Jaccard similarity</em></span>. Recall from last lecture that this is a similarity measure for sets: for two sets <span class="math inline">\(A,B \sse U\)</span>, the Jaccard similarity is defined as <span class="math display">\[J(A,B) = \frac{|A \cap B|}{|A \cup B|}.\]</span> Jaccard similarity is easily defined for multi-sets (see last lecture); here, to keep things simple, we do not allow an element to appear in a set more than once.</p>
<p>Random projection replaces <span class="math inline">\(k\)</span> real numbers with a single one. So an analog here would replace a set of elements with a single element. The plan is implement a random such mapping that preserves Jaccard similarity in expectation, and then to use independent trials as in Section [ss:step2] to boost the accuracy.</p>
<h2 id="minhash">MinHash</h2>
<p>For sets, the analog of random projection is the <span><em>MinHash</em></span> subroutine:</p>
<ol>
<li><p>Choose a permutation <span class="math inline">\(\pi\)</span> of the universe <span class="math inline">\(U\)</span> uniformly at random.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p></li>
<li><p>Map each set <span class="math inline">\(S\)</span> to its minimum element <span class="math inline">\({\operatornamewithlimits{argmin}}_{x \in S} \pi
  (x)\)</span> under <span class="math inline">\(\pi\)</span>.</p></li>
</ol>
<p>Thus MinHash “projects” a set to its minimum element under a random permutation.</p>
<div class="figure">
<img src="fig4" alt="Two set A,B\sse U." />
<p class="caption">Two set <span class="math inline">\(A,B\sse U\)</span>.<span data-label="f:jaccard"></span></p>
</div>
<p>The brilliance of MinHash is that it gives a remarkably simple unbiased estimator of Jaccard similarity. To see this, consider an arbitrary pair of sets <span class="math inline">\(A,B \sse U\)</span> (Figure [f:jaccard]). First, if the smallest (under <span class="math inline">\(\pi\)</span>) element <span class="math inline">\(z\)</span> of <span class="math inline">\(A \cup B\)</span> lies in the intersection <span class="math inline">\(A \cap B\)</span>, then <span class="math inline">\({\operatornamewithlimits{argmin}}_{x \in A} \pi(x) =
{\operatornamewithlimits{argmin}}_{x \in B} \pi(x) = z\)</span>, so <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have the same MinHash. Second, if the smallest element <span class="math inline">\(z\)</span> of <span class="math inline">\(A \cup B\)</span> does not lie in <span class="math inline">\(A \cap B\)</span> — say it’s in <span class="math inline">\(A\)</span> but not <span class="math inline">\(B\)</span> — then the MinHash of <span class="math inline">\(A\)</span> is <span class="math inline">\(z\)</span> while the MinHash of <span class="math inline">\(B\)</span> is some element strictly larger than <span class="math inline">\(z\)</span> (under <span class="math inline">\(\pi\)</span>). Thus, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have the same MinHash exactly when the smallest element of <span class="math inline">\(A \cup B\)</span> lies in <span class="math inline">\(A \cap B\)</span>. Since all relative orderings of <span class="math inline">\(A \cup B\)</span> are equally likely under a random permutation, each element of <span class="math inline">\(A \cup B\)</span> is equally likely to be the smallest. Thus: <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\text{MinHash($A$)} = \text{MinHash($B$)}\right]} = \frac{|A \cap
  B|}{|A \cup B|} = J(A,B).\]</span></p>
<p>As usual, we can boost accuracy through the magic of independent trials. If we want an accurate estimate (up to <span class="math inline">\(\pm \eps\)</span>) of all <span class="math inline">\(\binom{n}{2}\)</span> Jaccard similarities of pairs of <span class="math inline">\(n\)</span> objects, then crunching some probabilities again shows that averaging <span class="math inline">\(\Theta(\eps^{-2} \log n)\)</span> independent estimates is good enough. Accurately estimating all Jaccard similarities is overkill for many applications, which motivates “locality sensitive hashing (LSH),” discussed briefly in the next section.</p>
<h1 id="s:lsh">A Glimpse of Locality Sensitive Hashing</h1>
<p>Recall from Section [ss:alta] the motivating application of filtering near-duplicate objects. If we only wanted to filter <span><em>exact</em></span> duplicates, then there is an easy and effective hashing-based solution:</p>
<ol>
<li><p>Hash all <span class="math inline">\(n\)</span> objects into <span class="math inline">\(b\)</span> buckets using a good hash function. (<span class="math inline">\(b\)</span> could be roughly <span class="math inline">\(n\)</span>, for example).</p></li>
<li><p>In each bucket, use brute-force search (i.e., compare all pairs) on the objects in that bucket to identify and remove duplicate objects.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p></li>
</ol>
<p>Why is this a good solution? Duplicate objects hash to the same bucket, so all duplicate objects are identified. With a good hash function and a sufficiently large number <span class="math inline">\(b\)</span> of buckets, different objects usually hash to different buckets. Thus, in a given bucket, we expect a small number of distinct objects, so brute-force search in a bucket does not waste much time comparing objects that are distinct and in the same bucket due to a hash function collision.</p>
<p>Naively extending this idea to filter <span><em>near-</em></span>duplicates fails utterly. The problem is that two objects <span class="math inline">\(x\)</span> and <span class="math inline">\(x&#39;\)</span> that are almost the same (but still distinct) are generally mapped to unrelated buckets by a good hash function. To extend duplicate detection to near-duplicate detection, we want a function <span class="math inline">\(h\)</span> such that, if <span class="math inline">\(x\)</span> and <span class="math inline">\(x&#39;\)</span> are almost the same, then <span class="math inline">\(h\)</span> is likely to map <span class="math inline">\(x\)</span> and <span class="math inline">\(x&#39;\)</span> to the same bucket. This is the idea behind <span><em>locality sensitive hashing (LSH)</em></span>.</p>
<p>(Additional optional material, not covered in lecture, to be added.)</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The trivial baseline for the nearest neighbor problem is brute-force search, where you just compute the distance between the query point and every point in the point set. Brute-force search scales linearly with the size of the point set. For large point sets one would prefer to maintain a data structure so that nearest-neighbor queries can be answered in sub-linear time.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Perhaps you’ve previously been tortured by the density function <span class="math inline">\(\tfrac{1}{\sqrt{2\pi}} e^{-x^2/2}\)</span>; we won’t need this here.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The proof is a few lines of crunching integrals, and can be found in any decent statistics textbook.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>But this doesn’t generally hold for non-independent random variables, right?<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>The fact that <span class="math inline">\(X^2\)</span> has expectation <span class="math inline">\(\mu^2\)</span> does not imply that <span class="math inline">\(X\)</span> has expectation <span class="math inline">\(\mu\)</span>. For example, suppose <span class="math inline">\(X\)</span> is equally likely to be 0 or 2 and <span class="math inline">\(\mu = \sqrt{2}\)</span>.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>We’ll discuss this idea in more detail next week, but we’ve already reviewed the tools needed to make this precise. Suppose <span class="math inline">\(X_1,\ldots,X_d\)</span> are independent and all have mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Then <span class="math inline">\(\sum_{i=1}^d X_i\)</span> has mean <span class="math inline">\(d\mu\)</span> and variance <span class="math inline">\(d\sigma^2\)</span>, and so the average <span class="math inline">\(\tfrac{1}{d} \sum_{i=1}^d X_i\)</span> has mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/d\)</span>. Thus, averaging <span class="math inline">\(d\)</span> independent unbiased estimates yields yields an unbiased estimate and drops the variance by a factor of <span class="math inline">\(d\)</span>.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>The constant suppressed by the <span class="math inline">\(\Theta\)</span> is reasonable, no more than 2. In practice, it’s worth checking if you get away with <span class="math inline">\(d\)</span> smaller than what is necessary for this theoretical guarantee. For typical applications, setting <span class="math inline">\(d\)</span> in the low hundreds should be good enough for acceptable results. See also Mini-Project #2.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Just as we use well-chosen hash functions in place of random functions, actual implementations of MinHash use hash functions instead of truly random permutations of <span class="math inline">\(U\)</span>. Unlike many applications of hashing, in this context it makes sense to use a hash function <span class="math inline">\(h\)</span> that maps <span class="math inline">\(U\)</span> back to itself, or perhaps to an even larger set to reduce the number of collisions. Collisions add error to the Jaccard similarity estimation procedure, but as long as there are few collisions — as with a good hash function with a large range — the effect is small.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>It’s often possible to be smarter here, for example if it’s possible to sort the objects.<a href="#fnref9">↩</a></p></li>
</ol>
</div>

{% endraw %}
