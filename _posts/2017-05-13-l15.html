---
layout: post
title: Lecture 15
author: Tim
---
{% raw %}

<h1 id="intro">Intro</h1>
<p>Thus far, we have seen a number of different approaches to extracting information from data. This week, we will discuss the Fourier transform, and other related transformations that map data into a rather different space, while preserving the information.</p>
<p>The Fourier transformation is usually presented as an operation that transforms data from the “time” or “space” domain, into “frequency” domain. That is, given a vector of data, with the <span class="math inline">\(i\)</span>th entry representing some value at time <span class="math inline">\(i\)</span> (or some value at location <span class="math inline">\(i\)</span>), the Fourier transform will map the vector to a different vector, <span class="math inline">\(w\)</span>, where the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(w\)</span> represents the “amplitude” of frequency <span class="math inline">\(i\)</span>. Here, the amplitude will be some complex number.</p>
<p>We will revisit the above interpretation of the Fourier transformation. First, a quick disclaimer: one of the main reasons people get confused by Fourier transforms is because there are a number of seemingly different interpretations of the Fourier transformation. In general, it is helpful to keep all of the interpretations in mind, since in different settings, certain interpretations will be more natural than others.</p>
<h1 id="the-fourier-transformation">The Fourier Transformation</h1>
<p>We begin by defining the (discrete) Fourier transformation of a length <span class="math inline">\(n\)</span> vector <span class="math inline">\(v\)</span>. As will become apparent shortly, it will prove more convenient to think of vectors and matrices as being <span class="math inline">\(0\)</span>-indexed. Hence <span class="math inline">\(v=v_0,v_1,\ldots,v_{n-1}.\)</span></p>
<p>Given a length <span class="math inline">\(n\)</span> vector <span class="math inline">\(v\)</span>, its Fourier transform <span class="math inline">\({\mathcal{F}}(v)\)</span> is a complex-valued vector of length <span class="math inline">\(n\)</span> defined as the product <span class="math display">\[\mathcal{F}(v) = M_n v,\]</span> where the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(M_n\)</span> is defined as follows: Let <span class="math inline">\(w_n = e^{-\frac{2 \pi i}{n}} = \cos(\frac{2 \pi i}{n}) - i \sin(\frac{2 \pi i}{n})\)</span> be an <span class="math inline">\(n\)</span>th root of unit (i.e. <span class="math inline">\(w_n^n = 1\)</span>). The <span class="math inline">\(j,k\)</span>th entry of <span class="math inline">\(M_n\)</span> is defined to be <span class="math inline">\(w_n^{jk}\)</span>. [Note that <span class="math inline">\(M_n\)</span> is 0-indexed.] Note that for any integer <span class="math inline">\(j\)</span>, <span class="math inline">\(w_n^j\)</span> is also an <span class="math inline">\(n\)</span>th root of unity, since <span class="math inline">\((w_n^j)^n = \left((w_n)^n\right)^j = 1^j = 1.\)</span></p>
<p>Throughout, we will drop the subscripts on <span class="math inline">\(M_n\)</span> and <span class="math inline">\(w_n\)</span>, and just refer to matrix <span class="math inline">\(M\)</span> and complex number <span class="math inline">\(w\)</span>, though keep in mind that both are functions of the dimension of the vector in question: if we are referring to the Fourier transform of a length <span class="math inline">\(n\)</span> vector, then <span class="math inline">\(M\)</span> is <span class="math inline">\(n \times n\)</span>, and <span class="math inline">\(w = e^{-2 \pi i/n}.\)</span></p>
<p>Figure [fig1] plots <span class="math inline">\(M_{20}\)</span>—the matrix corresponding to the Fourier transformation of length <span class="math inline">\(20\)</span> vectors. Each entry of the matrix is represented by an arrow, corresponding to the associated complex number, plotted on the complex plain. The upper left entry of the matrix corresponds to <span class="math inline">\(M_{20}(0,0)=w^0 = 1,\)</span> which is the horizontal unit vector in the complex plain. The entry <span class="math inline">\(M_{20}(1,1) = w^1\)</span> corresponds to the complex number that is <span class="math inline">\(1/20\)</span>th of the way around the complex unit circle. Please look at the Figure [fig1] until you understand exactly why each of the rows/columns looks the way it does. In the remainder of these notes, we will omit the subscript <span class="math inline">\(M_{n}\)</span> and just write <span class="math inline">\(M\)</span>, in settings where the dimensionality of the transform is clear from the context.</p>
<p> [fig1]</p>
<p>We will see a few other interpretations of the Fourier transform, though if you ever get confused, you should always go back to the above definition: the Fourier transform of a vector <span class="math inline">\(v\)</span> is simply the product <span class="math inline">\(M v\)</span>, for the matrix <span class="math inline">\(M\)</span> as defined above.</p>
<h2 id="basic-properties">Basic Properties</h2>
<p>Perhaps the two most important properties of the Fourier transform are the following:</p>
<ul>
<li><p>The Fourier transformation of a length <span class="math inline">\(n\)</span> vector can be computed in time <span class="math inline">\(O(n \log n)\)</span>, using the “Fast Fourier Transformation” algorithm. This is a basic recursive divide/conquer algorithm, and is described in the last section of these notes. It might seem miraculous that we can actually multiply a vector by the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(M\)</span> in much less time than it would take to even write <span class="math inline">\(M\)</span>—-the intuition behind why this is possible can be gleaned from Figure [fig1]: <span class="math inline">\(M\)</span> is a highly structured matrix, with lots of repeated structure (e.g. the top half looks rather similar to the bottom half).</p></li>
<li><p>The Fourier transform is <em>almost</em> its own inverse. Letting <span class="math inline">\(M\)</span> denote the transformation matrix as defined above, <span class="math display">\[M M = \left( \begin{array}{ccccc}
n &amp; 0 &amp; \ldots &amp; 0 &amp; 0 \\
0 &amp; \ldots &amp; 0 &amp; 0 &amp; n\\
0 &amp; \ldots &amp; 0 &amp; n &amp; 0\\
0 &amp; \ldots &amp; n &amp; 0 &amp; 0\\
 \vdots &amp; \iddots &amp;  &amp; \vdots \\
n &amp; 0 &amp; \ldots &amp; 0\end{array} \right).\]</span></p></li>
</ul>
<p>Specifically, <span class="math inline">\(w = M(Mv)\)</span> is the same as <span class="math inline">\(v\)</span> if one scales the entries by <span class="math inline">\(1/n\)</span>, and then reverses the order of the last <span class="math inline">\(n-1\)</span> entries. In general, it is convenient to think of the inverse Fourier transform as being essentially the same as the Fourier transform; their properties are essentially identical.</p>
<p>Explicitly, the inverse Fourier transform is multiplication by the matrix <span class="math inline">\(M^{-1}\)</span>, whose <span class="math inline">\(j,k\)</span>th entry is <span class="math inline">\((M^{-1})_{j,k} = w^{-jk} = e^{2 j k \pi i / n}.\)</span> From our definition, it is clear that <span class="math inline">\(M^{-1} M v = v,\)</span> and hence the inverse transform maps the transformed vector <span class="math inline">\(M v\)</span> back to <span class="math inline">\(v\)</span>.</p>
<h2 id="an-alternate-perspective">An alternate perspective</h2>
<p>Given the form of the inverse transformation explained above, a different interpretation of the Fourier transformation of a vector <span class="math inline">\(v\)</span>, is the vector of (complex-valued) coefficients <span class="math inline">\(c_0,\ldots,c_{n-1}\)</span> in the unique representation of <span class="math inline">\(v\)</span> in the “Fourier basis”. Namely, the coefficients satisfy <span class="math display">\[v = \sum_{j = 0}^{n-1} c_j  b_j,\]</span> where <span class="math inline">\(b_j\)</span> is a length <span class="math inline">\(n\)</span> vector defined by the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(M^{-1},\)</span> namely <span class="math inline">\(b_j(k) = e^{2 \pi i \frac{j k}{n}}.\)</span> (The lack of a minus sign in the exponent is <em>not</em> a typo!!! ) The uniqueness of this representation can be observed from the fact that the columns of <span class="math inline">\(M^{-1}\)</span>, which are the vectors <span class="math inline">\((b_j)\)</span>, are linearly independent (and, in fact, are orthogonal to each other).</p>
<p>What does this all mean? The <span class="math inline">\(j\)</span>th column of <span class="math inline">\(M^{-1}\)</span> is a vector whose entries go around the complex unit circle <span class="math inline">\(j\)</span> times—you can think of this vector as representing the <em>frequency</em> <span class="math inline">\(j\)</span>, whose real component consists of a cosine that cycles <span class="math inline">\(j\)</span> times, and the complex portion consists of a sine that cycles <span class="math inline">\(j\)</span> times. The <span class="math inline">\(j\)</span>th coordinate of the Fourier transform <span class="math inline">\(M v\)</span> tells you how much of the <span class="math inline">\(j\)</span>th frequency the signal, <span class="math inline">\(v\)</span> has. In this sense, the Fourier transform gives the representation of <span class="math inline">\(v\)</span> in the frequency domain, as a sum of different frequencies.</p>
<h2 id="the-fourier-transform-as-polynomial-evaluation-and-interpolation">The Fourier Transform as Polynomial Evaluation and Interpolation</h2>
<p>One of the more illuminating properties of the Fourier transform is that it can be regarded as <em>both</em> polynomial evaluation, and polynomial interpolation.</p>
<p>Given a length <span class="math inline">\(n\)</span> vector <span class="math inline">\(v\)</span>, define <span class="math display">\[P_v(x) = v_0 + v_1 x + v_2 x^2 + \ldots + v_{n-1} x^{n-1}\]</span> to be the polynomial associated to vector <span class="math inline">\(v\)</span>. The <span class="math inline">\(k\)</span>th entry of the transform <span class="math inline">\(M v\)</span>, is <span class="math display">\[v_0 + v_1 w^k + v_2 w^{2k} + v_3 w^{3k} + \ldots v_{n-1}w^{(n-1)k}  = P_v(w^k).\]</span> Hence the Fourier transform of a vector <span class="math inline">\(v\)</span> is simply the vector of evaluations of the associated polynomial <span class="math inline">\(P_v(x),\)</span> evaluated at the <span class="math inline">\(n\)</span> complex roots of unity.</p>
<p>The fact that the transform can be computed in time <span class="math inline">\(O(n \log n)\)</span> should be a bit striking: it takes <span class="math inline">\(O(n)\)</span> operations to evaluate <span class="math inline">\(P_v(x)\)</span> at a single value—we are claiming that we can evaluate <span class="math inline">\(P_v(x)\)</span> at all <span class="math inline">\(n\)</span> roots of unity in an amount of time that is only a logarithmic (as opposed to linear) factor more than the amount of time needed to make a single evaluation.</p>
<p>Now for the real magic: if the Fourier transform takes a vector representation of a polynomial, and returns the evaluation at the roots of unity, then the inverse transform takes a vector representing the values of <span class="math inline">\(P_v\)</span> evaluated at the roots of unity, and returns the coefficients of the polynomial <span class="math inline">\(P_v\)</span>. This is simply <em>polynomial interpolation</em>!!!</p>
<p>Recall from grade school that interpolating polynomials is often a headache; evaluating a polynomial at some number is easy—you just plug it in, and you are done. Polynomial interpolation usually involves writing out a big system of equations, and getting terms to cancel, etc—usually polynomial interpolation is much harder than polynomial evaluation. The fact that the Fourier transform is essentially its own inverse (and both the transform and its inverse can be computed in time <span class="math inline">\(O(n \log n)\)</span> means that interpolating a polynomial from its evaluation at the <span class="math inline">\(n\)</span> roots of unity is easy (and is no harder than simply evaluating the polynomial at the roots of unity)!</p>
<p>This duality between polynomial evaluation and interpolation (at the roots of unity) provides the main connection between convolution and the Fourier transform, which we explore in Section [sec:conv].</p>
<h1 id="the-fast-fourier-transformation">The Fast Fourier Transformation</h1>
<p>The Fast Fourier Transformation was first discovered by Gauss, though was largely forgotten, and was independently discovered by Cooley and Tukey in the 1950’s. All we want to do is compute the product <span class="math inline">\(M v\)</span>, where <span class="math inline">\(M\)</span> is the <span class="math inline">\(n \times n\)</span> Fourier transformation matrix whose <span class="math inline">\(j,k\)</span>th entry is <span class="math inline">\(w^{jk}\)</span> for <span class="math inline">\(w= e^{-2 \pi i /n}\)</span> is an <span class="math inline">\(n\)</span>th root of unity. This high level intuition why it might be possible to compute this product in much less time than it would take to even write out the matrix <span class="math inline">\(M\)</span>, is because matrix <span class="math inline">\(M\)</span> has tons of structure. In fact, <span class="math inline">\(M\)</span> has so much structure that we will be able to compute <span class="math inline">\(M v\)</span> by essentially reusing portions of the computation.</p>
<p>The Fast Fourier transform (FFT) algorithm is a recursive algorithm. We will describe it in the case that <span class="math inline">\(n=2^k\)</span> for some integer <span class="math inline">\(k\)</span>. In the case that <span class="math inline">\(n\)</span> is not a power of <span class="math inline">\(2\)</span> similar ideas and some messy tricks still allow for fast computation, though we will not discuss these modifications.</p>
<p>To define the FFT, all we will do is rummage around inside the matrix <span class="math inline">\(M_n\)</span>, and try to find some reoccurring copies of submatrices that resemble <span class="math inline">\(M_{n/2}\)</span>—the matrix corresponding to the Fourier transformation of a vector of length <span class="math inline">\(n/2\)</span>. Figure [fig7] depicts <span class="math inline">\(M_{16},\)</span> with the entries color coded for ease of reference.</p>
<p> [fig7]</p>
<p>We will now “rummage” around in <span class="math inline">\(M_n\)</span> and identify four submatrices that all resemble <span class="math inline">\(M_{n/2}.\)</span> If <span class="math inline">\(w_{n} = e^{-2 \pi i /n},\)</span> then <span class="math inline">\(w_n^2 = w_{n/2}.\)</span> Hence it follows that the even indexed columns of the first half of the rows of <span class="math inline">\(M_n\)</span> <em>exactly</em> form the matrix <span class="math inline">\(M_{n/2}.\)</span> [For example, the black entries of Figure [fig7] exactly correspond to <span class="math inline">\(M_{8}\)</span>.] Additionally, the even columns of the second half of the rows (i.e. the blue elements) are identical to the even columns of the first half of the rows ( i.e. the black elements), and hence are also a copy of <span class="math inline">\(M_{n/2}\)</span>. Now for the odd columns: first observe that the <span class="math inline">\(j\)</span>th odd column of <span class="math inline">\(M_n\)</span> is each to the <span class="math inline">\(j\)</span>th even column, just scaled by <span class="math inline">\(w_n^j\)</span>. Additionally, the odd columns of the second half of the rows (i.e. the pink elements) are negatives of the odd columns of the first half of the rows (i.e. the red elements)</p>
<p>Based on the above, we have shown the following: the first <span class="math inline">\(n/2\)</span> indices of the Fourier transform of a length <span class="math inline">\(n\)</span> vector <span class="math inline">\(v\)</span> can be computed as <span class="math inline">\({\mathcal{F}}(v(evens)) + {\mathcal{F}}(v(odds))\times s,\)</span> where <span class="math inline">\(s\)</span> is the vector whose <span class="math inline">\(j\)</span>th entry is <span class="math inline">\(w_n^j\)</span>, and “<span class="math inline">\(\times\)</span>” denotes element-wise multiplication. Similarly, the second half of the indices of the Fourier transform <span class="math inline">\({\mathcal{F}}(v)\)</span> can be computed as <span class="math inline">\({\mathcal{F}}(v(evens)) - {\mathcal{F}}(v(odds))\times s\)</span>.</p>
<p>The crucial observation is that the above decomposition only involves the computation of two Fourier transforms of length <span class="math inline">\(n/2\)</span> vectors, and two element-wise multiplications and additions. Letting <span class="math inline">\(T(n)\)</span> denote the time it will take to compute the Fourier transform of a length <span class="math inline">\(n\)</span> vector, we have shown that the above recursive procedure will result in an algorithm whose runtime satisfies <span class="math display">\[T(n) = 2 T(n/2) + O(n).\]</span> Solving this recurrence relation yields that <span class="math inline">\(T(n) = O(n \log n).\)</span></p>
<p>For clarity, we restate the recursive algorithm derived from the above decomposition:</p>
<p><span> </span></p>
<h1 id="sec:conv">Convolution</h1>
<p>Convolution is an incredibly useful tool that is closely intertwined with the Fourier transform. There are a number of interpretations/definitions, though perhaps the most insightful is via polynomial multiplication.</p>
<p>The convolution of two vectors, <span class="math inline">\(v,w\)</span> of respective lengths <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span> is denotes <span class="math inline">\(v \ast w = w \ast v\)</span>, is the vector of coefficients of the product of the polynomials associated to <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>, <span class="math inline">\(P_v(x) P_w(x).\)</span></p>
<p>Let <span class="math inline">\(v = [1, 2, 3], w = [ 4, 5]\)</span>, then their convolution <span class="math inline">\(v \ast w = [4, 13, 22, 15]\)</span> representing the fact that <span class="math inline">\(P_v(x) = 1 + 2x + 3x^2,\)</span> <span class="math inline">\(P_w(x) =4 + 5x,\)</span> and their product <span class="math inline">\((1+2x+3x^2)(4+5x) = 4 + 13x + 22x^2 + 15 x^3.\)</span></p>
<p>The following fact—that convolutions can be computed incredibly quickly via the Fast Fourier transformation—is the main reason that convolutions are as useful and pervasive as they are.</p>
<p>[fact1] The convolution <span class="math inline">\(v \ast w\)</span> can be expressed as the inverse Fourier transformation of the component-wise product of the Fourier transformations of <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>: <span class="math display">\[v \ast w = {\mathcal{F}}^{-1}\left({\mathcal{F}}(v) \times {\mathcal{F}}(w) \right),\]</span> where “<span class="math inline">\(\times\)</span>” denotes the element-wise product of the two vectors.</p>
<p>This fact follows immediately from the polynomial interpolation/evaluation view of the Fourier transform: The product of the evaluations of <span class="math inline">\(P_v\)</span> and <span class="math inline">\(P_w\)</span> will be the evaluations of the polynomial corresponding to the product of <span class="math inline">\(P_v\)</span> and <span class="math inline">\(P_w\)</span>, hence the interpolation of these evaluations will give the coefficients of this product, namely the convolution <span class="math inline">\(v \ast w\)</span>. Of course, in order for this to make sense, we need to ensure that the number of evaluation points in larger than the degree of <span class="math inline">\(P_v(x)P_w(x)\)</span>: i.e. if <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are length <span class="math inline">\(n\)</span> vectors, then we should pad them with <span class="math inline">\(n\)</span> zeros, and compute their Fourier transforms as length <span class="math inline">\(2n\)</span> vectors.</p>
<p>Another nice property of Fact [fact1] is that it implies that convolutions can be computed in time <span class="math inline">\(O(n \log n)\)</span>, and that they can also be inverted (provided the inverse exists) in time <span class="math inline">\(O(n \log n)\)</span>. To clarify, if we are given a vector <span class="math inline">\(q = v \ast w\)</span> for some known vector <span class="math inline">\(w\)</span>, we can invert this operation and find <span class="math inline">\(v\)</span>. In particular, we have <span class="math display">\[v = {\mathcal{F}}^{-1}\left( {\mathcal{F}}(q) ./ {\mathcal{F}}(w)\right),\]</span> where “<span class="math inline">\(./\)</span>” denotes component-wise division. We will see some applications of this in the next section.</p>
<h2 id="examples">Examples</h2>
<p>Consider Figure [fig2] representing some vector <span class="math inline">\(v\)</span>, and let <span class="math inline">\(w\)</span> be a Gaussian (see Figure [fig3]). The Fourier transform of a Gaussian is a Gaussian—the transform of a skinny Gaussian is a fat Gaussian, and vice versa. Figure [fig4] depicts the convolution <span class="math inline">\(v \ast w\)</span>. Look at this figure, and the definition of convolution, until it makes sense to you.</p>
<p> [fig2]</p>
<p> [fig3]</p>
<p> [fig4]</p>
<p>We can also try to invert a convolution: in this example, we will be looking at images and 2-dimensional convolutions and Fourier transforms. For our purposes, don’t worry about the details of this and just think of these as high dimensional analogs.</p>
<p>TODO: add image/motion blur and de-blurred.</p>
<h2 id="convolutions-everywhere">Convolutions Everywhere!</h2>
<p>Given a vector <span class="math inline">\(a\)</span>, consider the transformation that takes a vector <span class="math inline">\(v\)</span>, and returns the convolution <span class="math inline">\(v \ast a\)</span>. For every vector <span class="math inline">\(a\)</span>, this transformation has two crucial properties:</p>
<ul>
<li><p>Linearity: For any vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>, <span class="math display">\[(v + w) \ast a = (v \ast a) + (w \ast a),\]</span> and for any constant <span class="math inline">\(c\)</span>, <span class="math inline">\((cv) \ast a = c (v \ast a).\)</span></p></li>
<li><p>Translational invariance: For any vector <span class="math inline">\(v\)</span>, if one “shifts” it to the right by padding it with <span class="math inline">\(k\)</span> zeros, then the convolution of the shifted <span class="math inline">\(v\)</span> with <span class="math inline">\(a\)</span> will be the same as if one shifted the convolution of <span class="math inline">\(v\)</span> and <span class="math inline">\(a\)</span>. For example, for <span class="math inline">\(v = [1, 2, 3],\)</span> <span class="math inline">\(a = [4,5]\)</span>, <span class="math inline">\(v \ast a = [4 13 22]\)</span>, and if we shift <span class="math inline">\(v\)</span> to get <span class="math inline">\(v_s = [0, 0, 0, 0, 1, 2, 3]\)</span>, then <span class="math inline">\(v_s \ast a = [0, 0, 0, 0, 4, 13, 22],\)</span> namely the shifted vector <span class="math inline">\(v \ast a\)</span>.</p></li>
</ul>
<p>While the above two properties are easily seen to hold for convolution, it turns out that they <em>define</em> convolution:</p>
<p>[fact2] And transformation of a vector that is linear, and translation invariant is a convolution!</p>
<p>To appreciate the above fact, we will discuss two examples of natural transformations for which we can fruitfully leverage the fact that they are convolutions.</p>
<p>Nearly every physical force is linear, and translation invariant. For example, consider gravity. Gravity is linear (i.e. one can just add up the effects of different gravitational fields), and it is translation invariant (i.e. if I move 10 feet to the left, the gravitational field that my mass exerts gets translated 10 feet to the left, but is otherwise invariant). Hence the above fact shows that gravity is a convolution. We just need to figure out what to convolve by.</p>
<p>For the purposes of this example, we will work in 2 dimensions. Recall that the gravitational force exerted on a point at location <span class="math inline">\(x,y\)</span> by a unit of mass at the origin has magnitude <span class="math inline">\(\frac{1}{x^2+y^2},\)</span> and is in the direction <span class="math inline">\(\left(\frac{-x}{\sqrt{x^2+y^2}} , \frac{-y}{\sqrt{x^2+y^2}} \right).\)</span> Hence, given a description of the density of some (2-d) asteroid, we can simply convolve this density with the function <span class="math inline">\(f_X(x,y) = \frac{-x}{(x^2+y^2)^{1.5}}\)</span> to compute the <span class="math inline">\(X\)</span>-coordinate of the force of gravity everywhere that results from the given density plot. Similarly for the <span class="math inline">\(Y\)</span>-coordinate of the gravitational force. The Figure [fig5] depicts the density of an asteroid in space. Figure [fig6] depicts the force of gravity, whose <span class="math inline">\(X\)</span> component is the convolution of the asteroid by the function <span class="math inline">\(f_X(x,y) = \frac{-x}{(x^2+y^2)^{1.5}}\)</span> and the <span class="math inline">\(Y\)</span> component is the convolution of the asteroid by the function <span class="math inline">\( f_Y(x,y) = \frac{-y}{(x^2+y^2)^{1.5}}\)</span>.</p>
<p>Note that since the computations are convolutions, they can be computed in time <span class="math inline">\(O(n \log n)\)</span>, where <span class="math inline">\(n\)</span> is the number of points in our grid. Naively, one might have thought that we would have needed to spend time <span class="math inline">\(O(n^2)\)</span> to compute the gravitational effect on every point caused by every other point. This illustrates one reason why convolutions and fast fourier transformations are so important to the efficient simulation of many physical systems.</p>
<p> [fig5]</p>
<p> [fig6]</p>
<p>If we were given the gravitational field depicted in Figure [fig6], we could invert the convolution to derive the density of the asteroid. This inversion would could also be computed just as efficiently using fast Fourier transformations because <span class="math inline">\(v = {\mathcal{F}}^{-1} \left( {\mathcal{F}}(v \ast w) ./ {\mathcal{F}}w \right),\)</span> where “<span class="math inline">\(./\)</span>” denotes element-wise division.</p>
<p>To see a second illustration of the power of Fact [fact2], note that differentiation is also both linear, and translation invariant (i.e. if you shift a function over to the right by 3 units, then you also shift its derivative over by 3 units). Since differentiation is linear and translation invariant, it is a convolution, we just need to figure out what it is a convolution of. Since differentiation only really applies to continuous functions, and we have been talking about discrete Fourier transformations, thus far, we will be somewhat informal. Consider a vector <span class="math inline">\(v\)</span> with <span class="math inline">\(n\)</span> components, and imagine it as a function <span class="math inline">\(v(x)\)</span>, defined for <span class="math inline">\(x = 0,1,\ldots, n-1\)</span>. Recall that <span class="math display">\[v(x) = q(0) + q(1) e^{2 \pi i \frac{x}{n}}+ q(2) e^{2 \pi i \frac{2x}{n}}+ q(3) e^{2 \pi i \frac{3x}{n}}+\ldots + q(n-1)e^{2 \pi i \frac{2(n-1)}{n}}.\]</span> If we differentiate, ignoring that <span class="math inline">\(x\)</span> is supposed to only be defined on the integers, we get: <span class="math display">\[v&#39;(x) = 0\cdot q(0) +  i\cdot q(1) e^{2 \pi i \frac{x}{n}}+ (2i) \cdot q(2) e^{2 \pi i \frac{2x}{n}}+ (3i)\cdot q(3) e^{2 \pi i \frac{3x}{n}}+\ldots +(n-1)i \cdot q(n-1)e^{2 \pi i \frac{2(n-1)}{n}}.\]</span> Namely, the <span class="math inline">\(j\)</span>th entry of the Fourier transform of the derivative of <span class="math inline">\(v\)</span> is obtained by multiplying the <span class="math inline">\(j\)</span>th entry of the Fourier transform of <span class="math inline">\(v\)</span> by <span class="math inline">\(ij\)</span>.</p>
<p>If we want to take a second or third derivative, we simply convolve again (and again). Recall that we can also invert convolutions just as computationally efficiently as we can compute them. In this case, this will let us solve <em>differential equations</em> extremely efficiently. For example, suppose we have some 2-d image depicting some density of space, <span class="math inline">\(f(x,y)\)</span>. We can very efficiently compute the sum of the second derivatives in the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> direction: <span class="math inline">\(g(x,y) = \frac{d^2 g(x,y)}{dx^2} +  \frac{d^2 g(x,y)}{dy^2}\)</span>. Given <span class="math inline">\(g(x,y)\)</span>, we can also just as easily invert this convolution to obtain <span class="math inline">\(f(x,y)\)</span> (up to an additive constant). This corresponds to solving the Poisson equation (a second-order differential equation) that arises in several places in physics.</p>
<p>TODO: add figures of this.</p>
<h1 id="beyond-the-fourier-transform-wavelets-and-other-bases">Beyond the Fourier Transform: Wavelets and Other Bases</h1>
<p>TODO: whats wrong with the Fourier Transform. (example of audio whistling vs percussive sounds).</p>
<p>TODO: Wavelets</p>
<p>TODO: Moments</p>

{% endraw %}
