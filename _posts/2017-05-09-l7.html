---
layout: post
title: Lecture 7
author: Tim
---
{% raw %}
<h1 id="s:toy">1  A Toy Example</h1>
<p>The following toy example gives a sense of the problem solved by principal component analysis (PCA) and many of the reasons why you might want to apply it to a data set — to visualize the data in a lower-dimensional space, to understand the sources of variability in the data, and to understand correlations between different coordinates of the data points.</p>
<p>Suppose you ask some friends to rate, on a scale of 1 to 10, four different foods: kale salad, Taco Bell, sashimi, and pop tarts, with the results shown in Table [table:toy].</p>
<table>
<caption>Your friends’ ratings of four different foods.<span data-label="table:toy"></span></caption>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">kale</th>
<th align="center">taco bell</th>
<th align="center">sashimi</th>
<th align="center">pop tarts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Alice</td>
<td align="center">10</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">7</td>
</tr>
<tr class="even">
<td align="center">Bob</td>
<td align="center">7</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">10</td>
</tr>
<tr class="odd">
<td align="center">Carolyn</td>
<td align="center">2</td>
<td align="center">9</td>
<td align="center">7</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">Dave</td>
<td align="center">3</td>
<td align="center">6</td>
<td align="center">10</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>So in our usual notation, <span class="math inline">\(m\)</span> (the number of data points) is 4, and <span class="math inline">\(n\)</span> (the number of dimensions) is also 4. Note that, in contrast to the linear regression problem discussed last week, the data points do not have “labels” of any sort. Can we usefully visualize this data set in fewer than 4 dimensions? Can we understand the forces at work behind the differences in opinion of the various foods?</p>
<p>The key observation is that each row (data point) can be approximately expressed as <span class="math display">\[\label{eq:toy1}
\bar{{\bf x}} + a_1{\bf v}_1 + a_2{\bf v}_2,\]</span> where <span class="math display">\[\bar{{\bf x}} = (5.5,4.5,5,5.5)\]</span> is the average of the data points, <span class="math display">\[{\bf v}_1 = (3,-3,-3,3),\]</span> <span class="math display">\[{\bf v}_2 = (1,-1,1,-1),\]</span> and <span class="math inline">\((a_1,a_2)\)</span> is (1,1) for Alice, (1,-1) for Bob, (-1,-1) for Carolyn, and (-1,1) for Dave (Figure [f:plot]). For example, <span class="math inline">\({\bf x} + {\bf v}_1 + {\bf v}_2 = (9.5,0.5,3,7.5)\)</span>, which is approximately equal to Alice’s scores. Similarly, <span class="math inline">\({\bf x} - {\bf v}_1 - {\bf v}_2 =
(1.5,8.5,7,3.5)\)</span>, which is approximately equal to Carolyn’s scores.</p>
<div class="figure">
<img src="/cs168/images/l7/cluster.jpg" alt="Visualizing 4-dimensional data in the plane." />
<p class="caption">Visualizing 4-dimensional data in the plane.<span data-label="f:plot"></span></p>
</div>
<p>What use is such an approximation? One answer is that it provides a way to visualize the data points, as in Figure [f:plot]. With more data points, we can imagine inspecting the resulting figure for clusters of similar data points.</p>
<p>A second answer is that it helps interpret the data. We think of each data point as having a “<span class="math inline">\(v_1\)</span>-coordinate” (i.e., <span class="math inline">\(a_1\)</span>) and a “<span class="math inline">\(v_2\)</span>-coordinate” (<span class="math inline">\(a_2\)</span>). What does the vector <span class="math inline">\(v_1\)</span> “mean?” The first and fourth coordinates both have a large positive value (and so are positively correlated) while the second and third coordinates have a large negative value (and so are positively correlated with each other and negatively correlated with the first and fourth coordinates). Noting that kale salad and pop tarts are vegetarian<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> while Taco Bell and sashimi are not, we can interpret the <span class="math inline">\(v_1\)</span> coordinate as indicating the extent to which someone has vegetarian preferences.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> By similar reasoning, we can interpret the second vector <span class="math inline">\(v_2\)</span> as indicating the extent to which someone is health-conscious. The fact that <span class="math inline">\(v_1\)</span> has larger coordinates than <span class="math inline">\(v_2\)</span> indicates that it is the stronger of the two effects.</p>
<p>We’ll see that the vectors <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>, once normalized, correspond to the “top two principal components” of the data. The point of PCA is to compute approximations of the type  automatically, including for large data sets.</p>
<h2 id="goal-of-pca">1.1  Goal of PCA</h2>
<p>The goal of PCA is to approximately express each of <span class="math inline">\(m\)</span> <span class="math inline">\(n\)</span>-dimensional vectors<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> <span class="math inline">\({\bf x}_1,\ldots,{\bf x}_m \in
\mathbb{R}^n\)</span> as linear combinations of <span class="math inline">\(k\)</span> <span class="math inline">\(n\)</span>-dimensional vectors <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k \in \mathbb{R}^n\)</span>, so that <span class="math display">\[{\bf x}_i \approx \sum_{j=1}^k a_{ij}{\bf v}_j\]</span> for each <span class="math inline">\(i=1,2,\ldots,m\)</span>. (In the toy example, <span class="math inline">\(m=n=4\)</span> and <span class="math inline">\(k=2\)</span>.) PCA offers a formal definition of which <span class="math inline">\(k\)</span> vectors are the “best” ones for this purpose. Next lecture, we’ll see that there are also good algorithms for computing these vectors.</p>
<h2 id="relation-to-dimensionality-reduction">Relation to Dimensionality Reduction</h2>
<p>The high-level goal of PCA should remind you of a couple of topics studied in previous lectures. First, recall Johnson-Lindenstrauss (JL) dimensionality reduction (Lecture #4), where the goal is also to re-express a bunch of high-dimensional points as low-dimensional points. Recall that this method maps a point <span class="math inline">\({\bf x}\)</span> to a point <span class="math inline">\(({
{\langle {{\bf x}} , {{\bf r}_1} \rangle}
},\ldots,{
{\langle {{\bf x}} , {{\bf r}_k} \rangle}
})\)</span>, where each <span class="math inline">\({\bf r}_j\)</span> is a random unit vector (independent standard Gaussians in each coordinate, then normalized).<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> The same <span class="math inline">\({\bf r}_j\)</span>’s are used for all data points. PCA and JL dimensionality reduction are different in the following respects.</p>
<ol>
<li><p>JL dimensionality reduction assumes that you care about the Euclidean distance between each pair of points — it is designed to approximately preserve these distances. PCA offers no guarantees about preserving pairwise distances.</p></li>
<li><p>JL dimensionality reduction is data-oblivious, in the sense that the random vectors <span class="math inline">\({\bf r}_1,\ldots,{\bf r}_k\)</span> are chosen without looking at the data points. PCA, by contrast, is deterministic, and the whole point is to compute vectors <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k\)</span> that “explain the data.”</p></li>
<li><p>For the above reason, the <span class="math inline">\(k\)</span> coordinates used in JL dimensionality reduction have no intrinsic meaning, while those used in PCA are often meaningful (recall the toy example).</p></li>
<li><p>JL dimensionality reduction generally only gives good results when <span class="math inline">\(k\)</span> is at least in the low hundreds. On the other hand, this value of <span class="math inline">\(k\)</span> works for every data set of a given size. PCA can give meaningful results even when <span class="math inline">\(k\)</span> is 1 or 2, but there are also data sets where it provides little interesting information (for any <span class="math inline">\(k\)</span>).</p></li>
</ol>
<h2 id="relation-to-linear-regression">Relation to Linear Regression</h2>
<p>Both PCA and linear regression (see Lectures #5 and #6) concern fitting the “best” <span class="math inline">\(k\)</span>-dimensional subspace to a point set. One difference between the two methods is in the interpretation of the input data. With linear regression, each data point has a real-valued label, which can be interpreted as a special “label coordinate.” The goal of linear regression is to learn the relationship between this special coordinate and the other coordinates. This makes sense when the “label” is a dependent variable that is approximately a linear combination of all of the other coordinates (the independent variables). In PCA, by contrast, all coordinates are treated equally, and they are not assumed to be independent from one another. This makes sense when there is a set of latent (i.e., hidden/underlying) variables, and all of the coordinates of your data are (approximately) linear combinations of those variables.</p>
<p>Second, PCA and linear regression use different definitions of “best fit.” Recall that in linear regression the goal is to minimize the total squared error, where the error on a data point is the difference between the linear function’s prediction and the actual label of the data point. With one-dimensional data (together with labels), this corresponds to computing the line that minimizes the sum of the squared vertical distances between the line and the data points (Figure [f:perp](a)). This reflects the assumption that the coordinate corresponding to labels is the important one. We define the PCA objective function formally below; for two-dimensional data, the goal is to compute the line that minimizes the sum of the squared <span><em>perpendicular</em></span> distances between the line and the data points (Figure [f:perp](b)). This reflects the assumption that all coordinates play a symmetric role and so the usual Euclidean distance is most appropriate (e.g., rotating the point set just results in the “best-fit” line being rotated accordingly). On Mini-Project #4 you will learn more about which situations call for PCA and which call for linear regression.</p>
<h1 id="defining-the-problem">Defining the Problem</h1>
<h2 id="ss:pp">Preprocessing</h2>
<p>Before using PCA, it’s important to preprocess the data. First, the points <span class="math inline">\({\bf x}_1,\ldots,{\bf x}_m\)</span> should be centered around the origin, in the sense that <span class="math inline">\(\sum_{i=1}^m {\bf x}_i\)</span> is the all-zero vector. This is easy to enforce by subtracting (i.e., shifting) each point by the “sample mean” <span class="math inline">\(\bar{{\bf x}} = \tfrac{1}{m} \sum_{i=1}^m {\bf x}_i\)</span>. (In the toy example, we had <span class="math inline">\(\bar{{\bf x}} = (5.5,4.5,5,5.5)\)</span>.) After finding the best-fit line for the shifted point set, one simply shifts the line back by the original sample mean to get the best-fit line for the original uncentered data set. This shifting trick makes the necessary linear algebra simpler and clearer.</p>
<p>Second, in many applications it is important to scale each coordinate appropriately. The most common approach to this is: if <span class="math inline">\({\bf x}_1,\ldots,{\bf x}_m\)</span> is a point set centered at the origin, then for each coordinate <span class="math inline">\(j=1,2,\ldots,n\)</span>, divide the <span class="math inline">\(j\)</span>th coordinate of every point by the “sample deviation” in that coordinate, <span class="math inline">\(\sqrt{\sum_{i=1}^m x_{ij}^2}\)</span>. The motivation for this coordinate scaling is the fact that, without it, the result of PCA would be highly sensitive to the units in which each coordinate is measured. For example, changing units from miles to kilometers in some coordinate yields the “same” data set in some sense, and yet this change would scale up all values in this coordinate, which in turn would cause a different “best-fit” line to be computed.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> In some applications, like with images — where all coordinates are in the same units, namely pixel intensities — there is no need to do such coordinate scaling. (The same goes for the toy example in Section [s:toy].)</p>
<h2 id="the-objective-function">The Objective Function</h2>
<p>For clarity, we first discuss the special case of <span class="math inline">\(k=1\)</span>, where the goal is to fit the “best” line to a data set. Everything in this section generalizes easily to the case of general <span class="math inline">\(k\)</span> (Section [ss:largek]).</p>
<p>As foreshadowed above, PCA defines the “best-fit line” as the one that minimizes the average squared Euclidean distance between the line and the data points: <span class="math display">\[\label{eq:min}
\underset{{{\bf v} \,:\, {
{\| {{\bf v}} \|}
} = 1}}{\operatorname{argmin}} \frac{1}{m} \sum_{i=1}^m \left( 
(\text{distance between ${\bf x}_i$ and line spanned by ${\bf v}$})^2 \right).\]</span> Note that we’re identifying a line (through the origin) with a unit vector <span class="math inline">\({\bf v}\)</span> in the same direction. Minimizing the Euclidean distances between the points and the chosen line should seem natural enough; the one thing you might be wondering about is why we square these distances before adding them up. One reason is that it ensures that the best-fit line passes through the origin. Another is a tight connection to variance maximization, discussed next.</p>
<div class="figure">
<img src="ip" alt="The geometry of the inner product." />
<p class="caption">The geometry of the inner product.<span data-label="f:ip"></span></p>
</div>
<p>Recall the geometry of projections and the inner product (Figure [f:ip]). In particular, <span><em><span class="math inline">\({
{\langle {{\bf x}_i} , {{\bf v}} \rangle}
}\)</span> is the (signed) length of the projection of <span class="math inline">\({\bf x}_i\)</span> onto the line spanned by <span class="math inline">\({\bf v}\)</span>.</em></span> Recall the Pythagorean Theorem: for a right triangle with sides <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and hypotenuse <span class="math inline">\(c\)</span>, <span class="math inline">\(a^2 + b^2 =
c^2\)</span>. Instantiating this for the right triangle shown in Figure [f:ip], we have <span class="math display">\[\label{eq:pythag}
(\text{dist(${\bf x}_i \leftrightarrow$ line)})^2
+ {
{\langle {{\bf x}_i} , {{\bf v}} \rangle}
}^2 = {
{\| {{\bf x}_i} \|}
}^2.\]</span> The right-hand side of  is a constant, independent of the choice of line <span class="math inline">\({\bf v}\)</span>. Thus, there is a zero-sum game between the squared distance between a point <span class="math inline">\({\bf x}_i\)</span> and the line spanned by <span class="math inline">\({\bf v}\)</span> and the squared length of the projection of <span class="math inline">\({\bf x}_i\)</span> on this line — making one of these quantities bigger makes the other one smaller. This implies that the objective function of maximizing the squared projections — the <span><em>variance</em></span> of the projection of the point set — is equivalent to the original objective in : <span class="math display">\[\label{eq:max}
\underset{{{\bf v} \,:\, {
{\| {{\bf v}} \|}
} = 1}}{\operatorname{argmax}} \frac{1}{m} \sum_{i=1}^m 
{
{\langle {{\bf x}_i} , {{\bf v}} \rangle}
}^2 .\]</span></p>
<div class="figure">
<img src="cluster" alt="For the good line, the projection of the points onto the line keeps the two clusters separated, while the projection onto the bad line merges the two clusters." />
<p class="caption">For the good line, the projection of the points onto the line keeps the two clusters separated, while the projection onto the bad line merges the two clusters.<span data-label="f:cluster"></span></p>
</div>
<p>The objective function of maximizing variance is natural in its own right, and the fact that PCA’s objective function admits multiple interpretations builds confidence that it’s performing a fundamental operation on the data. For example, imagine the data points fall into two well-separated clusters; see Figure [f:cluster] for an illustration but imagine a high-dimensional version that can’t be so easily eyeballed. In this case, maximizing variance corresponds to preserving the separation between the two clusters post-projection. (With a poorly chosen line, the two clusters would project on top of one another, obscuring the structure in the data.)</p>
<p>PCA effectively assumes that variance in the data corresponds to interesting information. One can imagine scenarios where such variance corresponds to noise rather than signal, in which case PCA may not produce illuminating results. (See Section [s:fail] for more on PCA failure cases.)</p>
<h2 id="ss:largek">Larger Values of <span class="math inline">\(k\)</span></h2>
<p>The discussion so far focused on the <span class="math inline">\(k=1\)</span> case (fitting a line to the data), but the case of larger <span class="math inline">\(k\)</span> is almost the same. For general <span class="math inline">\(k\)</span>, the objective functions of minimizing the squares of distances to a <span class="math inline">\(k\)</span>-dimensional subspace and of maximizing the variance of the projections onto a <span class="math inline">\(k\)</span>-dimensional subspace are again equivalent. So the PCA objective is now <span class="math display">\[\label{eq:maxk}
\underset{{\text{$k$-dimensional subspaces $S$}}}{\operatorname{argmax}} \frac{1}{m} \sum_{i=1}^m 
(\text{length of ${\bf x}_i$&#39;s projection on $S$})^2.\]</span></p>
<p>To rephrase this objective in a more convenient form, recall the following basic linear algebra. First, vectors <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k\)</span> are <span><em>orthonormal</em></span> if they all have unit length (<span class="math inline">\({
{\| {{\bf v}_i} \|}
}_2 = 1\)</span> for all <span class="math inline">\(i\)</span>) and are orthogonal (<span class="math inline">\({
{\langle {{\bf v}_i} , {{\bf v}_j} \rangle}
} =
0\)</span> for all <span class="math inline">\(i \neq j\)</span>). For example, the standard basis vectors (of the form <span class="math inline">\((0,0,\ldots,0,1,0,\ldots,0)\)</span>) are orthonormal. Rotating these vectors gives more set of orthonormal vectors.</p>
<p>The <span><em>span</em></span> of a collection <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k \in
\mathbb{R}^n\)</span> of vectors is all of their linear combinations: <span class="math inline">\(\{ \sum_{j=1}^k
\lambda_j {\bf v}_j \,:\, \lambda_1,\ldots,\lambda_k \in \mathbb{R} \}\)</span>. If <span class="math inline">\(k=1\)</span>, then this span is a line through the origin; if <span class="math inline">\(k=2\)</span> and <span class="math inline">\({\bf v}_1,{\bf v}_2\)</span> are linearly independent (i.e., not multiples of each other) then the span is a plane (through the origin); and so on.</p>
<p>One nice fact about orthonormal vectors <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k\)</span> is that the squared projection length of a point onto the subspace spanned by the vectors is just the sum of the squares of its projections onto each of the vectors: <span class="math display">\[\label{eq:project}
(\text{length of ${\bf x}_i$&#39;s projection on
  $\mbox{span}({\bf v}_1,\ldots,{\bf v}_k)$})^2
=
\sum_{j=1}^k {
{\langle {{\bf x}_i} , {{\bf v}_j} \rangle}
}^2.\]</span> (For intuition, consider first the case where <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k\)</span> are all standard basis vectors, so <span class="math inline">\({
{\langle {{\bf x}_i} , {{\bf v}_j} \rangle}
}\)</span> just picks out one coordinate of <span class="math inline">\({\bf x}_i\)</span>). This identity is <span><em>not</em></span> true if the <span class="math inline">\({\bf v}_j\)</span>’s are not orthonormal (e.g., imagine if <span class="math inline">\(k=2\)</span> and <span class="math inline">\({\bf v}_1,{\bf v}_2\)</span> are linearly independent but nearly the same vector).</p>
<p>Combining  and , we can state the objective of PCA for general <span class="math inline">\(k\)</span> in its standard form: compute orthonormal vectors <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k\)</span> to maximize <span class="math display">\[\label{eq:largek}
\frac{1}{m} \sum_{i=1}^m \underbrace{\sum_{j=1}^k
  {
{\langle {{\bf x}_i} , {{\bf v}_j} \rangle}
}^2}_{\text{squared projection length}}.\]</span> The resulting <span class="math inline">\(k\)</span> orthonormal vectors are called the <span><em>top <span class="math inline">\(k\)</span> principal components</em></span> of the data.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p>To recap, the formal definition of the problem solved by PCA is:</p>
<ul>
<li><p><span><em>given <span class="math inline">\({\bf x}^1,\ldots,{\bf x}^m \in \mathbb{R}^n\)</span> and a parameter <span class="math inline">\(k \geq 1\)</span>, compute orthonormal vectors <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k \in \mathbb{R}^n\)</span> to maximize .</em></span></p></li>
</ul>
<p>Next lecture we discuss computational methods for finding the top <span class="math inline">\(k\)</span> principal components. We conclude this lecture with use cases and “non-use cases” (i.e., failure modes) of PCA. Given a black-box that computes principal components, what would you do with it?</p>
<h1 id="use-cases">Use Cases</h1>
<h2 id="data-visualization">Data Visualization</h2>
<h3 id="sss:recipe">A General Recipe</h3>
<p>PCA is commonly used to visualize data. In this use case, one typically takes <span class="math inline">\(k\)</span> to be 1, 2, or 3. Given data points <span class="math inline">\({\bf x}_1,\ldots,{\bf x}_m \in \mathbb{R}^n\)</span>, here is the approach:</p>
<ol>
<li><p>Perform PCA to get the top <span class="math inline">\(k\)</span> principal components <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k \in \mathbb{R}^n\)</span>.</p></li>
<li><p>For each data point <span class="math inline">\({\bf x}_i\)</span>, define its “<span class="math inline">\({\bf v}_1\)</span>-coordinate” as <span class="math inline">\({
{\langle {{\bf x}_i} , {{\bf v}_1} \rangle}
}\)</span>, its “<span class="math inline">\({\bf v}_2\)</span>-coordinate” as <span class="math inline">\({
{\langle {{\bf x}_i} , {{\bf v}_2} \rangle}
}\)</span>, and so on. This associates <span class="math inline">\(k\)</span> coordinates with each data point <span class="math inline">\({\bf x}_i\)</span>, according to the extent to which it points in the same direction as each of the top <span class="math inline">\(k\)</span> principal components. (So a large positive coordinate means that <span class="math inline">\({\bf x}_i\)</span> is close to the same direction as <span class="math inline">\({\bf v}_j\)</span>, while a large negative coordinate means that <span class="math inline">\({\bf x}_i\)</span> points in the opposite direction.)</p></li>
<li><p>Plot the point <span class="math inline">\({\bf x}_i\)</span> in <span class="math inline">\(\mathbb{R}^k\)</span> as the point <span class="math inline">\(({
{\langle {{\bf x}_i} , {{\bf v}_1} \rangle}
},\ldots,{
{\langle {{\bf x}_i} , {{\bf v}_k} \rangle}
})\)</span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p></li>
</ol>
<p>This is more or less what we did with our toy example in Figure [f:plot], although the scaling was different because our vectors <span class="math inline">\({\bf v}_1\)</span> and <span class="math inline">\({\bf v}_2\)</span> were not unit vectors.</p>
<p>What is the “meaning” of the point <span class="math inline">\(({
{\langle {{\bf x}_i} , {{\bf v}_1} \rangle}
},\ldots,{
{\langle {{\bf x}_i} , {{\bf v}_k} \rangle}
})\)</span>? Recall that one way to think about PCA is as a method for approximating a bunch of data points as linear combinations of the same <span class="math inline">\(k\)</span> vectors. PCA uses the vectors <span class="math inline">\({\bf v}_1,\ldots,{\bf v}_k\)</span> for this purpose, and the coordinates <span class="math inline">\(({
{\langle {{\bf x}_i} , {{\bf v}_1} \rangle}
},\ldots,{
{\langle {{\bf x}_i} , {{\bf v}_k} \rangle}
})\)</span> specify the linear combination of these vectors that most closely approximate <span class="math inline">\({\bf x}_i\)</span> (with respect to Euclidean distance). That is, PCA approximates each <span class="math inline">\({\bf x}_i\)</span> as <span class="math display">\[{\bf x}_i \approx \sum_{j=1}^k {
{\langle {{\bf x}_i} , {{\bf v}_j} \rangle}
}{\bf v}_j.\]</span></p>
<h3 id="interpreting-the-results">Interpreting the Results</h3>
<p>Both the <span class="math inline">\({\bf v}_j\)</span>’s and the projections of the <span class="math inline">\({\bf x}_i\)</span>’s onto them can be interesting. Here are some things to look at:</p>
<ol>
<li><p>Look at the data points with the largest (most positive) and smallest (most negative) projections <span class="math inline">\({
{\langle {{\bf x}_i} , {{\bf v}_1} \rangle}
}\)</span> on the first principal component. Does this suggest a potential “meaning” for the component? Are there data points clustered together at either of the two ends, and if so, what do these have in common?</p></li>
<li><p>Plot all points according to their <span class="math inline">\(k\)</span> coordinate values (i.e., projection lengths onto the top <span class="math inline">\(k\)</span> principal components). Do any interesting clusters pop out (e.g., with <span class="math inline">\(k=2\)</span>, in any of the four corners)? For example, by looking at pairs that have similar second coordinates — both pairs with similar first coordinates, and pairs with very different first coordinates — it is often possible to obtain a rough interpretation of the second principal component.</p></li>
<li><p>Looking at the coordinates of a principal component — the linear combination of the original data point attributes that it uses — can also be helpful. (E.g., see the Eigenfaces application below.)</p></li>
</ol>
<h3 id="sss:europe">Do Genomes Encode Geography?</h3>
<p>Can you infer where someone grew up from their DNA? In a remarkable study, Novembre et al. <span class="citation"></span> used PCA to show that in some cases the answer is “yes.” This is a case study in how PCA can reveal that the data “knows” things that you wouldn’t expect.</p>
<p>Novembre et al. <span class="citation"></span> considered a data set comprising 1387 Europeans (the rows). Each person’s genome was examined in the same 200,000 “SNPs” (the columns) — positions that tend to exhibit gene mutation.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> So in one of these positions, maybe 90% of all people have a “C,” while 10% have an “A.” In a nutshell, Novembre et al. <span class="citation"></span> apply PCA to this data set (with <span class="math inline">\(m \approx 1400\)</span>, <span class="math inline">\(n
\approx 200,000\)</span>, and <span class="math inline">\(k=2\)</span>) and plot the data according to the top two principal components <span class="math inline">\({\bf v}_1\)</span> and <span class="math inline">\({\bf v}_2\)</span> (using the exact same recipe as in Section [sss:recipe], with the <span class="math inline">\(x\)</span>- and <span class="math inline">\(y\)</span>-axes corresponding to <span class="math inline">\({\bf v}_1\)</span> and <span class="math inline">\({\bf v}_2\)</span>).<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>We emphasize that this plot depends on genomic data <span><em>only</em></span>. To interpret it, the authors then color-coded each plotted point according to the country of origin of the corresponding person (this information was available in the meta-data). Check out the resulting plot in <span class="citation"></span>, available from the course Web page — it’s essentially a map of Europe! The first principal component <span class="math inline">\({\bf v}_1\)</span> corresponds roughly to the latitude of where someone’s from, and <span class="math inline">\({\bf v}_2\)</span> to the longitude (rotated <span class="math inline">\(16^{\circ}\)</span> counter-clockwise). That is, <span><em>their genome “knows” where they’re from!</em></span> This suggests that many Europeans have bred locally for long enough for geographic information to be reflected in their DNA.</p>
<p>Is this conclusion obvious in hindsight? Not really. For example, if you repeat the experiment using U.S. data, the top principal components correspond more to waves of immigration than to anything geographic. So in this case the top principal components reflect temporal rather than spatial effects.</p>
<p>There are competing mathematical models for genetic variation. Some of these involve spatial decay, while others are based on multiple discrete and well-differentiated groups. The study of Novembre et al. <span class="citation"></span> suggests that the former models are much more appropriate for Europeans.</p>
<h2 id="data-compression">Data Compression</h2>
<p>One famous “killer application” of PCA in computer science is the Eigenfaces project <span class="citation"></span>. Here, the data points are a bunch of images of faces — all framed in the same way, under the same lighting conditions. Thus <span class="math inline">\(n\)</span> is the number of pixels (around 65K) and each dimension encodes the intensity of one pixel. It turns out that using only the top 100–150 principal components is enough to represent almost all of the images with high accuracy — far less than the 65K needed for exactly representing all of the images.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> Each of these principal components can be viewed as an image and in many cases interpreted directly. (By contrast, in the previous example, we couldn’t interpret the top two principal components by inspection — we needed to plot the data points first.) For example, one principal component may indicate the presence or absence of glasses, another the presence or absence of a mustache, and so on. (See the figures in <span class="citation"></span>, available from the course Web site.) These techniques have been used in face recognition, which boils down to a nearest-neighbor-type computation in the low-dimensional space spanned by the top principal components. There have of course been lots of advances in face recognition since the 1991 publication of <span class="citation"></span>, but PCA remains a key building block in modern approaches to the problem.</p>
<h1 id="s:fail">Failure Cases</h1>
<p>When and why does PCA fail?</p>
<ol>
<li><p>You messed up the scaling/normalizing. PCA is sensitive to different scalings/normalizations of the coordinates. Much of the artistry of getting good results from PCA involves choosing an appropriate scaling for the different data coordinates, and perhaps additional preprocessing of the data (like outlier removal).</p></li>
<li><p>Non-linear structure. PCA is all about finding linear structure in your data. If your data has some low-dimensional, but non-linear structure, e.g., you have two data coordinates, <span class="math inline">\(x,y\)</span>, with <span class="math inline">\(y \approx x^2\)</span>, then PCA will not find this.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> Similarly for GPS data from someone on a ferris wheel (Figure [f:ferris]) — the data is one dimensional (each point can be specified by an angle) but this dimension will not be identified by PCA.</p>
<div class="figure">
<img src="ferris" alt="GPS data from a ferris wheel. PCA is not designed to discover nonlinear structure." />
<p class="caption">GPS data from a ferris wheel. PCA is not designed to discover nonlinear structure.<span data-label="f:ferris"></span></p>
</div></li>
<li><p>Non-orthogonal structure. It is nice to have coordinates that are interpretable, like in our toy example (Section [s:toy]) and our Europe example (Section [sss:europe]). Even if your data is essentially linear, the fact that the principal components are all orthogonal will often mean that after the top few components, it will be almost impossible to interpret the meanings of the components. In the Europe example, the third and later components are forced to orthogonal to the latitude and longitude components. This is a rather artificial constraint to put on an interpretable feature.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Little-known fact: pop tarts — the unfrosted kinds, at least — are inadvertently vegan.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Conversely, if we knew which friends were vegetarian and which were not, we could deduce which foods are most likely to be vegetarian.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Representing, for example: images (dimensions = pixels); measurements (dimensions = sensors); documents (dimensions = words); and so on.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>The notation <span class="math inline">\({
{\langle {{\bf x}} , {{\bf y}} \rangle}
}\)</span> indicates the inner (or dot) product <span class="math inline">\(\sum_{i=1}^n x_iy_i\)</span> of the two vectors.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>It should be geometrically clear, already in two dimensions, that stretching out one coordinate affects the best line through the points. See also Figure [f:line].<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>In the toy example, the two vectors <span class="math inline">\((3,-3,-3,3)\)</span> and <span class="math inline">\((1,-1,1,-1)\)</span>, once normalized to be unit vectors, are close to the top two principal components. The actual principal components have messier numbers so we don’t report them here. Try computing them with one line of Matlab code!<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Alternatively, it can sometimes be illuminating to plot points using a subset of the top <span class="math inline">\(k\)</span> principal components — the first and third components, say.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>You’ll work with a similar data set on Mini-Project #4.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>As usual with PCA, for best results non-trivial preprocessing of the data was necessary.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>We’ll see next lecture that principal components are in fact eigenvectors of a suitable matrix — hence the term “Eigenfaces.”<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>As mentioned in the context of linear regression last lecture, you can always add extra non-linear dimensions to your data, like coordinates <span class="math inline">\(x^2, y^2, xy,\)</span> etc., and then PCA this higher-dimensional data. This idea doesn’t always work well with PCA, however.<a href="#fnref11">↩</a></p></li>
</ol>
</div>
{% endraw %}
