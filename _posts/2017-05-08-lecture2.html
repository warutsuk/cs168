---
layout: post
title: Lecture 2
author: Tim
---
{% raw %}
<h1 id="the-heavy-hitters-problem">The Heavy Hitters Problem</h1>
<h2 id="ss:majority">Finding the Majority Element</h2>
<p>Let’s begin with a problem that many of you have seen before. It’s a common question in technical interviews. You’re given as input an array <span class="math inline">\(A\)</span> of length <span class="math inline">\(n\)</span>, with the promise that it has a <span> <em>majority element</em></span> — a value that is repeated in strictly more than <span class="math inline">\(n/2\)</span> of the array’s entries. Your task is to find the majority element.</p>
<p>In algorithm design, the usual “holy grail” is a linear-time algorithm. For this problem, your post-CS161 toolbox already contains a subroutine that gives a linear-time solution — just compute the median of <span class="math inline">\(A\)</span>. (Note: it must be the majority element.) So let’s be more ambitious: can we compute the majority element with a single left-to-right pass through the array? If you haven’t seen it before, here’s the solution:</p>
<ul>
<li><p>Initialize counter := 0, current := NULL.</p>
<p>[current stores the frontrunner for the majority element]</p></li>
<li><p>For <span class="math inline">\(i=1\)</span> to <span class="math inline">\(n\)</span>:</p>
<ul>
<li><p>If counter == 0:</p>
<p>[In this case, there is no frontrunner.]</p>
<ul>
<li><p>current := A[i]</p></li>
<li><p>counter++</p></li>
</ul></li>
<li><p>else if A[i]==current:</p>
<p>[In this case, our confidence in the current frontrunner goes up.]</p>
<ul>
<li><p>counter++</p></li>
</ul></li>
<li><p>else</p>
<p>[In this case, our confidence in the current frontrunner goes down.]</p>
<ul>
<li><p>counter- -</p></li>
</ul></li>
</ul></li>
<li><p>Return current</p></li>
</ul>
<p>For example, suppose the input is the array <span class="math inline">\(\{2,1,1\}\)</span>. The first iteration of the algorithm makes “2” the current guess of the majority element, and sets the counter to 1. The next element decreases the counter back to 0 (since <span class="math inline">\(1 \neq 2\)</span>). The final iteration resets the current guess to “1” (with counter value 1), which is indeed the majority element.</p>
<p>More generally, the algorithm correctly computes the majority element of any array that possesses one. We encourage you to formalize a proof of this statement (e.g., by induction on <span class="math inline">\(n\)</span>). The intuition is that each entry of <span class="math inline">\(A\)</span> that contains a non-majority-value can only “cancel out” one copy of the majority value. Since more than <span class="math inline">\(n/2\)</span> of the entries of <span class="math inline">\(A\)</span> contain the majority value, there is guaranteed to be a copy of it left standing at the end of the algorithm.</p>
<p>But so what? It’s a cute algorithm, but isn’t this just a toy problem? It is, but modest generalizations of the problem are quite close to problems that people really want to solve in practice.</p>
<h2 id="ss:hh">The Heavy Hitters Problem</h2>
<p>In the <span><em>heavy hitters</em></span> problem, the input is an array <span class="math inline">\(A\)</span> of length <span class="math inline">\(n\)</span>, and also a parameter <span class="math inline">\(k\)</span>. You should think of <span class="math inline">\(n\)</span> as very large (in the hundreds of millions, or billions), and <span class="math inline">\(k\)</span> as modest (10, 100, or 1000). The goal is to compute the values that occur in the array at least <span class="math inline">\(n/k\)</span> times.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Note that there can be at most <span class="math inline">\(k\)</span> such values; and there might be none. The problem of computing the majority element corresponds to the heavy hitters problem with <span class="math inline">\(k \approx 2-\delta\)</span> for a small value <span class="math inline">\(\delta &gt; 0\)</span>, and with the additional promise that a majority element exists.</p>
<p>The heavy hitters problem has lots of applications, as you can imagine. We’ll be more specific later when we discuss a concrete solution, but here are some high-level examples:<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<ol>
<li><p>Computing popular products. For example, <span class="math inline">\(A\)</span> could be all of the page views of products on yesterday. The heavy hitters are then the most frequently viewed products.</p></li>
<li><p>Computing frequent search queries. For example, <span class="math inline">\(A\)</span> could be all of the searches on Google yesterday. The heavy hitters are then searches made most often.</p></li>
<li><p>Identifying heavy TCP flows. Here, <span class="math inline">\(A\)</span> is a list of data packets passing through a network switch, each annotated with a source-destination pair of IP addresses. The heavy hitters are then the flows that are sending the most traffic. This is useful for, among other things, identifying denial-of-service attacks.</p></li>
<li><p>Identifying volatile stocks. Here, <span class="math inline">\(A\)</span> is a list of stock trades.</p></li>
</ol>
<p>It’s easy to think of more. Clearly, it would be nice to have a good algorithm for the heavy hitters problem at your disposal for data analysis.</p>
<p>The problem is easy to solve efficiently if <span class="math inline">\(A\)</span> is readily available in main memory — just sort the array and do a linear scan over the result, outputting a value if and only if it occurs (consecutively) at least <span class="math inline">\(n/k\)</span> times. After being spoiled by our slick solution for finding a majority element, we naturally want to do better. Can we solve the heavy hitters problem with a single pass over the array? This question isn’t posed quite correctly, since it allows us to cheat: we could make a single pass over the array, make a local copy of it in our working memory, and then apply the sorting-based solution to our local copy. Thus what we mean is: can we solve the Heavy Hitters problem with a single pass over the array, using only a small amount of auxiliary space?<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<h2 id="an-impossibility-result">An Impossibility Result</h2>
<p>The following fact might surprise you.</p>
<p>[fact:hh] There is <span><em>no</em></span> algorithm that solves the Heavy Hitters problems in one pass while using a sublinear amount of auxiliary space.</p>
<p>We next explain the intuition behind Fact [fact:hh]. We encourage you to devise a formal proof, which follows the same lines as the intuition.</p>
<p>Set <span class="math inline">\(k = n/2\)</span>, so that our responsibility is to output any values that occur at least twice in the input array <span class="math inline">\(A\)</span>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> Suppose <span class="math inline">\(A\)</span> has the form <span class="math display">\[\underbrace{|x_1|x_2|x_3|\cdots|x_{n-1}|}_{\text{set $S$ of distinct
    elements}}y|,\]</span> where <span class="math inline">\(x_1,\ldots,x_{n-1}\)</span> are an arbitrary set <span class="math inline">\(S\)</span> of distinct elements (in <span class="math inline">\(\{1,2,\ldots,n^2\}\)</span>, say) and the final entry <span class="math inline">\(y\)</span> may or may not be in <span class="math inline">\(S\)</span>. By definition, we need to output <span class="math inline">\(y\)</span> if and only if <span class="math inline">\(y \in S\)</span>. That is, <span><em>answering membership queries reduces to solving the Heavy Hitters problem.</em></span> By the “membership problem,” we mean the task of preprocessing a set <span class="math inline">\(S\)</span> to answer queries of the form “is <span class="math inline">\(y \in S\)</span>”? (A hash table is the most common solution to this problem.) It is intuitive that you cannot correctly answer all membership queries for a set <span class="math inline">\(S\)</span> without storing <span class="math inline">\(S\)</span> (thereby using linear, rather than constant, space) — if you throw some of <span class="math inline">\(S\)</span> out, you might get a query asking about the part you threw out, and you won’t know the answer. It’s not too hard to make this idea precise using the Pigeonhole Principle.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<h2 id="ss:epshh">The Approximate Heavy Hitters Problem</h2>
<p>What should we make of Fact [fact:hh]? Should we go home with our tail between our legs? Of course not — the applications that motivate the heavy hitters problem are not going away, and we still want to come up with non-trivial algorithms for them. In light of Fact [fact:hh], the best-case scenario would be to find a relaxation of the problem that remains relevant for the motivating applications and also admits a good solution.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p>In the <span><em><span class="math inline">\(\eps\)</span>-approximate heavy hitters (<span class="math inline">\(\eps\)</span>-HH) problem</em></span>, the input is an array <span class="math inline">\(A\)</span> of length <span class="math inline">\(n\)</span> and user-defined parameters <span class="math inline">\(k\)</span> and <span class="math inline">\(\eps\)</span>. The responsibility of an algorithm is to output a list of values such that:</p>
<ol>
<li><p>Every value that occurs at least <span class="math inline">\(\frac{n}{k}\)</span> times in <span class="math inline">\(A\)</span> is in the list.</p></li>
<li><p>Every value in the list occurs at least <span class="math inline">\(\frac{n}{k} -\eps n\)</span> times in <span class="math inline">\(A\)</span>.</p></li>
</ol>
<p>What prevents us from taking <span class="math inline">\(\eps = 0\)</span> and solving the exact version of the problem? We allow the space used by a solution to grow as <span class="math inline">\(\tfrac{1}{\eps}\)</span>, so as <span class="math inline">\(\eps \downarrow 0\)</span> the space blows up (as is necessary, by Fact [fact:hh]).</p>
<p>For example, suppose we take <span class="math inline">\(\eps = \tfrac{1}{2k}\)</span>. Then, the algorithm outputs every value with frequency count at least <span class="math inline">\(\tfrac{n}{k}\)</span>, and only values with frequency count at least <span class="math inline">\(\tfrac{n}{2k}\)</span>. Thinking back to the motivating examples in Section [ss:hh], such an approximate solution is essentially as useful as an exact solution. Space usage <span class="math inline">\(O(\tfrac{1}{\eps}) = O(k)\)</span> is also totally palatable; after all, the output of the heavy hitters or <span class="math inline">\(\eps\)</span>-HH problem already might be as large as <span class="math inline">\(k\)</span> elements.</p>
<h1 id="the-count-min-sketch">The Count-Min Sketch</h1>
<h2 id="discussion">Discussion</h2>
<p>This section presents an elegant small-space data structure, the <span> <em>count-min sketch</em></span> <span class="citation"></span>, that can be used to solve the <span class="math inline">\(\eps\)</span>-HH problem. There are also several other good solutions to the problem, including some natural “counter-based” algorithms that extend the algorithm in Section [ss:majority] for computing a majority element <span class="citation"></span>. We focus on the count-min sketch for a number of reasons.</p>
<ol>
<li><p>It has been implemented in real systems. For example, AT<span>&amp;</span>T has used it in network switches to perform analyses on network traffic using limited memory <span class="citation"></span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> At Google, a precursor of the count-min sketch (called the “count sketch” <span class="citation"></span>) has been implemented on top of their MapReduce parallel processing infrastructure <span class="citation"></span>. One of the original motivations for this primitive was log analysis (e.g., of source code check-ins), but presumably it is now used for lots of different analyses.</p></li>
<li><p>The data structure is based on hashing, and as such fits in well with the current course theme.</p></li>
<li><p>The data structure introduces a new theme, present in many of the next few lectures, of “lossy compression.” The goal here is to throw out as much of your data as possible while still being able to make accurate inferences about it. What you want to keep depends on the type of inference you want to support. For approximately preserving frequency counts, the count-min sketch shows that you can throw out almost all of your data!</p></li>
</ol>
<p>We’ll only discuss how to use the count-min sketch to solve the approximate heavy hitters problem, but it is also useful for other related tasks (see <span class="citation"></span> for a start). Another reason for its current popularity is that its computations parallelize easily — as we discuss its implementation, you might want to think about this point.</p>
<h2 id="a-role-model-the-bloom-filter">A Role Model: The Bloom Filter</h2>
<p>This section briefly reviews the bloom filter data structure, which is a role model for the count-min sketch. No worries if you haven’t seen bloom filters before; our treatment of the count-min sketch below is self-contained. There are also review videos covering the details of bloom filters on the course Web site.</p>
<p>The raison d’être of a bloom filter is to solve the <span><em>membership problem</em></span>. The client can insert elements into the bloom filter and the data structure is responsible for remembering what’s been inserted. The bloom filter doesn’t do much, but what it does it does very well.</p>
<p>Hash tables also offer a good solution to the membership problem, so why bother with a bloom filter? The primary motivation is to save space — a bloom filter compresses the stored set more than a hash table. In fact, the compression is so extreme that a bloom filter cannot possibly answer all membership queries correctly. That’s right, it’s a <span><em>data structure that makes errors.</em></span> Its errors are “one-sided,” with no false negatives (so if you inserted an element, the bloom filter will always confirm it) but with some false positives (so there are “phantom elements” that the data structure claims are present, even though they were never inserted). For instance, using 8 bits per stored element — well less than the space required for a pointer, for example — bloom filters can achieve a false positive probability less than 2%. More generally, bloom filters offer a smooth trade-off between the space used and the false positive probability. Both the insertion and lookup operations are super-fast (<span class="math inline">\(O(1)\)</span> time) in a bloom filter, and what little work there is can also be parallelized easily.</p>
<p>Bloom filters were invented in 1970 <span class="citation"></span>, back when space was at a premium for everything, even spellcheckers.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> This century, bloom filters have gone viral in the computer networking community <span class="citation"></span>. Saving space is still a big win in many networking applications, for example by making better use of the scarce main memory at a router or by reducing the amount of communication required to implement a network protocol.</p>
<p>Bloom filters serve as a role model for the count-min sketch in two senses. First, bloom filters offer a proof of concept that sacrificing a little correctness can yield significant space savings. Note this is exactly the trade-off we’re after: Fact [fact:hh] states that exactly solving the heavy hitters problem requires linear space, and we’re hoping that by relaxing correctness — i.e., solving the <span class="math inline">\(\eps\)</span>-HH problem instead — we can use far less space. Second, at a technical level, if you remember how bloom filters are implemented, you’ll recognize the count-min sketch implementation as a bird of the same feather.</p>
<h2 id="count-min-sketch-implementation">Count-Min Sketch: Implementation</h2>
<p>The count-min-sketch supports two operations: (<span class="math inline">\(x\)</span>) and (<span class="math inline">\(x\)</span>).<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> The operation (<span class="math inline">\(x\)</span>) is supposed to return the <span><em>frequency count</em></span> of <span class="math inline">\(x\)</span>, meaning the number of times that (<span class="math inline">\(x\)</span>) has been invoked in the past.</p>
<p>The count-min sketch has two parameters, the number of buckets <span class="math inline">\(b\)</span> and the number of hash functions <span class="math inline">\(\ell\)</span>. We’ll figure out how to choose these parameters in Section [ss:error], but for now you might want to think of <span class="math inline">\(b\)</span> as in the thousands and of <span class="math inline">\(\ell\)</span> as 5.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> The point of <span class="math inline">\(b\)</span> is to compress the array <span class="math inline">\(A\)</span> (since <span class="math inline">\(b \ll n\)</span>). This compression leads to errors. The point of <span class="math inline">\(\ell\)</span> is to implement a few “independent trials,” which allows us to reduce the error. What’s important, and kind of amazing, is that these parameters are <span><em>independent</em></span> of the length <span class="math inline">\(n\)</span> of the array that we are processing (recall <span class="math inline">\(n\)</span> might be in the billions, or even larger).</p>
<div class="figure">
<embed src="cms.pdf" />
<p class="caption">Running (<span class="math inline">\(x\)</span>) on the CMS data structure. Each row corresponds to a hash function <span class="math inline">\(h_i\)</span>.<span data-label="f:cms"></span></p>
</div>
<p>The data structure is just a <span class="math inline">\(\ell \times b\)</span> 2-D array CMS of counters (initially all 0). See Figure [f:cms]. After choosing <span class="math inline">\(\ell\)</span> hash functions <span class="math inline">\(h_1,\ldots,h_{\ell}\)</span>, each mapping the universe of objects to <span class="math inline">\(\{1,2,\ldots,b\}\)</span>, the code for (<span class="math inline">\(x\)</span>) is simply:</p>
<ul>
<li><p>for <span class="math inline">\(i=1,2,\ldots,\ell\)</span>:</p>
<ul>
<li><p>increment CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>]</p></li>
</ul></li>
</ul>
<p>Assuming that every hash function can be evaluated in constant time, the running time of the operation is clearly <span class="math inline">\(O(\ell)\)</span>.</p>
<p>To motivate the implementation of (<span class="math inline">\(x\)</span>), fix a row <span class="math inline">\(i \in \{1,2,\ldots,\ell\}\)</span>. Every time (<span class="math inline">\(x\)</span>) is called, the same counter CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>] in this row gets incremented. Since counters are never decremented, we certainly have <span class="math display">\[\label{eq:inc}
\text{CMS[$i$][$h_i(x)$]} \ge f_x,\]</span> where <span class="math inline">\(f_x\)</span> denotes the frequency count of object <span class="math inline">\(x\)</span>. If we’re lucky, then equality holds in . In general, however, there will be <span><em>collisions</em></span>: objects <span class="math inline">\(y \neq x\)</span> with <span class="math inline">\(h_i(y) =
h_i(x)\)</span>. (Note with <span class="math inline">\(b \ll n\)</span>, there will be lots of collisions.) Whenever (<span class="math inline">\(y\)</span>) is called for an object <span class="math inline">\(y\)</span> that collides with <span class="math inline">\(x\)</span> in row <span class="math inline">\(i\)</span>, this will also increment the same counter CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>]. So while CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>] cannot underestimate <span class="math inline">\(f_x\)</span>, it generally overestimates <span class="math inline">\(f_x\)</span>.</p>
<p>The <span class="math inline">\(\ell\)</span> rows of the count-min sketch give <span class="math inline">\(\ell\)</span> different estimates of <span class="math inline">\(f_x\)</span>. How should we aggregate these estimates? Later in the course, we’ll see scenarios where using the mean or the median is a good way to aggregate. Here, our estimates suffer only one-sided error — all of them can only be bigger than the number <span class="math inline">\(f_x\)</span> we want to estimate, and so it’s a no-brainer which estimate we should pay attention to. The <span><em>smallest</em></span> of the estimates is clearly the best estimate. Thus, the code for (<span class="math inline">\(x\)</span>) is simply:</p>
<ul>
<li><p>return <span class="math inline">\(\min_{i=1}^{\ell} \text{CMS[$i$][$h_i(x)$]}\)</span></p></li>
</ul>
<p>The running time is again <span class="math inline">\(O(\ell)\)</span>. By , the data structure has one-sided error — it only returns overestimates of true frequency counts, never underestimates. The key question is obviously: <span><em>how large are typical overestimates?</em></span> The answer depends on how we set the parameters <span class="math inline">\(b\)</span> and <span class="math inline">\(\ell\)</span>. As <span class="math inline">\(b\)</span> gets bigger, we’ll have fewer collisions and hence less error. As <span class="math inline">\(\ell\)</span> gets bigger, we’ll take the minimum over more independent estimates, resulting in tighter estimates. Thus the question is whether or not modest values of <span class="math inline">\(b\)</span> and <span class="math inline">\(\ell\)</span> are sufficient to guarantee that the overestimates are small. This is a quantitative question that can only be answered with mathematical analysis; we do this in the next section (and the answer is yes!).</p>
<p>The implementation details of the count-min sketch are very similar to those of a bloom filter. The latter structure only uses bits, rather than integer-valued counters. When an object is inserted into a bloom filter, <span class="math inline">\(\ell\)</span> hash functions indicate <span class="math inline">\(\ell\)</span> bits that should be set to 1 (whether or not they were previously 0 or  1). The count-min sketch, which is responsible for keeping counts rather than just tracking membership, instead increments <span class="math inline">\(\ell\)</span> counters. Looking up an object in a bloom filter just involves checking the <span class="math inline">\(\ell\)</span> bits corresponding to that object — if any of them are still 0, then the object has not been previously inserted. Thus Lookup in a bloom filter can be thought of as taking the minimum of <span class="math inline">\(\ell\)</span> bits, which exactly parallels the  operation of a count-min-sketch. That the count-min sketch only overestimates frequency counts corresponds to the bloom filter’s property that it only suffers from false positives.</p>
<h2 id="ss:error_heur">Count-Min Sketch: Heuristic Error Analysis</h2>
<p>The goal of this section is to analyze how much a count-min sketch overestimates frequency counts, as a function of the parameters <span class="math inline">\(b\)</span> and <span class="math inline">\(\ell\)</span>. Once we understand the relationship between the error and these parameters, we can set the parameters to guarantee simultaneously small space and low error.</p>
<p>Fix an object <span class="math inline">\(x\)</span>. Let’s first think about a single row <span class="math inline">\(i\)</span> of the count-min sketch; we’ll worry about taking the minimum over rows later. After a bunch of (<span class="math inline">\(x\)</span>) operations have been executed, what’s the final value of CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>], row <span class="math inline">\(i\)</span>’s estimate for the frequency count of <span class="math inline">\(x\)</span>?</p>
<p>If we’re lucky and no other objects collide with <span class="math inline">\(x\)</span> in the <span class="math inline">\(i\)</span>th row, then CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>] is just the true frequency count <span class="math inline">\(f_x\)</span> of <span class="math inline">\(x\)</span>. If we’re unlucky and some object <span class="math inline">\(y\)</span> collides with <span class="math inline">\(x\)</span> in the <span class="math inline">\(i\)</span>th row, then <span class="math inline">\(y\)</span> contributes its own frequency count <span class="math inline">\(f_y\)</span> to CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>]. More generally, CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>] is the sum of the contributions to this counter by <span class="math inline">\(x\)</span> and all other objects that collide with it: <span class="math display">\[\label{eq:z_heur}
\text{CMS[$i$][$h_i(x)$]} = f_x + \sum_{y \in S} f_y,\]</span> where <span class="math inline">\(S = \{ y \neq x \,:\, h_i(y) = h_i(x) \}\)</span> denotes the objects that collide with <span class="math inline">\(x\)</span> in the <span class="math inline">\(i\)</span>th row. In , <span class="math inline">\(f_x\)</span> and the <span class="math inline">\(f_y\)</span>’s are fixed constants (independent of the choice of <span class="math inline">\(h_i\)</span>), while the set <span class="math inline">\(S\)</span> will be different for different choices of the hash function <span class="math inline">\(h_i\)</span>.</p>
<p>Recall that a good hash function spreads out a data set as well as if it were a random function. With <span class="math inline">\(b\)</span> buckets and a good hash function <span class="math inline">\(h_i\)</span>, we expect <span class="math inline">\(x\)</span> to collide with a roughly <span class="math inline">\(1/b\)</span> fraction of the other elements <span class="math inline">\(y \neq
x\)</span> under <span class="math inline">\(h_i\)</span>. Thus we expect <span class="math display">\[\label{eq:error_heur}
\text{CMS[$i$][$h_i(x)$]} = f_x + \frac{1}{b} \sum_{y \neq x} f_y \le
f_x + \frac{n}{b},\]</span> where in the inequality we use that the sum of the frequency counts is exactly the total number <span class="math inline">\(n\)</span> of increments (each increment adds 1 to exactly one frequency count). See also Section [ss:error] for a formal (non-heuristic) derivation of .</p>
<p>We should be pleased with . Recall the definition of the <span class="math inline">\(\eps\)</span>-approximate heavy hitters problem (Section [ss:epshh]): the goal is to identify objects with frequency count at least <span class="math inline">\(\tfrac{n}{k}\)</span>, without being fooled by any objects with frequency count less than <span class="math inline">\(\tfrac{n}{k} -\eps n\)</span>. This means we just need to estimate the frequency count of an object up to additive one-sided error <span class="math inline">\(\eps n\)</span>. If we take the number of buckets <span class="math inline">\(b\)</span> in the count-min sketch to be equal to <span class="math inline">\(\tfrac{1}{\eps}\)</span>, then  says the expected overestimate of a given object is at most <span class="math inline">\(\eps n\)</span>. Note that the value of <span class="math inline">\(b\)</span>, and hence the number of counters used by the data structure, is completely independent of <span class="math inline">\(n\)</span>! If you think of <span class="math inline">\(\eps = .001\)</span> and <span class="math inline">\(n\)</span> as in the billions, then this is pretty great.</p>
<p>So why aren’t we done? We’d like to say that, in addition to the expected overestimate of a frequency count being small, with very large probability the overestimate of a frequency count is small. (For a role model, recall that typical bloom filters guarantee a false positive probability of 1-2%.) This requires translating our bound on an expectation to a bound on a probability.</p>
<p>Next, we observe that  implies that the probability that a row’s overestimate of <span class="math inline">\(x\)</span> is more than <span class="math inline">\(\tfrac{2n}{b}\)</span> is less than 50%. (If not, the expected overestimate would be greater than <span class="math inline">\(\tfrac{1}{2} \cdot \tfrac{2n}{b} =
\tfrac{n}{b}\)</span>, contradicting .) This argument is a special case of “Markov’s inequality;” see Section [ss:error] for details.</p>
<p>A possibly confusing point in this heuristic analysis is: in the observation above, what is the probability over, exactly? I.e., where is the randomness? There are two morally equivalent interpretations of the analysis in this section. The first, which is carried out formally and in detail in Section [ss:error], is to assume that the hash function <span class="math inline">\(h_i\)</span> is chosen uniformly at random from a universal family of hash functions (see CS161 for the definition). The second is to assume that the hash function <span class="math inline">\(h_i\)</span> is fixed and that the data is random. If <span class="math inline">\(h_i\)</span> is a well-crafted hash function, then your particular data set will almost always behave like random data.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p>
<p>Remember that everything we’ve done so far is just for a single row <span class="math inline">\(i\)</span> of the hash table. The output of (<span class="math inline">\(x\)</span>) exceeds <span class="math inline">\(f_x\)</span> by more than <span class="math inline">\(\eps n\)</span> only if <span><em>every</em></span> row’s estimate is too big. Assuming that the hash functions <span class="math inline">\(h_i\)</span> are independent,<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> we have <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\min_{i=1}^{\ell} \text{CMS[$i$][$h_i(x)$]}
 &gt; f_x + \frac{2n}{b}\right]}
= \prod_{i=1}^{\ell} {\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\text{CMS[$i$][$h_i(x)$]}
 &gt; f_x + \frac{2n}{b}\right]}
  \le \left( \frac{1}{2} \right)^{\ell}.\]</span></p>
<p>To get an overestimate threshold of <span class="math inline">\(\eps n\)</span>, we can set <span class="math inline">\(b=\tfrac{2}{\eps}\)</span> (so e.g., 200 when <span class="math inline">\(\eps = .01\)</span>). To drive the error probability — that is, the probability of an overestimate larger than this threshold — down to the user-specified value <span class="math inline">\(\delta\)</span>, we set <span class="math display">\[\left( \frac{1}{2} \right)^{\ell} = \delta\]</span> and solve to obtain <span class="math inline">\(\ell = \log_2 \tfrac{1}{\delta}\)</span>. (This is between 6 and 7 when <span class="math inline">\(\delta = .01\)</span>.) Thus the total number of counters required when <span class="math inline">\(\delta = \eps = .01\)</span> is barely over a thousand (no matter how long the array is!). See Section [ss:reportcard] for a detailed recap of all of the count-min sketch’s properties, and Section [ss:error] for a rigorous and optimized version of the heuristic analysis in this section.</p>
<h2 id="ss:error">Count-Min Sketch: Rigorous Error Analysis</h2>
<p>This section carries out a rigorous version of the heuristic error analysis in Section [ss:error_heur]. Let <span class="math inline">\(f_x\)</span> denote the true frequency count of <span class="math inline">\(x\)</span>, and <span class="math inline">\(Z_i\)</span> the (over)estimate CMS[<span class="math inline">\(i\)</span>][<span class="math inline">\(h_i(x)\)</span>]. <span class="math inline">\(Z_i\)</span> is a random variable over the state space equal to the set of all possible hash functions <span class="math inline">\(h_i\)</span>. (I.e., given an <span class="math inline">\(h_i\)</span>, <span class="math inline">\(Z_i\)</span> is fully determined.)</p>
<p>If we’re lucky and no other objects collide with <span class="math inline">\(x\)</span> in the <span class="math inline">\(i\)</span>th row, then <span class="math inline">\(Z_i = f_x\)</span>. If we’re unlucky and some object <span class="math inline">\(y\)</span> collides with <span class="math inline">\(x\)</span> in the <span class="math inline">\(i\)</span>th row, then <span class="math inline">\(y\)</span> contributes its own frequency count <span class="math inline">\(f_y\)</span> to <span class="math inline">\(Z_i\)</span>. As in , we can write <span class="math display">\[\label{eq:z}
Z_i = f_x + \sum_{y \in S} f_y,\]</span> where <span class="math inline">\(S = \{ y \neq x \,:\, h_i(y) = h_i(x) \}\)</span> denotes the objects that collide with <span class="math inline">\(x\)</span> in the <span class="math inline">\(i\)</span>th row. In , <span class="math inline">\(f_x\)</span> and the <span class="math inline">\(f_y\)</span>’s are fixed constants (independent of the choice of <span class="math inline">\(h_i\)</span>), while the set <span class="math inline">\(S\)</span> is random (i.e., different for different choices of <span class="math inline">\(h_i\)</span>).</p>
<p>To continue the error analysis, we make the following assumption:</p>
<ul>
<li><p>For every pair <span class="math inline">\(x,y\)</span> of distinct objects, <span class="math inline">\({\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[h_i(y)
  = h_i(x)\right]} \le \tfrac{1}{b}\)</span>.</p></li>
</ul>
<p>Assumption (*) basically says that, after conditioning on the bucket to which <span class="math inline">\(h_i\)</span> assigns an object <span class="math inline">\(x\)</span>, the bucket <span class="math inline">\(h_i\)</span> assigns to some other object <span class="math inline">\(y\)</span> is uniformly random. For example, the assumption would certainly be satisfied if <span class="math inline">\(h_i\)</span> is a completely random function. It is also satisfied if <span class="math inline">\(h_i\)</span> is chosen uniformly at random from a universal family — it is precisely the definition of such a family (see your CS161 notes).</p>
<p>Before using assumption (*) to analyze , we recall <span> <em>linearity of expectation</em></span>: for any real-valued random variables <span class="math inline">\(X_1,\ldots,X_m\)</span> defined on the same probability space, <span class="math display">\[\label{eq:linexp}
{\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\sum_{j=1}^m X_j\right]} = \sum_{j=1}^m {\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X_j\right]}.\]</span> That is, the expectation of a sum is just the sum of the expectations, <span><em>even if the random variables are not independent</em></span>.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> The statement is trivial to prove — just expand the expectations and reverse the order of summation — and insanely useful.</p>
<p>To put the pieces together, we first rewrite  as <span class="math display">\[\label{eq:z2}
Z_i = f_x + \sum_{y \neq x} f_y\one_y,\]</span> where <span class="math inline">\(\one_y\)</span> is the indicator random variable that indicates whether or not <span class="math inline">\(y\)</span> collides with <span class="math inline">\(x\)</span> under <span class="math inline">\(h_i\)</span>: <span class="math display">\[\one_y = \left\{ \begin{array}{cl}
1 &amp; \text{if $h_i(y) = h_i(x)$}\\
0 &amp; \text{otherwise.}
\end{array}\right.\]</span> Recalling that <span class="math inline">\(f_x\)</span> and the <span class="math inline">\(f_y\)</span>’s are constants, we can apply linearity of expectation to  to obtain <span class="math display">\[\label{eq:z3}
{\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[Z_i\right]} = f_x + \sum_{y \neq x} f_y \cdot {\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\one_y\right]}.\]</span> As indicator random variables, the <span class="math inline">\(\one_y\)</span>’s have very simple expectations: <span class="math display">\[\label{eq:error}
{\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\one_y\right]} =
1 \cdot \underbrace{{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\one_y = 1\right]}}_{={\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[h_i(y)=h_i(x)\right]}} +
\underbrace{0 \cdot {\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\one_y =
    0\right]}}_{=0}
= {\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[h_i(y)=h_i(x)\right]} \stackrel{(*)}{\le} \frac{1}{b}.\]</span> Combining  and  gives <span class="math display">\[\label{eq:error2}
{\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[Z_i\right]} \le f_x + \frac{1}{b} \sum_{y \neq x} f_y
\le f_x + \frac{n}{b}.\]</span></p>
<p>Next we translate this bound on an expectation to a bound on a probability. A simple and standard way to do this is via <span><em>Markov’s inequality.</em></span></p>
<p>[prop:markov] If <span class="math inline">\(X\)</span> is a nonnegative random variable and <span class="math inline">\(c &gt; 1\)</span> is a constant, then <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X &gt; c \cdot {\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X\right]}\right]} \le \frac{1}{c}.\]</span></p>
<p>The proof of Markov’s inequality is simple. For example, suppose you have a nonnegative random variable <span class="math inline">\(X\)</span> with expected value 10. How frequently could it take on a value greater than 100? (So <span class="math inline">\(c=10\)</span>.) In principle, it is possible that <span class="math inline">\(X\)</span> has value exactly 100 10% of the time (if it has value 0 the rest of the time). But it can’t have value strictly greater than 100 10% or more of the time — if it did, its expectation would be strictly greater than 10. An analogous argument applies to nonnegative random variables with any expectation and for any value of <span class="math inline">\(c\)</span>.</p>
<p>Let’s return to our error analysis, for a fixed object <span class="math inline">\(x\)</span> and row <span class="math inline">\(i\)</span>. Define <span class="math display">\[X = Z_i - f_x \ge 0\]</span> as the amount by which the <span class="math inline">\(i\)</span>th row of the count-min sketch overestimates <span class="math inline">\(x\)</span>’s frequency count <span class="math inline">\(f_x\)</span>. By , with <span class="math inline">\(b = \tfrac{e}{\eps}\)</span>, the expected value of <span class="math inline">\(X\)</span> is at most <span class="math inline">\(\tfrac{\eps n}{e}\)</span>.<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> Since this overestimate is always nonnegative, we can apply Markov’s inequality (Proposition [prop:markov]) with <span class="math inline">\({\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X\right]} =
\tfrac{\eps n}{e}\)</span> and <span class="math inline">\(c = e\)</span> to obtain <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X &gt; e \cdot \tfrac{\eps n}{e}\right]} \le \frac{1}{e}\]</span> and hence <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[Z_i &gt; f_x + \eps n\right]} \le \frac{1}{e}.\]</span></p>
<p>Assuming that the hash functions are chosen independently, we have <span class="math display">\[\label{eq:error3}
{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\min_{i=1}^{\ell} Z_i &gt; f_x + \eps n\right]}
=
\prod_{i=1}^{\ell} {\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[Z_i &gt; f_x + \eps n\right]}
\le \frac{1}{e^{\ell}}.\]</span> To achieve a target error probability of <span class="math inline">\(\delta\)</span>, we just solve for <span class="math inline">\(\ell\)</span> in  and find that <span class="math inline">\(\ell \ge \ln
\tfrac{1}{\delta}\)</span> rows are sufficient. For <span class="math inline">\(\delta\)</span> around 1%, <span class="math inline">\(\ell = 5\)</span> is good enough.</p>
<h2 id="ss:reportcard">Count-Min Sketch: Final Report Card</h2>
<ul>
<li><p>The space required is that for <span class="math inline">\(\tfrac{e}{\eps} \ln
  \tfrac{1}{\delta}\)</span> counters. Recall from Section [ss:epshh] that for the <span class="math inline">\(\eps\)</span>-HH problem, <span class="math inline">\(\eps = \tfrac{1}{2k}\)</span> is a sensible choice. For <span class="math inline">\(k = 100\)</span> and <span class="math inline">\(\delta = .01\)</span> this is in the low thousands. For larger values of <span class="math inline">\(k\)</span>, the number of counters needed scales linearly.<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a> In any case, the number of counters is totally independent of <span class="math inline">\(n\)</span> (which could be in billions)! This is the magic of the count-min sketch — you can throw out almost all of your data set and still maintain approximate frequency counts. Contrast this with bloom filters, and pretty much every other data structure that you’ve seen, where the space grows linearly with the number of processed elements.<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p></li>
<li><p>Assuming the hash functions take constant time to evaluate, the  and  operations run in <span class="math inline">\(O(\ln \tfrac{1}{\delta})\)</span> time.</p></li>
<li><p>The count-min sketch guarantees 1-sided error: no matter how the hash functions <span class="math inline">\(h_1,\ldots,h_{\ell}\)</span> are chosen, for every object <span class="math inline">\(x\)</span> with frequency count <span class="math inline">\(f_x\)</span>, the count-min sketch returns an estimate (<span class="math inline">\(x\)</span>) that is at least <span class="math inline">\(f_x\)</span>.</p></li>
<li><p>Assuming that each hash function <span class="math inline">\(h_1,\ldots,h_{\ell}\)</span> is chosen uniformly from a universal family, for every object <span class="math inline">\(x\)</span> with frequency count <span class="math inline">\(f_x\)</span>, the probability that the estimate (<span class="math inline">\(x\)</span>) output by the count-min sketch is greater than <span class="math inline">\(f_x + \eps n\)</span> is at most <span class="math inline">\(\delta\)</span>. One would expect comparable performance for fixed well-crafted hash functions <span class="math inline">\(h_1,\ldots,h_{\ell}\)</span> on pretty much any data set that you might encounter.</p></li>
</ul>
<h2 id="solving-the-eps-heavy-hitters-problem">Solving the <span class="math inline">\(\eps\)</span>-Heavy Hitters Problem</h2>
<p>The count-min sketch can be used to solve the <span class="math inline">\(\eps\)</span>-HH problem from Section [ss:epshh]. If the total number <span class="math inline">\(n\)</span> of array elements is known in advance, this is easy: set <span class="math inline">\(\eps = \tfrac{1}{2k}\)</span>, process the array elements using a count-min sketch in a single pass, and remember an element once its estimated frequency (according to the count-min sketch) is at least <span class="math inline">\(\tfrac{n}{k}\)</span>.</p>
<p>When <span class="math inline">\(n\)</span> is not known a priori, here is one way to solve the problem. Assume that <span class="math inline">\(\eps = \tfrac{1}{2k}\)</span> and so the number of counters is <span class="math inline">\(O(k \ln \tfrac{1}{\delta})\)</span>. In a single left-to-right pass over the array <span class="math inline">\(A\)</span>, maintain the number <span class="math inline">\(m\)</span> of array entries processed thus far. We store potential heavy hitters in a heap data structure. When processing the next object <span class="math inline">\(x\)</span> of the array, we invoke <span class="math inline">\(\inc(x)\)</span> followed by <span class="math inline">\(\cnt(x)\)</span>. If <span class="math inline">\(\cnt(x) \ge \tfrac{m}{k}\)</span>, then we store <span class="math inline">\(x\)</span> in the heap, using the key <span class="math inline">\(\cnt(x)\)</span>. (If <span class="math inline">\(x\)</span> was already in the heap, we delete it before re-inserting it with its new key value.) This requires one insertion and at most one deletion from the heap. Also, whenever <span class="math inline">\(m\)</span> grows to the point that some object <span class="math inline">\(x\)</span> stored in the heap has a key less than <span class="math inline">\(m/k\)</span> (checkable in <span class="math inline">\(O(1)\)</span> time via Find-Min), we delete <span class="math inline">\(x\)</span> from the heap (via Extract-Min). After finishing the pass, we output all of the objects in the heap.</p>
<p>Assume for simplicity that the count-min sketch makes no large errors, with <span class="math inline">\(\cnt(x) \in [f_x,f_x+\eps n]\)</span> for all <span class="math inline">\(x\)</span>. Every object <span class="math inline">\(x\)</span> with <span class="math inline">\(f_x \ge \tfrac{n}{k}\)</span> is in the heap at the end of the pass. (To see this, consider what happens the last time that <span class="math inline">\(x\)</span> occurs.) The “no large errors” assumption implies an approximate converse: every object <span class="math inline">\(x\)</span> in the heap has true frequency count at least <span class="math inline">\(\tfrac{n}{k} - \eps n =
\tfrac{n}{2k}\)</span> (other objects would be deleted from the heap by the end of the pass). These are exactly the two properties we ask of a solution to the <span class="math inline">\(\eps\)</span>-HH problem. If the count-min sketch makes large errors on a few objects, then these objects might erroneously appear in the final output as well. Ignoring the objects with large errors, the heap contains at most <span class="math inline">\(2k\)</span> objects at all times (why?), so maintaining the heap requires an extra <span class="math inline">\(O(\log k) = O(\log \tfrac{1}{\eps})\)</span> amount of work per array entry.</p>
<h1 id="lecture-take-aways">Lecture Take-Aways</h1>
<ol>
<li><p>Hashing is even cooler and more useful than you had realized.</p></li>
<li><p>The key ideas behind Bloom Filters extend to other lightweight data structures that are useful for solving other problems (not just the membership problem).</p></li>
<li><p>The idea of lossy compression. If you only want to approximately preserve certain properties, like frequency counts, then sometimes you can get away with throwing away almost all of your data. We’ll see another example next week: dimension reduction, where the goal is to approximately preserve pairwise measures (like similarity) between objects.</p></li>
<li><p>(Approximate) frequency counts/heavy hitters are exactly what you want in many applications — traffic at a network switch, click data at a major Web site, streaming data from a telescope or satellite, etc. It’s worth knowing that this useful primitive can be solved efficiently at a massive scale, with even less computation and space than most of the linear-time algorithms that you studied in CS161.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A similar problem is the “top-<span class="math inline">\(k\)</span> problem,” where the goal is to output the <span class="math inline">\(k\)</span> values that occur with the highest frequencies. The algorithmic ideas introduced in this lecture are also relevant for the top-<span class="math inline">\(k\)</span> problem.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>You wouldn’t expect there to be a majority element in any of these applications, but you might expect a non-empty set of heavy hitters when <span class="math inline">\(k\)</span> is 100, 1000, or 10000.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Rather than thinking of the array <span class="math inline">\(A\)</span> as an input fully specified in advance, we can alternatively think of the elements of <span class="math inline">\(A\)</span> as a “data stream,” which are fed to a “streaming algorithm” one element at a time. One-pass algorithms that use small auxiliary space translate to streaming algorithms that need only small working memory. One use case for streaming algorithms is when data arrives at such a fast rate that explicitly storing it is absurd. For example, this is often the reality in the motivating example of data packets traveling through a network switch. A second use case is when, even though data can be stored in its entirety and fully analyzed (perhaps as an overnight job), it’s still useful to perform lightweight analysis on the arriving data in real time. The first two applications (popular transactions or search queries) are examples of this.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>A simple modification of this argument extends the impossibility result to all interesting values of <span class="math inline">\(k\)</span> — can you figure it out?<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Somewhat more detail: if you always use sublinear space to store the set <span class="math inline">\(S\)</span>, then you need to reuse exactly the same memory contents for two different sets <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>. Your membership query answers will be the same in both cases, and in one of these cases some of your answers will be wrong.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>This impossibility result (Fact [fact:hh]) and our response to it (the <span class="math inline">\(\eps\)</span>-HH problem) serve as reminders that the skilled algorithm designer is respectful of but undaunted by impossibility results that limit what algorithms can do. For another example, recall your study in CS161 of methods for coping with <span class="math inline">\(NP\)</span>-complete problems.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>There is a long tradition in the Internet of designing routers that are “fast and dumb,” and many of them have far less memory than a typical smartphone.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>The proposal was to insert all correctly spelled words into a bloom filter. A false positive is then a misspelled word that the spellchecker doesn’t catch.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>The same data structure supports weighted increments, of the form (<span class="math inline">\(x\)</span>,<span class="math inline">\(\Delta\)</span>) for <span class="math inline">\(\Delta \ge 0\)</span>, in exactly the same way. With minor modifications, the data structure can even support deletions. We focus on the special case of incrementing by 1 for simplicity, and because it is sufficient for many of the motivating applications.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Where do we get <span class="math inline">\(\ell\)</span> hash functions from? The same way we get a single hash function. If we’re thinking of hash functions as being drawn at random from a universal family, we just make <span class="math inline">\(\ell\)</span> independent draws from the family. If we’re thinking about deterministic but well-crafted hash functions, in practice it’s usually sufficient to take two good hash functions <span class="math inline">\(h_1,h_2\)</span> and use <span class="math inline">\(\ell\)</span> linear combinations of them (e.g. <span class="math inline">\(h_1\)</span>, <span class="math inline">\(h_1+h_2\)</span>, <span class="math inline">\(h_1 + 2h_2\)</span>, …, <span class="math inline">\(h_1 + (\ell-1)h_2\)</span>). Another common hack, which you’ll implement on Mini-Project #1, is to derive each <span class="math inline">\(h_i\)</span> from a single well-crafted hash function <span class="math inline">\(h\)</span> by defining <span class="math inline">\(h_i(x)\)</span> as something like the hash (using <span class="math inline">\(h\)</span>) of the string formed by <span class="math inline">\(x\)</span> with “<span class="math inline">\(i\)</span>” appended to it.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>In an implementation that chooses <span class="math inline">\(h_i\)</span> deterministically as a well-crafted hash function, the error analysis below does not actually hold for an arbitrary data set. (Recall that for every fixed hash function there is a pathological data set where everything collides.) So instead we say that the analysis is “heuristic” in this case, meaning that while not literally true, we nevertheless expect reality to conform to its predictions (because we expect the data to be non-pathological). Whenever you do a heuristic analysis to predict the performance of an implementation, you should always measure the implementation’s performance to double-check that it’s working as expected. (Of course, you should do this even when you’ve proved performance bounds rigorously — there can always be unmodeled effects (cache performance, etc.) that cause reality to diverge from your theoretical predictions for it.)<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>Don’t forget that probabilities factor only for independent events. There are again two interpretations of this step: in the first, we assume that each <span class="math inline">\(h_i\)</span> is chosen independently and randomly from a universal family of hash functions; in the second, we assume that the <span class="math inline">\(h_i\)</span>’s are sufficiently well crafted that they almost always behave as if they were independent on real data.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>Note the analogous statement for products is false if the <span class="math inline">\(X_j\)</span>’s are not independent. For example, suppose <span class="math inline">\(X_1 \in \{0,1\}\)</span> is uniform while <span class="math inline">\(X_2 = 1-X_1\)</span>. Then <span class="math inline">\({\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X_1 \cdot X_2\right]} = 0\)</span> while <span class="math inline">\({\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X_1\right]} \cdot {\text{\bf E}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[X_2\right]} = \tfrac{1}{4}\)</span>.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>Here <span class="math inline">\(e = 2.718\ldots\)</span>, which gives slightly better constant factors than the choice of 2 in Section [ss:error_heur].<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>Note that the challenging case, and the case that often occurs in our motivating applications, is an array <span class="math inline">\(A\)</span> that simultaneously has lots of different elements but also a few elements that occur many times. If there are few distinct elements, one can maintain the frequency counts exactly using one counter per distinct element. If no elements occur frequently, then there’s nothing to do.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>How is this possible? Intuitively, with an error of <span class="math inline">\(\eps n\)</span> allowed, only the elements with with large (<span class="math inline">\(&gt; \eps n\)</span>) frequency counts matter, and there can be at most <span class="math inline">\(\tfrac{1}{\eps}\)</span> such elements (why?). Thus it is plausible that space proportional to <span class="math inline">\(\tfrac{1}{\eps}\)</span> might be enough. Of course, there’s still the issue of not knowing in advance which <span class="math inline">\(\approx \tfrac{1}{\eps}\)</span> elements are the important ones!<a href="#fnref16">↩</a></p></li>
</ol>
</div>
{% endraw %}
