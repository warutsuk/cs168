---
layout: post
title: "Lecture 9 : The Singular Value Decomposition (SVD) and Low-Rank Matrix Approximations" 
author: Tim
---
{% raw %}
<h1 id="s:toy">What Are The Missing Entries?</h1>
<p>Here’s a quiz for you: consider the following <span class="math inline">\(5 \times 3\)</span> matrix, with 7 entries shown and 8 entries missing: <span class="math display">\[\left[
  \begin{array}{ccc}
    7 &amp; ? &amp; ?\\
    ? &amp; 8 &amp; ?\\
    ? &amp; 12 &amp; 6\\
    ? &amp; ? &amp; 2\\
    21 &amp; 6 &amp; ?
  \end{array}
\right].\]</span> What are the missing entries?</p>
<p>This <span><em>matrix completion</em></span> problem seems a bit unfair, no? After all, each of the unknown entries could be anything, and there’s no way to know what they are. But what if I told you the additional hint that the complete matrix has “nice structure?” This could mean many things, but for the example let’s use an extreme assumption: that <span><em>all rows are multiples of each other.</em></span></p>
<p>Now, it is possible to recover all of the missing entries! For example, if the third row is a multiple of the second one, then each entry in the latter must be <span class="math inline">\(\tfrac{3}{2}\)</span> times the corresponding entry in the former (because of the “12” and “8” in the middle column). Thus we can conclude that the third entry of the second row must be a “4.” Similarly, the middle entry of the fourth row is a “4,” the last entry of the final row is a “3,” and so on. Here’s the completed matrix: <span class="math display">\[\label{eq:mc}
\left[
  \begin{array}{ccc}
    7 &amp; 2 &amp; 1\\
    28 &amp; 8 &amp; 4\\
    42 &amp; 12 &amp; 6\\
    14 &amp; 4 &amp; 2\\
    21 &amp; 6 &amp; 3
  \end{array}
\right].\]</span></p>
<p>The point of the example is that when you know something about the “structure” of a partially known matrix, then sometimes it’s possible to recover all of the “lost” information. The assumption that all rows are multiples of each other is pretty extreme — what would “matrix structure” mean more generally? One natural and useful definition, explained in the next section, is that of having low rank.</p>
<h1 id="matrix-rank">Matrix Rank</h1>
<p>You have probably seen the notion of matrix rank in previous courses, but let’s take a moment to page back in the relevant concepts.</p>
<h4 id="rank-0-matrices.">Rank-0 Matrices.</h4>
<p>There is only one rank-zero matrix of a given size, namely the all-zero matrix.</p>
<h4 id="rank-1-matrices.">Rank-1 Matrices.</h4>
<p>A rank-one matrix is precisely a non-zero matrix of the type assumed in the example above — all rows are (not necessarily integral) multiples of each other. In the example in , all columns are also multiples of each other; this is not an accident.</p>
<p>An equivalent definition of a rank-1 <span class="math inline">\(m \times n\)</span> matrix is as the <span><em>outer product</em></span> <span class="math inline">\(\u\v^{\top}\)</span> of an <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\u\)</span> and an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\v\)</span>:<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <span class="math display">\[\A = \u\v^{\top} = 
\left[
  \begin{array}{ccc}
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; u_1\v^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; u_2\v^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    &amp; \vdots &amp; \\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; u_m\v^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\end{array}
\right]
=
\left[
  \begin{array}{cccc}
{\rule[-1ex]{0.5pt}{2.5ex}}&amp; {\rule[-1ex]{0.5pt}{2.5ex}}&amp; &amp; {\rule[-1ex]{0.5pt}{2.5ex}}\\
v_1\u &amp; v_2\u &amp; \cdots &amp; v_n\u\\
{\rule[-1ex]{0.5pt}{2.5ex}}&amp; {\rule[-1ex]{0.5pt}{2.5ex}}&amp; &amp; {\rule[-1ex]{0.5pt}{2.5ex}}\end{array}
\right].\]</span> Note that each row is a multiple of <span class="math inline">\(\v^{\top}\)</span>, and each column is multiple of <span class="math inline">\(\u\)</span>.</p>
<h4 id="rank-2-matrices.">Rank-2 Matrices.</h4>
<p>A rank-two matrix is just a superposition (i.e., sum) of two rank-1 matrices: <span class="math display">\[\label{eq:rank}
\A = \u\v^{\top} + \w\z^{\top} 
=
\left[
  \begin{array}{ccc}
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; u_1\v^{\top} + w_1\z^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; u_2\v^{\top} + w_2\z^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    &amp; \vdots &amp; \\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; u_m\v^{\top} + w_m\z^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\end{array}
\right] =
\left[
  \begin{array}{cc}
{\rule[-1ex]{0.5pt}{2.5ex}}&amp; {\rule[-1ex]{0.5pt}{2.5ex}}\\
\u &amp; \w \\
{\rule[-1ex]{0.5pt}{2.5ex}}&amp; {\rule[-1ex]{0.5pt}{2.5ex}}\end{array}
\right]
\cdot 
\left[
  \begin{array}{ccc}
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \v^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\\
    {\rule[.5ex]{2.5ex}{0.5pt}}&amp; \z^{\top} &amp; {\rule[.5ex]{2.5ex}{0.5pt}}\end{array}
\right].\]</span> It’s worth spending some time checking and internalizing the equalities in .</p>
<p>OK not quite: a rank-2 matrix is one that can be written as the sum of two rank-1 matrices and is not itself a rank-0 or rank-1 matrix.</p>
<h4 id="rank-k-matrices.">Rank-k Matrices.</h4>
<p>The general definition of matrix rank should now be clear: a matrix <span class="math inline">\(\A\)</span> has rank <span class="math inline">\(k\)</span> if it can be written as the sum of <span class="math inline">\(k\)</span> rank-one matrices, and cannot be written as the sum of <span class="math inline">\(k-1\)</span> or fewer rank-one matrices.</p>
<p>Rephrased in terms of matrix multiplication, an equivalent definition is that <span class="math inline">\(\A\)</span> can written as, or “factored into,” the product of a long and skinny (<span class="math inline">\(m \times k\)</span>) matrix <span class="math inline">\(\Y\)</span> and a short and long (<span class="math inline">\(k
  \times n\)</span>) matrix <span class="math inline">\(\Z^{\top}\)</span> (Figure [f:rank]). (And that <span class="math inline">\(\A\)</span> cannot be likewise factored into the product of <span class="math inline">\(m
\times (k-1)\)</span> and <span class="math inline">\((k-1) \times n\)</span> matrices.)</p>
<div class="figure">
<embed src="rank.pdf" />
<p class="caption">Any matrix <span class="math inline">\(\A\)</span> of rank <span class="math inline">\(k\)</span> can be decomposed into a long and skinny matrix times a short and long one.<span data-label="f:rank"></span></p>
</div>
<p>There are many equivalent definitions of the rank of a matrix <span class="math inline">\(\A\)</span>. The following two conditions are equivalent to each other and to the definition above (any one of the three conditions implies the other two):</p>
<ol>
<li><p>The largest linearly independent subset of columns of <span class="math inline">\(\A\)</span> has size <span class="math inline">\(k\)</span>. That is, all <span class="math inline">\(n\)</span> columns of <span class="math inline">\(\A\)</span> arise as linear combinations of only <span class="math inline">\(k\)</span> of them.</p></li>
<li><p>The largest linearly independent subset of rows of <span class="math inline">\(\A\)</span> has size <span class="math inline">\(k\)</span>. That is, all <span class="math inline">\(m\)</span> rows of <span class="math inline">\(\A\)</span> arise as linear combinations of only <span class="math inline">\(k\)</span> of them.</p></li>
</ol>
<p>It is clear that the first condition implies the second and third: if <span class="math inline">\(\A = \Y\Z^{\top}\)</span>, then all of <span class="math inline">\(\A\)</span>’s columns are linear combinations of the <span class="math inline">\(k\)</span> columns of <span class="math inline">\(\Y\)</span>, and all of <span class="math inline">\(\A\)</span>’s rows are linear combinations of the <span class="math inline">\(k\)</span> rows of <span class="math inline">\(\Z^{\top}\)</span>. See your linear algebra course for proofs of the other implications (they are not difficult).</p>
<h1 id="s:intro">Low-Rank Matrix Approximations: Motivation</h1>
<p>The primary goal of this lecture is to identify the “best” way to approximate a given matrix <span class="math inline">\(\A\)</span> with a rank-<span class="math inline">\(k\)</span> matrix, for a target rank <span class="math inline">\(k\)</span>. Such a matrix is called a <span><em>low-rank approximation</em></span>. Why might you want to do this?</p>
<ol>
<li><p><span><em>Compression.</em></span> A low-rank approximation provides a (lossy) compressed version of the matrix. The original matrix <span class="math inline">\(\A\)</span> is described by <span class="math inline">\(mn\)</span> numbers, while describing <span class="math inline">\(\Y\)</span> and <span class="math inline">\(\Z^{\top}\)</span> requires only <span class="math inline">\(k(m+n)\)</span> numbers. When <span class="math inline">\(k\)</span> is small relative to <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, replacing the product of <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> by their sum is a big win. For example, when <span class="math inline">\(\A\)</span> represents an image (with entries = pixel intensities), <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are typically in the 100s. In other applications, <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> might well be in the tens of thousands or more. With images, a modest value of <span class="math inline">\(k\)</span> (say 100 or 150) is usually enough to achieve approximations that look a lot like the original image.</p>
<p>Thus low-rank approximations are a matrix analog to notions of dimensionality reduction for vectors (Lectures #4 and #7).</p></li>
<li><p><span><em>De-noising.</em></span> If <span class="math inline">\(\A\)</span> is a noisy version of some “ground truth” signal that is approximately low-rank, then passing to a low-rank approximation of the raw data <span class="math inline">\(\A\)</span> might throw out lots of noise and little signal, resulting in a matrix that is actually more informative than the original.</p></li>
<li><p><span><em>Matrix completion.</em></span> Low-rank approximations offer a first-cut approach to the matrix completion problem introduced in Section [s:toy]. (We’ll see more advanced approaches in Week 9.) Given a matrix <span class="math inline">\(\A\)</span> with missing entries, the first step is to obtain a full matrix <span class="math inline">\(\hat{\A}\)</span> by filling in the missing entries with “default” values. Exactly what these default values should be requires trial and error, and the success of the method is often highly dependent on this choice. Natural things to try for default values include: 0, the average of the known entries in the same column, the average of the known entries in the same row, and the average of the known entries of the matrix. The second step is to compute the best rank-<span class="math inline">\(k\)</span> approximation to <span class="math inline">\(\Ahat\)</span>. This approach works reasonably well when the unknown matrix is close to a rank-<span class="math inline">\(k\)</span> matrix and there are not too many missing entries.</p></li>
</ol>
<p>Our high-level plan for computing a rank-<span class="math inline">\(k\)</span> approximation of a matrix <span class="math inline">\(\A\)</span> is: (i) express <span class="math inline">\(\A\)</span> as a list of its ingredients, ordered by “importance;” (ii) keep only the <span class="math inline">\(k\)</span> most important ingredients. The non-trivial step (i) is made easy by the singular value decomposition, a general matrix operation discussed in the next section.</p>
<h1 id="the-singular-value-decomposition-svd">The Singular Value Decomposition (SVD)</h1>
<h2 id="definitions">Definitions</h2>
<p>We’ll start with the formal definitions, and then discuss interpretations, applications, and connections to concepts in previous lectures. A <span><em>singular value decomposition (SVD)</em></span> of an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\A\)</span> expresses the matrix as the product of three “simple” matrices: <span class="math display">\[\label{eq:svd}
\A = \U \S \V^{\top},\]</span> where:</p>
<ol>
<li><p><span class="math inline">\(\U\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix;<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p></li>
<li><p><span class="math inline">\(\V\)</span> is an <span class="math inline">\(n \times n\)</span> orthogonal matrix;</p></li>
<li><p><span class="math inline">\(\S\)</span> is an <span class="math inline">\(m \times n\)</span> diagonal matrix with nonnegative entries, and with the diagonal entries sorted from high to low (as one goes “northwest” to “southeast).”<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p></li>
</ol>
<div class="figure">
<embed src="svd.pdf" />
<p class="caption">The singular value decomposition (SVD). Each singular value in <span class="math inline">\(\S\)</span> has an associated left singular vector in <span class="math inline">\(\U\)</span>, and right singular vector in <span class="math inline">\(\V\)</span>.<span data-label="f:svd"></span></p>
</div>
<p>Note that in contrast to the decomposition discussed in Lecture #8 (<span class="math inline">\(\A = \Q\D\Q^{\top}\)</span> when <span class="math inline">\(\A\)</span> has the form <span class="math inline">\(\X^{\top}\X\)</span>), the orthogonal matrices <span class="math inline">\(\U\)</span> and <span class="math inline">\(\V\)</span> are <span><em>not</em></span> the same — since <span class="math inline">\(\A\)</span> need not be square, <span class="math inline">\(\U\)</span> and <span class="math inline">\(\V\)</span> need not even have the same dimensions.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p>The columns of <span class="math inline">\(\U\)</span> are called the <span><em>left singular vectors</em></span> of <span class="math inline">\(\A\)</span> (these are <span class="math inline">\(m\)</span>-vectors). The columns of <span class="math inline">\(\V\)</span> (that is, the rows of <span class="math inline">\(\V^{\top}\)</span>) are the <span><em>right singular vectors</em></span> of <span class="math inline">\(\A\)</span> (these are <span class="math inline">\(n\)</span>-vectors). The entries of <span class="math inline">\(\S\)</span> are the <span> <em>singular values</em></span> of <span class="math inline">\(\A\)</span>. Thus with each singular vector (left or right) there is an associated singular value. The “first” or “top” singular vector refers to the one associated with the largest singular value, and so on. See Figure [f:svd].</p>
<p>To better see how the SVD expresses <span class="math inline">\(\A\)</span> as a “list of its ingredients,” check that the factorization <span class="math inline">\(\A = \U\S\V^{\top}\)</span> is equivalent to the expression <span class="math display">\[\label{eq:ingred}
\A = \sum_{i=1}^{\min\{m,n\}} s_i \cdot \u_i\v_i^{\top},\]</span> where <span class="math inline">\(s_i\)</span> is the <span class="math inline">\(i\)</span>th singular value and <span class="math inline">\(\u_i,\v_i\)</span> are the corresponding left and right singular vectors. That is, the SVD expresses <span class="math inline">\(\A\)</span> as a nonnegative linear combination of <span class="math inline">\(\min\{m,n\}\)</span> rank-1 matrices, with the singular values providing the multipliers and the outer products of the left and right singular vectors providing the rank-1 matrices.</p>
<p>Every matrix <span class="math inline">\(\A\)</span> has a SVD. The proof is not deep, but is better covered in a linear algebra course than here. Geometrically, thinking of an <span class="math inline">\(m \times n\)</span> matrix as a mapping from <span class="math inline">\(\RR^n\)</span> to <span class="math inline">\(\RR^m\)</span>, this fact is kind of amazing: every matrix <span class="math inline">\(\A\)</span>, no matter how weird, is only performing a rotation in the domain (multiplication by <span class="math inline">\(\V^{\top}\)</span>), followed by scaling plus adding or deleting dimensions (multiplication by <span class="math inline">\(\S\)</span>) as needed, followed by a rotation in the range (multiplication by <span class="math inline">\(\U\)</span>). Along the lines of last lecture’s discussion, the SVD is “more or less unique.” The singular values of a matrix are unique. When a singular value appears multiple times, the subspaces spanned by the corresponding left and right singular vectors are uniquely defined, but arbitrary orthonormal bases can be chosen for each.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>There are pretty good algorithms for computing the SVD of a matrix; details are covered in any numerical analysis course. It is unlikely that you will ever need to implement one of these yourself. For example, in Matlab, you literally just write <span> [U,S,V] = svd(A)</span> to compute the SVD of <span class="math inline">\(\A\)</span>. The running time of the algorithm is the smaller of <span class="math inline">\(O(m^2n)\)</span> and <span class="math inline">\(O(n^2m)\)</span>, and the standard implementations of it have been heavily optimized. A typical laptop should have no trouble computing the SVD of a <span class="math inline">\(5000
\times 5000\)</span> dense matrix, but might choke on a <span class="math inline">\(10000 \times 10000\)</span> matrix. As we remark at the end of these notes, if you just want to compute the largest <span class="math inline">\(k\)</span> singular values, and their associated singular vectors, this can be computed significantly faster, in time roughly <span class="math inline">\(O(kmn)\)</span>.</p>
<h1 id="low-rank-approximations-from-the-svd">Low-Rank Approximations from the SVD</h1>
<p>If we want to best approximate a matrix <span class="math inline">\(\A\)</span> by a rank-<span class="math inline">\(k\)</span> matrix, how should we do it? If only we had a representation of the data matrix <span class="math inline">\(\A\)</span> as a sum of several ingredients, with these ingredients ordered by “importance,” then we could just keep the <span class="math inline">\(k\)</span> “most important” ones. But wait, the SVD gives us exactly such a representation! Recalling that the SVD expresses a matrix <span class="math inline">\(\A\)</span> as a sum of rank-1 matrices (weighted by the corresponding singular values), a natural idea is to keep only the first <span class="math inline">\(k\)</span> terms on the right-hand side of . That is, for <span class="math inline">\(\A\)</span> as in  and a target rank <span class="math inline">\(k\)</span>, the proposed rank-<span class="math inline">\(k\)</span> approximation is <span class="math display">\[\Ahat = \sum_{i=1}^{k} s_i \cdot \u_i\v_i^{\top},\]</span> where as usual we assume that the singular values have been sorted (<span class="math inline">\(s_1 \ge s_2 \ge \cdots s_{\min\{m,n\}} \ge 0\)</span>), and <span class="math inline">\(\u_i\)</span> and <span class="math inline">\(\v_i\)</span> denote the <span class="math inline">\(i\)</span>th left and right singular vectors. As the sum of <span class="math inline">\(k\)</span> rank-1 matrices, <span class="math inline">\(\Ahat\)</span> clearly has rank (at most) <span class="math inline">\(k\)</span>.</p>
<p>Here is an equivalent way to think about the proposed rank-<span class="math inline">\(k\)</span> approximation (see also Figure [f:lowrank]).</p>
<ol>
<li><p>Compute the SVD <span class="math inline">\(\A = \U \S \V^{\top}\)</span>, where <span class="math inline">\(\U\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix, <span class="math inline">\(\S\)</span> is a nonnegative <span class="math inline">\(m \times n\)</span> diagonal matrix with diagonal entries sorted from high to low, and <span class="math inline">\(\V^{\top}\)</span> is a <span class="math inline">\(n \times n\)</span> orthogonal matrix.</p></li>
<li><p>Keep only the top <span class="math inline">\(k\)</span> right singular vectors: set <span class="math inline">\(\V_k^{\top}\)</span> equal to the first <span class="math inline">\(k\)</span> rows of <span class="math inline">\(\V^{\top}\)</span> (a <span class="math inline">\(k \times n\)</span> matrix).</p></li>
<li><p>Keep only the top <span class="math inline">\(k\)</span> left singular vectors: set <span class="math inline">\(\U_k\)</span> equal to the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(\U\)</span> (an <span class="math inline">\(m \times k\)</span> matrix).</p></li>
<li><p>Keep only the top <span class="math inline">\(k\)</span> singular values: set <span class="math inline">\(\S_k\)</span> equal to the first <span class="math inline">\(k\)</span> rows and columns of <span class="math inline">\(\S\)</span> (a <span class="math inline">\(k \times k\)</span> matrix), corresponding to the <span class="math inline">\(k\)</span> largest singular values of <span class="math inline">\(\A\)</span>.</p></li>
<li><p>The rank-<span class="math inline">\(k\)</span> approximation is then <span class="math display">\[\label{eq:svdk}
\A_k = \U_k\S_k\V_k^{\top}.\]</span></p></li>
</ol>
<div class="figure">
<embed src="lowrank.pdf" />
<p class="caption">Low rank approximation via SVD. Recall that <span class="math inline">\(\S\)</span> is non-zero only on its diagonal, and the diagonal entries of <span class="math inline">\(S\)</span> are sorted from high to low. Our low rank approximation is <span class="math inline">\(\A_k = \U_k\S_k\V_k^{\top}\)</span>.<span data-label="f:lowrank"></span></p>
</div>
<p>Storing the matrices on the right-hand side of  takes <span class="math inline">\(O(k(m+n))\)</span> space, in contrast to the <span class="math inline">\(O(mn)\)</span> space required to store the original matrix <span class="math inline">\(\A\)</span>. This is a big win when <span class="math inline">\(k\)</span> is relatively small and <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are relatively large (as in many applications).</p>
<p>It is natural to interpret  as approximating the raw data <span class="math inline">\(\A\)</span> in terms of <span class="math inline">\(k\)</span> “concepts” (e.g., “math,” “music,” and “sports”), where the singular values of <span class="math inline">\(\S_k\)</span> express the signal strengths of these concepts, the rows of <span class="math inline">\(\V^{\top}\)</span> and columns of <span class="math inline">\(\U\)</span> express the “canonical row/column” associated with each concept (e.g., a customer that likes only music products, or a product liked only by music customers), and the rows of <span class="math inline">\(\U\)</span> (respectively, columns of <span class="math inline">\(\V^{\top}\)</span>) approximately express each row (respectively, column) of <span class="math inline">\(\A\)</span> as a linear combination (scaled by <span class="math inline">\(\S_k\)</span>) of the “canonical rows” (respectively, canonical columns).</p>
<p>Conceptually, this method of producing a low-rank approximation is as clean as could be imagined: we re-represent <span class="math inline">\(\A\)</span> using the SVD, which provides a list of <span class="math inline">\(\A\)</span>’s “ingredients,” ordered by “importance,” and we retain only the <span class="math inline">\(k\)</span> most important ingredients. But is the result of this elegant computation any good?</p>
<p>The next fact justifies this approach: this low-rank approximation is <span><em>optimal</em></span> in a natural sense. The guarantee is in terms of the “Frobenius norm” of a matrix <span class="math inline">\(\M\)</span>, which just means applying the <span class="math inline">\(\ell_2\)</span> norm to the matrix as if it were a vector: <span class="math inline">\(\|\M\|_F = \sqrt{\sum_{i,j} m_{ij}^2}\)</span>.</p>
<p>[fact:opt] For every <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\A\)</span>, rank target <span class="math inline">\(k \ge 1\)</span>, and rank-<span class="math inline">\(k\)</span> <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\B\)</span>, <span class="math display">\[\|\A - \A_k\|_F \le \|\A-\B\|_F,\]</span> where <span class="math inline">\(\A_k\)</span> is the rank-<span class="math inline">\(k\)</span> approximation  derived from the SVD of <span class="math inline">\(\A\)</span>.</p>
<p>We won’t prove Fact [fact:opt] formally, but see Section [s:pca] for a plausibility argument based on the properties we’ve already established about the closely related PCA method.</p>
<p>[rem:choosek] When producing a low-rank matrix approximation, we’ve been taking as a parameter the target rank <span class="math inline">\(k\)</span>. But how should <span class="math inline">\(k\)</span> be chosen? In a perfect world, the singular values of <span class="math inline">\(\A\)</span> give strong guidance: if the top few such values are big and the rest are small, then the obvious solution is to take <span class="math inline">\(k\)</span> equal to the number of big values. In a less perfect world, one takes <span class="math inline">\(k\)</span> as small as possible subject to obtaining a useful approximation — of course what “useful” means depends on the application. Rules of thumb often take the form: choose <span class="math inline">\(k\)</span> such that the sum of the top <span class="math inline">\(k\)</span> singular values is at least <span class="math inline">\(c\)</span> times as big as the sum of the other singular values, where <span class="math inline">\(c\)</span> is a domain-dependent constant (like 10, say).</p>
<p>Using the SVD to produce low-rank matrix approximations is another example of a useful paradigm for lossy compression. The first step of the paradigm is to re-express the raw data exactly as a decomposition into several terms (as in ). The second step is to throw away all but the “most important” terms, yielding an approximation of the original data. This paradigm works well when you can find a representation of the data such that most of the interesting information is concentrated in just a few components of the decomposition. The appropriate representation will depend on the data set — though there are a few rules of thumb, as we’ll discuss — and of course, messy enough data sets might not admit any nice representations at all.</p>
<h1 id="ss:reduces">PCA Reduces to SVD</h1>
<p>There is an interesting relationship between the SVD and the decompositions we discussed last week. Recall in the last lecture we used the fact that <span class="math inline">\(\XX\)</span>, as a symmetric <span class="math inline">\(n \times n\)</span> matrix, can be written as <span class="math inline">\(\XX = \Q\D\Q^{\top}\)</span>, where <span class="math inline">\(\Q\)</span> is an <span class="math inline">\(n \times n\)</span> orthogonal matrix and <span class="math inline">\(\D\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix. (Here <span class="math inline">\(\X\)</span> is the data matrix, with each of the <span class="math inline">\(m\)</span> rows representing a data point in <span class="math inline">\(\RR^n\)</span>.) Consider the SVD <span class="math inline">\(\X = \U \S \V^{\top}\)</span> and what its existence means for <span class="math inline">\(\XX\)</span>: <span class="math display">\[\label{eq:QDQ}
\XX = (\U\S\V^{\top})^{\top}(\U\S\V^{\top}) = \V \S^{\top} \underbrace{\U^{\top}\U}_{=I} \S
\V^{\top} = \V \D \V^{\top},\]</span> where <span class="math inline">\(\D\)</span> is a diagonal matrix with diagonal entries equal to the squares of the diagonal entries of <span class="math inline">\(\S\)</span> (if <span class="math inline">\(m &lt; n\)</span> then the remaining <span class="math inline">\(n-m\)</span> diagonal entries of <span class="math inline">\(\D\)</span> are 0).</p>
<p>Recall from last lecture that if you decompose <span class="math inline">\(\XX\)</span> as <span class="math inline">\(\Q \D \Q^{\top}\)</span>, then the rows of <span class="math inline">\(\Q^{\top}\)</span> are the eigenvectors of <span class="math inline">\(\XX\)</span>. The computation in  therefore shows that the rows of <span class="math inline">\(\V^{\top}\)</span> are the eigenvectors of <span class="math inline">\(\XX\)</span>. Thus, <span><em>the right singular vectors of <span class="math inline">\(\X\)</span> are the same as the eigenvectors of <span class="math inline">\(\XX\)</span></em></span>. Similarly, the eigenvalues of <span class="math inline">\(\XX\)</span> are the squares of the singular values of <span class="math inline">\(\X\)</span>.</p>
<p>Thus <span><em>principal components analysis (PCA) reduces to computing the SVD of <span class="math inline">\(\A\)</span> (without forming <span class="math inline">\(\XX\)</span>)</em></span>. Recall that the output of PCA, given a target <span class="math inline">\(k\)</span>, is simply the top <span class="math inline">\(k\)</span> eigenvectors of the covariance matrix <span class="math inline">\(\XX\)</span>. The SVD <span class="math inline">\(\U \S \V^{\top}\)</span> of <span class="math inline">\(\X\)</span> hands you these eigenvectors on a silver platter — they are simply the first <span class="math inline">\(k\)</span> rows of <span class="math inline">\(\V^{\top}\)</span>. This is an alternative to the Power Iteration method discussed last lecture. So which method for computing eigenvectors is better? There is no clear answer; in many cases, either should work fine, and if performance is critical you’ll want to experiment with both. Certainly the Power Iteration method, which finds the eigenvectors of <span class="math inline">\(\XX\)</span> one-by-one, looks like a good idea when you only want the top few eigenvectors (as in our data visualization use cases). If you want many or all of them, then the SVD — which gives you all of the eigenvectors, whether you want them or not — is probably the first thing to try. Now that we understand the close connection between the SVD and the PCA method, let’s return to Fact [fact:opt], which states that the SVD-based rank-<span class="math inline">\(k\)</span> approximation is optimal (with respect to the Frobenius norm). Intuitively, this fact holds because: (i) minimizing the Frobenius norm <span class="math inline">\(\|\A-\B\|_F\)</span> is equivalent to minimizing the average (over <span class="math inline">\(i\)</span>) of the squared Euclidean distances between the <span class="math inline">\(i\)</span>th rows of <span class="math inline">\(\A\)</span> and <span class="math inline">\(\B\)</span>; (ii) the SVD uses the same vectors to approximate the rows of <span class="math inline">\(\A\)</span> as PCA (the top eigenvectors of <span class="math inline">\(\AA\)</span>/right singular vectors of <span class="math inline">\(\A\)</span>); and (iii) PCA, by definition, chooses its <span class="math inline">\(k\)</span> vectors to minimize the average squared Euclidean distance between the rows of <span class="math inline">\(\A\)</span> and the <span class="math inline">\(k\)</span>-dimensional subspace of linear combinations of these vectors. The contribution of a row of <span class="math inline">\(\A-\A_k\)</span> to the Frobenius norm corresponds exactly to one of these squared Euclidean distances.</p>
<h1 id="more-on-pca-vs.svd">More on PCA vs. SVD</h1>
<p>PCA and SVD are closely related, and in data analysis circles you should be ready for the terms to be used almost interchangeably. There are differences, however. First, PCA refers to a data analysis approach — a choice of how to define the “best” way to approximate a bunch of data points as linear combinations of a small set of vectors. Meanwhile, the SVD is a general operation defined on all matrices. For example, it doesn’t really make sense to talk about “applying PCA” to a matrix <span class="math inline">\(\A\)</span> unless the rows of <span class="math inline">\(\A\)</span> have clear semantics — typically, as data points <span class="math inline">\(\x_1,\ldots,\x_m\)</span> in <span class="math inline">\(\RR^n\)</span>. By contrast, the SVD  is well defined for every matrix <span class="math inline">\(\A\)</span>, whatever the semantics for <span class="math inline">\(\A\)</span>. In the particular case where <span class="math inline">\(\A\)</span> is a matrix where the rows represent data points, the SVD can be interpreted as performing the calculations required by PCA. (The SVD is also useful for many other computational tasks.)</p>
<p>We can also make more of an “apples vs. apples” comparison in the following way. Let’s define the “PCA operation” as taking an <span class="math inline">\(m
\times n\)</span> data matrix <span class="math inline">\(\X\)</span> as input, and possibly a parameter <span class="math inline">\(k\)</span>, and outputting all (or the top <span class="math inline">\(k\)</span>) eigenvectors of the covariance matrix <span class="math inline">\(\XX\)</span>. The “SVD operation” takes as input an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\X\)</span> and outputs <span class="math inline">\(\U\)</span>, <span class="math inline">\(\S\)</span>, and <span class="math inline">\(\V^{\top}\)</span>, where the rows of <span class="math inline">\(\V^{\top}\)</span> are the eigenvectors of <span class="math inline">\(\XX\)</span>. Thus <span><em>the SVD gives strictly more information than required by PCA</em></span>, namely the matrix <span class="math inline">\(\U\)</span>.</p>
<p>Is the additional information <span class="math inline">\(\U\)</span> provided by the SVD useful? In applications where you want to understand the <span><em>column</em></span> structure of <span class="math inline">\(\X\)</span>, in addition to the row structure, the answer is “yes.” To see this, let’s review some interpretations of the SVD . On the one hand, the decomposition expresses every row of <span class="math inline">\(\X\)</span> as a linear combinations of the rows of <span class="math inline">\(\V^{\top}\)</span>, with the rows of <span class="math inline">\(\U\S\)</span> providing the coefficients of these linear combinations. That is, we can interpret the rows of <span class="math inline">\(\X\)</span> in terms of the rows of <span class="math inline">\(\V^{\top}\)</span>, which is useful when the rows of <span class="math inline">\(\V^{\top}\)</span> have interesting semantics. Analogously, the decomposition in  expresses the <span><em>columns</em></span> of <span class="math inline">\(\X\)</span> as linear combinations of the columns of <span class="math inline">\(\U\)</span>, with the coefficients given by the columns of <span class="math inline">\(\S\V^{\top}\)</span>. So when the columns of <span class="math inline">\(\U\)</span> are interpretable, the decomposition gives us a way to understand the columns of <span class="math inline">\(\X\)</span>.</p>
<p>In some applications, we really only care about understanding the rows of <span class="math inline">\(\X\)</span>, and the extra information <span class="math inline">\(\U\)</span> provided by the SVD over PCA is irrelevant. In other applications, both the rows and the columns of <span class="math inline">\(\X\)</span> are interesting in their own right. For example:</p>
<ol>
<li><p>Suppose rows of <span class="math inline">\(\X\)</span> are indexed by customers, and the columns by products, with the matrix entries indicating who likes what. We are interested in understanding the rows, and in the best-case scenario, the right singular vectors (rows of <span class="math inline">\(\V^{\top}\)</span>) are interpretable as “customer types” or “canonical customers” and the SVD expresses each customer as a mixture of customer types. For example, perhaps one or both of your instructors can be understood simply as a mixture of a “math customer,” a “music customer,” and a “sports customer.” In the ideal case, the left singular vectors (columns of <span class="math inline">\(\U\)</span>) can be interpreted as “product types,” where the “types” are the same as for customers, and the SVD expresses each product as a mixture of product types (the extent to which a product appeals to a “math customer,” a “music customer,” etc.).</p></li>
<li><p>Suppose the matrix represents data about drug interactions, with the rows of <span class="math inline">\(\X\)</span> indexed by proteins or pathways, and the columns by chemicals or drugs. We’re interested in understanding both proteins and drugs in their own right, as mixtures of a small set of “basic types.”</p></li>
</ol>
<p>In the above two examples, what we really care about is the relationships between two groups of objects — customers and products, or proteins and drugs — the labeling of one group as the “rows” of a matrix and the other as the “columns” is arbitrary. In such cases, you should immediately think of the SVD as a potential tool for better understanding the data. When the columns of <span class="math inline">\(\X\)</span> are not interesting in their own right, PCA already provides the relevant information.</p>
<h1 id="s:pca">PCA-Based Low-Rank Approximations (Optional)</h1>
<p>The techniques developed for PCA can also be used to produce low-rank matrix approximations, as follows.</p>
<ol>
<li><p>Preprocess the given matrix <span class="math inline">\(\A\)</span> so that the rows sum to the all-zero vector and, optionally, normalize each column (like last week).</p></li>
<li><p>Form the covariance matrix <span class="math inline">\(\AA\)</span>.</p></li>
<li><p>In the notation of Figure [f:rank], take the <span class="math inline">\(k\)</span> rows of <span class="math inline">\(\Z^{\top}\)</span> to be the top <span class="math inline">\(k\)</span> principal components of <span class="math inline">\(\A\)</span> — the <span class="math inline">\(k\)</span> eigenvectors <span class="math inline">\(\v_1,\v_2,\ldots,\v_k\)</span> of <span class="math inline">\(\AA\)</span> that have the largest eigenvalues.</p></li>
<li><p>For <span class="math inline">\(i=1,2,\ldots,m\)</span>, the <span class="math inline">\(i\)</span>th row of the matrix <span class="math inline">\(\Y\)</span> is defined as the projections <span class="math inline">\(({
{\langle {\x_i} , {\v_1} \rangle}
},\ldots,{
{\langle {\x_i} , {\v_k} \rangle}
})\)</span> of <span class="math inline">\(\x_i\)</span> onto the vectors <span class="math inline">\(\v_1,\ldots,\v_k\)</span>. This is the best approximation, in terms of Euclidean distance from <span class="math inline">\(\x_i\)</span>, of <span class="math inline">\(\x_i\)</span> as a linear combination of <span class="math inline">\(\v_1,\ldots,\v_k\)</span>.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p></li>
</ol>
<p>The above four steps certainly produce a matrix <span class="math display">\[\label{eq:YZ}
\Y \cdot \Z^{\top}\]</span> that has rank at most <span class="math inline">\(k\)</span>. How does it compare to the SVD-based low-rank approximation? In fact, it is exactly the same!<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<p>[fact:same] The matrix <span class="math inline">\(\A_k\)</span> defined in  and the matrix <span class="math inline">\(\A_k\)</span> defined in  are identical.</p>
<p>We won’t prove Fact [fact:same], but pause to note its plausibility. We defined <span class="math inline">\(\Z^{\top}\)</span> to be the top <span class="math inline">\(k\)</span> principal components of <span class="math inline">\(\A\)</span> — the first <span class="math inline">\(k\)</span> eigenvectors of the covariance matrix <span class="math inline">\(\AA\)</span>. As noted in Section [ss:reduces], the right singular vector of <span class="math inline">\(\A\)</span> (i.e., the rows of <span class="math inline">\(\V^{\top}\)</span>) are also the eigenvectors of <span class="math inline">\(\AA\)</span>. Thus, the matrices <span class="math inline">\(\Z^{\top}\)</span> and <span class="math inline">\(\V_k^{\top}\)</span> are identical, both equal to the top <span class="math inline">\(k\)</span> eigenvectors of <span class="math inline">\(\AA\)</span>/top <span class="math inline">\(k\)</span> right singular vectors of <span class="math inline">\(\A\)</span>. Given this, it is not surprising that the two definitions of <span class="math inline">\(\A_k\)</span> are the same: both the matrix <span class="math inline">\(\Y\)</span> in  and the matrix <span class="math inline">\(\U_k \S_k\)</span> in  are intuitively defining the linear combinations of the rows of <span class="math inline">\(\Z^{\top}\)</span> and <span class="math inline">\(\V^{\top}_k\)</span> that give the best approximation to <span class="math inline">\(\A\)</span>. In the PCA-based solution in Section [s:pca], this is explicitly how <span class="math inline">\(\Y\)</span> is defined; the SVD encodes the same linear combinations in the form <span class="math inline">\(\U_k \S_k\)</span>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Contrast this with the <span><em>inner</em></span> (a.k.a. <span><em>dot</em></span>) product <span class="math inline">\(\u^{\top}\v
= \sum_{i=1}^n u_iv_i\)</span>, which is only defined for two vectors of the same dimension and results in a scalar.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Recall from last lecture that a matrix is <span><em>orthogonal</em></span> if its columns (or equivalently, its rows) are orthonormal vectors, meaning they all have norm 1 and the inner product of any distinct pair of them is 0.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>When we say that a (not necessarily square) matrix is diagonal, we mean what you’d think: only the entries of the form <span class="math inline">\((i,i)\)</span> are allowed to be non-zero.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Even small numerical examples are tedious to do in detail — the orthogonality constraint on singular vectors ensures that most of the numbers are messy. The easiest way to get a feel for what SVDs look like is to feed a few small matrices into the SVD subroutine supported by your favorite environment (Matlab, python’s numpy library, etc.).<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Also, one can always multiply the <span class="math inline">\(i\)</span>th left and right singular vectors by -1 to get another SVD.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>For example, with <span class="math inline">\(k=2\)</span>, these values <span class="math inline">\(({
{\langle {\x_i} , {\w_1} \rangle}
},{
{\langle {\x_i} , {\w_2} \rangle}
})\)</span> are the values that we plotted in the “map of Europe” example in Lecture #7.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>We’re assuming that identical preprocessing of <span class="math inline">\(\A\)</span>, if any, is done in both cases.<a href="#fnref7">↩</a></p></li>
</ol>
</div>



{% endraw %}
