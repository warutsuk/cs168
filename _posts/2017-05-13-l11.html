---
layout: post
title: Lecture 11
author: Tim
---
{% raw %}

<p>Spectral graph theory is the powerful and beautiful theory that arises from the following question:</p>
<blockquote>
<p>What properties of a graph are exposed/revealed if we 1) represent the graph as a matrix, and 2) study the eigenvectors/eigenvalues of that matrix.</p>
</blockquote>
<p>Most of our lectures thus far have been motivated by concrete problems, and the problems themselves led us to the algorithms and more widely applicable techniques and insights that we then went on to discuss. This section has the opposite structure: we will begin with the very basic observation that graphs can be represented as matrices, and then ask “what happens if we apply the linear algebraic tools to these matrices”? That is, we will begin with a technique, and in the process of gaining an understanding for what the technique does, we will arrive at several extremely natural problems that can be approached via this technique.</p>
<p>Throughout these lecture notes we will consider undirected, and unweighted graphs (i.e. all edges have weight 1), that do not have any self-loops. Most of the definitions and techniques will extend to both directed graphs, as well as weighted graphs, though we will not discuss these extensions here.</p>
<h1 id="graphs-as-matrices">Graphs as Matrices</h1>
<p>Given a graph, <span class="math inline">\(G = (V,E)\)</span> with <span class="math inline">\(|V|=n\)</span> vertices, there are a number of matrices that we can associate to the graph, including the adjacency matrix. As we will see, one extremely natural matrix is the <em>Laplacian</em> of the graph:</p>
<p>Given a graph <span class="math inline">\(G=(V,E),\)</span> with <span class="math inline">\(|V|=n\)</span>, the <em>Laplacian</em> matrix associated to <span class="math inline">\(G\)</span> is an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(L_G = D - A\)</span>, where <span class="math inline">\(D\)</span> is the degree matrix—a diagonal matrix with <span class="math inline">\(D(i,i)\)</span> is the degree of the <span class="math inline">\(i\)</span>th node in <span class="math inline">\(G\)</span>, and <span class="math inline">\(A\)</span> is the adjacency matrix, with <span class="math inline">\(A(i,j) = 1\)</span> if and only if <span class="math inline">\((i,j) \in E\)</span>. Namely, <span class="math display">\[L_G(i,j) = \begin{cases} deg(i) &amp;\mbox{if } i = j\\ -1  &amp; \mbox{if } (i,j) \in E \equiv 1\\ 0 &amp;\mbox{otherwise.} \end{cases}\]</span></p>
<p>For ease of notation, in settings where the underlying graph is clear, we will omit the subscript, and simply refer to the Laplacian as <span class="math inline">\(L\)</span> rather than <span class="math inline">\(L_G\)</span>.</p>
<p>To get a better sense of the Laplacian, let us consider what happens to a vector when we multiply it by <span class="math inline">\(L\)</span>: if <span class="math inline">\(Lv = w,\)</span> then <span class="math display">\[w(i) = deg(i) v(i) - \sum_{j: (i,j) \in E} v_j = \sum_{j: (i,j) \in E} \left(v(i) - v(j) \right).\]</span> That is the <span class="math inline">\(i\)</span>th element of the product <span class="math inline">\(Lv\)</span> is the sum of the differences between <span class="math inline">\(v(i)\)</span> and the indices of <span class="math inline">\(v\)</span> corresponding to the neighbors of <span class="math inline">\(i\)</span> in the graph <span class="math inline">\(G\)</span>.</p>
<p>Building off this calculation, we will now interpret the quantity <span class="math inline">\(v^t L v\)</span>, which will prove to be the main intuition that we come back to when reasoning about the eigenvalues/eigenvectors of <span class="math inline">\(L\)</span>: <span class="math display">\[\begin{aligned}
v^t L v &amp; = &amp; \sum_i v(i) \sum_{j: (i,j) \in E} \left( v(i) - v(j) \right)\\ &amp; = &amp;\sum_{(i,j) \in E}v(i)\left(v(i) - v(j)\right)\\ &amp; = &amp; \sum_{i&lt;j: (i,j) \in E}v(i)\left(v(i) - v(j)\right) + v(j)\left(v(j) - v(i)\right)  \\ &amp; = &amp;\sum_{i&lt;j: (i,j) \in E}\left(v(i) - v(j)\right) ^2.\end{aligned}\]</span></p>
<p>In other words, if one interprets the vector <span class="math inline">\(v\)</span> as assigning a number to each vertex in the graph <span class="math inline">\(G\)</span>, the quantity <span class="math inline">\(v^t L v\)</span> is exactly the sum of the squares of the differences between the values of neighboring nodes. Phrased alternately, if one were to place the vertices of the graph on the real numberline, with the <span class="math inline">\(i\)</span>th node placed at location <span class="math inline">\(v(i)\)</span>, then <span class="math inline">\(v^tL v\)</span> is precisely the sum of the squares of the lengths of all the edges of the graph.</p>
<h1 id="the-eigenvalues-and-eigenvectors-of-the-laplacian">The Eigenvalues and Eigenvectors of the Laplacian</h1>
<p>What can we say about the eigenvalues of the Laplacian, <span class="math inline">\(\lambda_1 \le \lambda_2 \le \ldots \le \lambda_n\)</span>? Since <span class="math inline">\(L\)</span> is a real-valued symmetric matrix, all of its eigenvalues are real numbers, and its eigenvectors are orthogonal to eachother. The calculation above showed that for any vector <span class="math inline">\(v\)</span>, <span class="math inline">\(v^t L v = \sum_{i&lt;j: (i,j) \in E} (v(i)-v(j))^2.\)</span> Because this expression is the sum of squares, we conclude that <span class="math inline">\(v^t L v \ge 0,\)</span> and hence all the eigenvalues of the Laplacian must be nonnegative.</p>
<p>The eigenvalues of <span class="math inline">\(L\)</span> contain information about the structure (or, more precisely, information about the extent to which the graph has structure). We will see several illustrations of this claim; this is a very hot research area, and there continues to be research uncovering new insights into the types of structural information contained in the eigenvalues of the Laplacian.</p>
<h2 id="the-zero-eigenvalue">The zero eigenvalue</h2>
<p>The laplacian always has at least one eigenvalue that is 0. To see this, consider the vector <span class="math inline">\(v = (1/\sqrt{n},\ldots,1/\sqrt{n})\)</span>, and recall that the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(Lv\)</span> is <span class="math inline">\(\sum_{j:(i,j) \in E} v(i) - v(j) = \sum (1/\sqrt{n} - 1/\sqrt{n}) = 0=0\cdot v(i).\)</span> The following theorem shows that the multiplicity of the zeroth eigenvalue reveals the number of connected components of the graph.</p>
<p>The number of zero eigenvalues of the Laplacian <span class="math inline">\(L_G\)</span> (i.e. the multiplicity of the 0 eigenvalue) equals the number of connected components of the graph <span class="math inline">\(G\)</span>.</p>
<p>We first show that the number of zero eigenvalues is at least the number of connected components of <span class="math inline">\(G\)</span>. Indeed, assume that <span class="math inline">\(G\)</span> has <span class="math inline">\(k\)</span> connected components, corresponding to the partition of <span class="math inline">\(V\)</span> into disjoint sets <span class="math inline">\(S_1,\ldots,S_k.\)</span> Define <span class="math inline">\(k\)</span> vectors <span class="math inline">\(v_1,\ldots,v_k\)</span> s.t. <span class="math inline">\(v_i\)</span>(j) = <span class="math inline">\(1/\sqrt{|S_i|}\)</span> if <span class="math inline">\(j \in S_i\)</span>, and <span class="math inline">\(0\)</span> otherwise. For <span class="math inline">\(i=1,\ldots,k,\)</span> it holds that <span class="math inline">\(||v_i||=1\)</span>. Additionally, for <span class="math inline">\(i \neq j\)</span>, because the sets <span class="math inline">\(S_i,S_j\)</span> are disjoint, <span class="math inline">\(\langle v_i,v_j \rangle = 0.\)</span> Finally, note that <span class="math inline">\(L v_i = 0\)</span>. Hence there is a set of <span class="math inline">\(k\)</span> orthonormal vectors that are all eigenvectors of <span class="math inline">\(L\)</span>, with eigenvalue 0.</p>
<p>To see that the number of 0 eigenvalues is at most the number of connected components of <span class="math inline">\(G\)</span>, note that since <span class="math inline">\(v^t L v = \sum_{i&lt;j, (i,j) \in E} (v(i)-v(j))^2,\)</span> this expression can only be zero if <span class="math inline">\(v\)</span> is constant on every connected component. To see that there is no way of finding a <span class="math inline">\(k+1st\)</span> vector <span class="math inline">\(v\)</span> that is a zero eigenvector, orthogonal to <span class="math inline">\(v_1,\ldots,v_k\)</span> observe that any eigenvector, <span class="math inline">\(v\)</span> must be nonzero in some coordinate, hence assume that <span class="math inline">\(v\)</span> is nonzero on a coordinate in set <span class="math inline">\(S_i\)</span>, and hence is nonzero and constant on all indices in set <span class="math inline">\(S_i\)</span>, in which case <span class="math inline">\(v\)</span> can not be orthogonal to <span class="math inline">\(v_i\)</span>., and there can be no <span class="math inline">\(k+1st\)</span> eigenvector with eigenvalue 0.</p>
<h2 id="intuition-of-lowest-and-highest-eigenvalueseigenvectors">Intuition of lowest and highest eigenvalues/eigenvectors</h2>
<p>To think about the smallest eigenvalues and their associated eigenvectors, it is helpful to come back to the fact that <span class="math inline">\(v^t L v = \sum_{(i,j) \in E, i&lt; j}(v(i)-v(j))^2\)</span> is the sum of the squares of the distances between neighbors. Hence the eigenvectors corresponding to the lowest eigenvalues correspond to vectors for which neighbors have similar values. Eigenvectors with eigenvalue 0 are constant on each connected component, and the smallest nonzero eigenvector will be the vector that minimizes this squared distance between neighbors, subject to being a unit vector that is orthogonal to all zero eigenvectors. That is, the eigenvector corresponding to the smallest <em>non-zero</em> eigenvalue will be minimizing this sum of squared distances, subject to the constraint that the values are somewhat spread out on each connected component.</p>
<p>Analogous reasoning applies to the maximum eigenvalue. The maximum eigenvalue <span class="math inline">\(\max_{||v||=1} v^t L v\)</span> will try to maximize the discrepancy between neighbors’ values of <span class="math inline">\(v\)</span>.</p>
<p>To get a more concrete intuition for what the small/large eigenvalues and their associated eigenvectors look like, you should try to work out (either in your head, or using a computer) the eigenvectors/values for lots of common graphs (e.g. the cycle, the path, the binary tree, the complete graph, random graphs, etc.) See the example/figure below [After the assignment is due, I will add the examples from the pset to these lecture notes.]</p>
<div class="figure">
<embed src="figs2.eps" />
<p class="caption">Spectral embeddings of the cycle on 20 nodes (top two figures), and the <span class="math inline">\(20\times 20\)</span> grid (bottom two figures). Each figure depicts the embedding, with the red lines connecting points that are neighbors in the graph. The left two plots show the embeddings onto the eigenvectors corresponding to the second and third smallest eigenvalues; namely, the <span class="math inline">\(i\)</span>th node is plotted at the point <span class="math inline">\(\left(v_2(i),v_3(i)\right)\)</span> where <span class="math inline">\(v_k\)</span> is the eigenvector corresponding to the <span class="math inline">\(k\)</span>th largest eigenvalue. The right two plots show the embeddings onto the eigenvectors corresponding to the largest and second–largest eigenvalues. Note that in the left plots, neighbors in the graph are close to each other. In the right plots, most points end up far from all their neighbors.<span data-label="fig1"></span></p>
</div>
<h1 id="applications-of-spectral-graph-theory">Applications of Spectral Graph Theory</h1>
<p>We briefly describe several problems for which considering the eigenvalues/eigenvectors of a graph Laplacian proves useful.</p>
<h2 id="visualizing-a-graph-spectral-embeddings">Visualizing a graph: Spectral Embeddings</h2>
<p>Suppose one is given a list of edges for some graph. What is the right way of visualizing, or drawing the graph so as to reveal its structure? Ideally, one might hope to find an embedding of the graph into <span class="math inline">\({\mathbb{R}}^d\)</span>, for some small <span class="math inline">\(d =2,3,\ldots\)</span>. That is, we might hope to associate some point, say in 2 dimension, to each vertex of the graph. Ideally, we would find an embedding that largely respects the structure of the graph. What does this mean? One interpretation is simply that we hope that most vertices end up being close to most of their neighbors.</p>
<p>Given this interpretation, embedding a graph onto the eigenvectors corresponding to the small eigenvalues seems extremely natural. Recall that the small eigenvalues correspond to unit vectors <span class="math inline">\(v\)</span> that try to minimize the quantity <span class="math inline">\(v^t L v = \frac{1}{2} \sum_{(i,j) \in E} \left(v(i)-v(j) \right)\)</span>, namely, these are the vectors that are trying to map neighbors to similar values. Of course, if the graph has a single connected component, the smallest eigenvector <span class="math inline">\(v_1 = (1/\sqrt{n},\ldots,1/\sqrt{n})\)</span>, which is not helpful for embedding, as all points have the same value. In this case, we should start by considering <span class="math inline">\(v_2\)</span>, then <span class="math inline">\(v_3\)</span>,et.</p>
<p>While there are many instances of graphs for which the spectral embedding does not make sense, it is a very natural first thing to try. As Figure [fig1] illustrates, in some cases the embedding onto the smallest two eigenvectors really does correspond to the “natural”/“intuitive” way to represent the graph in <span class="math inline">\(2\)</span>-dimensions.</p>
<h2 id="spectral-clusteringpartitioning">Spectral Clustering/Partitioning</h2>
<p>How can one find large, insular clusters in large graphs? How can we partition the graph into several large components so as to minimize the number of edges that cross between the components? These questions questions arise naturally in the study of any large networks (social networks, protein interaction networks, semantic/linguistic graphs of word usage, etc.)</p>
<p>In section [blah] we will discuss some specific metrics for the quality of a given cluster or partition of a graph, and give some quantitative bounds on these metrics in terms of the second eigenvalue of the graph Laplacian.</p>
<p>For the time being, just understand the intuition that the eigenvectors corresponding to small eigenvalues are, in some sense, trying to find good partitions of the graph. These low eigenvectors are trying to find ways of assigning different numbers to vertices, such that neighbors have similar values. Additionally, since they are all orthogonal, each eigenvectors is trying to find a “different”/“new” such partition. This is the intuition why it is often a good idea to look at the clusters/partitions suggested by a few different small eigenvectors. [This point is especially clear on the next homework.]</p>
<h2 id="graph-coloring">Graph Coloring</h2>
<p>Consider the motivating example of allocation one of <span class="math inline">\(k\)</span> different radio bandwidths to each radio station. If two radio stations have overlapping regions of broadcast, then they cannot be assigned the same bandwidth (otherwise there will be interference for some listeners). On the other hand, if two stations are very far apart, they can broadcast on the same bandwidth without worrying about interference from each other. This problem can be modeled as the problem of <span class="math inline">\(k\)</span>-coloring a graph. In this case, the nodes of the graph will represent radio stations, and there will be an edge between two stations if their broadcast ranges overlap. The problem then, is to assign one of <span class="math inline">\(k\)</span> colors (i.e. one of <span class="math inline">\(k\)</span> broadcast frequencies) to each vertex, such that no two adjacent vertices have the same color. This problem arises in several other contexts, including various scheduling tasks (in which, for example, tasks are mapped to processes, and edges connect tasks that cannot be performed simultaneously).</p>
<p>This problem of finding a <span class="math inline">\(k\)</span>-color, or even deciding whether a <span class="math inline">\(k\)</span>-coloring of a graph exists, is NP-hard in general. One natural heuristic, which is motivated by the right column of Figure [fig1], is to embed the graph onto the eigenvectors corresponding to the <em>highest</em> eigenvalues. Recall that these eigenvectors are trying to make vertices as different from their neighbors as possible. As one would expect, in these embeddings, points that are close together in the embedding tend to <em>not</em> be neighbors in the original graph. Hence one might imagine a <span class="math inline">\(k\)</span>-coloring heuristic as follows: 1) plot the embedding of the graph onto the top 2 or 3 eigenvectors of the Laplacian, and 2) locally partition the points in this space into <span class="math inline">\(k\)</span> regions, e.g. using <span class="math inline">\(k\)</span>-means, or a <span class="math inline">\(kd\)</span>-tree, and 3) assign all the points in each region the same color. Since neighbors in the graph will tend to be far apart in the embeddings, this will tend to give a decent coloring of the graph.</p>
<h1 id="blah">Conductance, isoperimeter, and the second eigenvalue</h1>
<p>There are several natural metrics for quantifying the quality of a graph partition. We will state two such metrics. First, it will be helpful to define the <em>boundary</em> of a partition:</p>
<p>Given a graph <span class="math inline">\(G = (V,E)\)</span>, and a set <span class="math inline">\(S \subset V\)</span>, the <em>boundary</em> of <span class="math inline">\(S\)</span>, denoted <span class="math inline">\(\delta(S)\)</span> is defined to be the set of edges of <span class="math inline">\(G\)</span> with exactly one endpoint in set <span class="math inline">\(S\)</span>.</p>
<p>One natural characterization of the quality of a partition <span class="math inline">\((S,V \backslash S)\)</span> is the <em>isoperimetric ratio</em> of the set, defined to be the ratio of the size of the boundary of <span class="math inline">\(S\)</span> to the minimum of <span class="math inline">\(|S|\)</span> and <span class="math inline">\(|V \backslash S|:\)</span></p>
<p>The <em>isoperimetric ratio</em> of a set <span class="math inline">\(S\)</span>, denoted <span class="math inline">\(\theta(S)\)</span>, is defined as <span class="math display">\[\theta(S) = \frac{|\delta(S)|}{\min (|S|,|V \backslash S|)}.\]</span> The <em>isoperimetric number</em> of a graph <span class="math inline">\(G\)</span> is defined as <span class="math inline">\(\theta_G = \min_{S \subset V} \theta(S).\)</span></p>
<p>A related notion of the quality of a partition, is the <em>conductance</em>, which is the ratio of the size of the boundary <span class="math inline">\(\delta(S)\)</span> to the minimum of the number of edges involved in <span class="math inline">\(S\)</span> and the number involved in <span class="math inline">\(V \backslash S\)</span> (where an edge is double-counted if both its endpoints lie within the set). Formally, this is the following:</p>
<p>The <em>conductance</em> of a partition of a graph into two sets, <span class="math inline">\(S, V \backslash S\)</span>, is defined as <span class="math display">\[cond(S) = \frac{|\delta(S)|}{\min (\sum_{i \in S} degree(i), \sum_{i \not \in S} degree(i) )}.\]</span></p>
<h2 id="connections-with-lambda_2">Connections with <span class="math inline">\(\lambda_2\)</span></h2>
<p>There are connections between the second eigenvalue of a graph’s Laplacian and both the isoperimetric number as well as the minimum conductance of the graph. The following surprisingly easily proved theorem makes the first connection rigorous:</p>
<p>[thm:is] Given any graph <span class="math inline">\(G=(V,E)\)</span> and any set <span class="math inline">\(S \subset V\)</span>, <span class="math inline">\(\theta(S) \ge \lambda_2 (1-\frac{|S|}{|V|}).\)</span> In other words, the isoperimetric number of the graph satisfies: <span class="math display">\[\theta_{G} \ge \lambda_2 (1-\frac{|S|}{|V|}).\]</span></p>
<p>Given a set <span class="math inline">\(S\)</span>, consider the associated vectors <span class="math inline">\(v_S\)</span> defined as follows: <span class="math display">\[v_S(i) =  \begin{cases} 1-\frac{|S|}{|V|} &amp;\mbox{if } i \in S\\ -\frac{|S|}{|V|}  &amp; \mbox{if } i \not \in S \end{cases}\]</span></p>
<p>We now compute <span class="math inline">\(\frac{v_S^t L v_S}{v_S^t v}\)</span>. First, we consider the numerator: <span class="math display">\[v_S^t L v_s = \sum_{i&lt;j:(i,j) \in E}\left(v_S(i)-v_S(j)\right)^2  = |\delta(S)|,\]</span> since the only terms in this sum that contribute a non-zero term correspond to edges with exactly one endpoint in set <span class="math inline">\(S\)</span>, namely the boundary edges. We now calculate <span class="math inline">\(v_S^t v_S = |S|(1-\frac{|S|}{|V|})^2 + (|V|-|S|)(\frac{|S|}{|V|})^2 = |S|(1-\frac{|S|}{|V|}).\)</span></p>
<p>Hence we have shown that <span class="math inline">\(\frac{v_S^t L v_S}{v_S^t v_S} = \frac{|\delta(S)|}{|S|(1-\frac{|S|}{|V|})}.\)</span> On the other hand, we also know that <span class="math inline">\(\sum_i v_S(i) = 0\)</span>, hence <span class="math inline">\(\langle v_S, (1,1\ldots,1)\rangle = 0.\)</span> Recall that <span class="math display">\[\lambda_2 = \min_{v:\langle v, (1,\ldots,1) \rangle=0} \frac{v^t L v}{v^t v} \le \frac{v_S^t L v_S}{v_S^t v_S} = \frac{|\delta(S)|}{|S|(1-\frac{|S|}{|V|})}.\]</span> Multiplying both sides of this inequality by <span class="math inline">\((1-\frac{|S|}{|V|})\)</span> yields the theorem.</p>
<p>What does Theorem [thm:is] actually mean? It says that if <span class="math inline">\(\lambda_2\)</span> is large (say, some constant bounded away from zero), then <em>all</em> small sets <span class="math inline">\(|S|\)</span> have <em>lots</em> of outgoing edges—linear in <span class="math inline">\(|S|\)</span>. For example, if <span class="math inline">\(\lambda_2 \ge 1/2\)</span>, then for all sets <span class="math inline">\(S\)</span> with <span class="math inline">\(|S| \le |V|/4,\)</span> we have that <span class="math inline">\(\theta(S) \ge \frac{1}{2}(1-1/4) = 3/8\)</span>, which implies that <span class="math inline">\(|\delta(S)| \ge \frac{3}{8}|S|.\)</span></p>
<p>There is a partial converse to Theorem [thm:is], known as <em>Cheeger’s Inequality</em>, which argues that if <span class="math inline">\(\lambda_2\)</span> is small, then there exists at least one set <span class="math inline">\(S\)</span>, such that the conductance of the set <span class="math inline">\(S\)</span> is also small. Cheeger’s inequality is usually formulated in terms of the eigenvalues of the <em>normalized Laplacian</em> matrix,defined by normalizing entry <span class="math inline">\(L(i,j)\)</span> by <span class="math inline">\(1/\sqrt{deg(i)\cdot deg(j)}\)</span>. Rather than formally defining the normalized Laplacian, we will simply state the theorem for regular graphs (graphs where all nodes have the same degree):</p>
<p>If <span class="math inline">\(\lambda_2\)</span> is the second smallest eigenvalue of the Laplacian of a <span class="math inline">\(d\)</span>-regular graph <span class="math inline">\(G = (V,E),\)</span> then there exists a set <span class="math inline">\(S \subset V\)</span> such that <span class="math display">\[cond(S) \le \frac{\sqrt{2 \lambda_2}}{\sqrt{d}}.\]</span></p>
<h1 id="random-walks-mixing-times-and-the-second-eigenvalue">Random walks, mixing times, and the second eigenvalue</h1>
<p>coming soon.</p>

{% endraw %}
