---
layout: post
title: Lecture 5
author: Tim
---
{% raw %}

<h1 id="preamble">Preamble</h1>
<p>Gradient descent is an extremely simple algorithm — simpler than most of the algorithms you studied in CS161 — that has been around for centuries. These days, the main “killer app” is machine learning. Model-fitting often reduces to optimization — for example, maximizing the likelihood of observed data over a family of generative models. A remarkably large fraction of modern machine learning research, including some of the much-hyped recent work on “deep learning,” boils down to implementing variants of gradient descent on a very large scale (i.e., for huge training sets). Indeed, the choice of models in many machine learning applications is driven as much by computational considerations — whether or not gradient descent can be implemented quickly — as by any other criteria.</p>
<p>The first goal of this lecture is to develop the geometry and intuition behind gradient descent, to the point that the algorithm seems totally obvious. The second goal is to make the general method concrete with a case study on linear regression. (The method is also very useful for many other problems.) Next lecture we’ll talk about extensions to the basic method and the basic problem formulation that will bring you a step closer to the state-of-the-art in modern machine learning.</p>
<p>Throughout this lecture, you might want to keep Figure [f:gddemo] in mind. The figures show a succession of lines that are an increasingly good fit for a collection of points in the plane. “Linear regression” just means computing the best-fitting line, and this succession of lines is generated by successive iterations of gradient descent. This is not all supposed to make 100% sense yet, of course — the rest of the lecture explains what’s going on — but this example should provide you with a concrete picture to refer back to as the lecture proceeds.</p>
<h1 id="how-to-think-about-gradient-descent">How to Think About Gradient Descent</h1>
<h2 id="unconstrained-optimization">Unconstrained Optimization</h2>
<p>Viewed the right way, gradient descent is really easy to understand and remember — more so than most algorithms. First off, what problem is gradient descent trying to solve? <span> <em>Unconstrained optimization</em></span>, meaning that for a given real-valued function <span class="math inline">\(f:\RR^n \rightarrow \RR\)</span> defined on <span class="math inline">\(n\)</span>-dimensional Euclidean space, the goal is <span class="math display">\[\min f(\w)\]</span> subject to <span class="math display">\[\w \in \RR^n.\]</span> Note that maximizing a function falls into this problem definition, since maximizing <span class="math inline">\(f\)</span> is the same as minimizing <span class="math inline">\(-f\)</span>. In this lecture, we’ll always assume that <span class="math inline">\(f\)</span> is differentiable and hence also continuous.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> For example, in our linear regression case study (Section [s:lr]), <span class="math inline">\(\w\)</span> will encode a linear prediction function and <span class="math inline">\(f\)</span> the mean-squared error of the function.</p>
<h2 id="ss:1">Warm-Up #1: <span class="math inline">\(n=1\)</span></h2>
<p>Suppose first that <span class="math inline">\(n=1\)</span>, so <span class="math inline">\(f:\RR \rightarrow \RR\)</span> is a univariate real-valued function. We can visualize the graph of <span class="math inline">\(f\)</span> in the usual way; see Figures [f:convex] and [f:uni]. The intuition we’ll develop for gradient descent in this simple case gives a surprisingly accurate picture of what’s going on in the general case (for any number of dimensions).</p>
<p>What would it mean to try to minimize <span class="math inline">\(f\)</span> via greedy local search? For example, in the parabola in Figure [f:convex], if we start at the point <span class="math inline">\(x_0\)</span>, then we look to the right (<span class="math inline">\(f\)</span> goes up) and to the left (<span class="math inline">\(f\)</span> goes down), and go further to the left. (Recall we want to make <span class="math inline">\(f\)</span> as small as possible.) If we start at <span class="math inline">\(x_1\)</span>, then <span class="math inline">\(f\)</span> is decreasing to the right and increasing to the left, so we’d move further to the right. In either case, the algorithm terminates at the bottom of the basin.</p>
<p>A little more formally — we’ll be precise when we discuss the general case — the basic algorithm in the <span class="math inline">\(n=1\)</span> case is the following:</p>
<ol>
<li><p>while <span class="math inline">\(f&#39;(x) \neq 0\)</span></p>
<ol>
<li><p>if <span class="math inline">\(f&#39;(x) &gt; 0\)</span> — so <span class="math inline">\(f\)</span> is increasing — then move <span class="math inline">\(x\)</span> a little to the left;</p></li>
<li><p>if <span class="math inline">\(f&#39;(x) &lt; 0\)</span> then move <span class="math inline">\(x\)</span> a little to the right.</p></li>
</ol></li>
</ol>
<p>Note that at each step, the derivative of <span class="math inline">\(f\)</span> is used to decide which direction to move in. Intuitively, we’re releasing a ball at some point of the graph of the function of <span class="math inline">\(f\)</span>, and the algorithm terminates at the final resting point of this ball (after gravity has done its work).</p>
<p>Not all functions are as nice as the parabola in Figure [f:convex], however. Consider the function shown in Figure [f:uni]. If we start at point <span class="math inline">\(x_0\)</span>, then we go left and wind up at the bottom of the left basin. If we start at <span class="math inline">\(x_1\)</span>, then we go right and the algorithm terminates at the bottom of the right basin.</p>
<div class="figure">
<img src="basins" alt="With a non-convex objective function, gradient descent can yield different local minima with different start states." />
<p class="caption">With a non-convex objective function, gradient descent can yield different local minima with different start states.<span data-label="f:uni"></span></p>
</div>
<p>Our two examples exhibit two obvious differences.</p>
<ol>
<li><p>In Figure [f:convex], no matter which starting point is chosen, the termination point of the algorithm remains the same. In Figure [f:uni], where you end up depends on where you start.</p></li>
<li><p>In Figure [f:convex], the algorithm always terminates at the global minimum. In Figure [f:uni], the algorithm might terminate at a local minimum — meaning there’s no way to improve <span class="math inline">\(f\)</span> by moving a little bit in either direction — that is not a global minimum.</p></li>
</ol>
<p>What is it about the functions that lead to these differences? The answer is <span><em>convexity</em></span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> A function is convex if all chords of its graph only lie above the graph. It is visually clear that the function in Figure [f:convex] is convex while the function in Figure [f:uni] is not. Mathematically, a function is convex if and only if <span class="math display">\[f(\tfrac{1}{2}\w+\tfrac{1}{2}\z) \le \underbrace{\tfrac{1}{2}f(\w) +
  \tfrac{1}{2}f(\z)}_{\text{midpoint of chord between $f(\w)$ and $f(\z)$}}\]</span> for every <span class="math inline">\(\w,\z \in \RR^n\)</span>. That is, for points <span class="math inline">\(\w\)</span> and <span class="math inline">\(\z\)</span>, if you take the average of <span class="math inline">\(\w\)</span> and <span class="math inline">\(\z\)</span> and then apply <span class="math inline">\(f\)</span>, you’ll get a smaller number than if you first apply <span class="math inline">\(f\)</span> to <span class="math inline">\(\w\)</span> and <span class="math inline">\(\z\)</span> and then average the results. A seemingly stronger but in fact equivalent condition is <span class="math display">\[\label{eq:convex}
\underbrace{f(\lambda \w+(1-\lambda)\z)}_{\text{graph}} \le
\underbrace{\lambda f(\w) + (1-\lambda)f(\z)}_{\text{point on chord}}\]</span> for all <span class="math inline">\(\w,\z \in \RR^n\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>. Note that these definitions makes sense for any function <span class="math inline">\(f:\RR^n
\rightarrow \RR\)</span>, not just in the <span class="math inline">\(n=1\)</span> case. It’s not always easy to check whether or not a given function is convex, but there is a mature analytical toolbox for this purpose (taught in EE364, for example).</p>
<p>Already with <span class="math inline">\(n=1\)</span> and in Figure [f:uni], we observed that with a non-convex function <span class="math inline">\(f\)</span>, gradient descent can compute a local minimum that is worse (i.e., larger) than a global minimum. The converse also holds: if <span class="math inline">\(f\)</span> is convex, then gradient descent can only terminate at a global minimum.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> To see this, suppose <span class="math inline">\(x\)</span> is sub-optimal and <span class="math inline">\(x^*\)</span> is optimal. As <span class="math inline">\(\lambda\)</span> goes from 0 to 1, the expression <span class="math inline">\(\lambda f(x^*) + (1-\lambda) f(x)\)</span> changes linearly from <span class="math inline">\(f(x)\)</span> to <span class="math inline">\(f(x^*)\)</span>. Inequality  then implies that moving toward <span class="math inline">\(x^*\)</span> from <span class="math inline">\(x\)</span> can only decrease <span class="math inline">\(f\)</span>. Thus gradient descent will not get stuck at <span class="math inline">\(x\)</span>.</p>
<h2 id="ss:linear">Warm-Up #2: Linear Functions</h2>
<p>In almost all of the applications of gradient descent, the number <span class="math inline">\(n\)</span> of dimensions is much larger than <span class="math inline">\(1\)</span>. Already with <span class="math inline">\(n=2\)</span> we see an immediate complication: from a point <span class="math inline">\(\w \in \RR^n\)</span>, there’s an infinite number of directions in which we could move, not just 2.</p>
<p>To develop our intuition, we first consider the rather silly case of <span><em>linear functions</em></span>, meaning functions of the form <span class="math display">\[\label{eq:linear}
f(\w) = \c^T\w + b,\]</span> where <span class="math inline">\(\c \in \RR^n\)</span> is an <span class="math inline">\(n\)</span>-vector and <span class="math inline">\(b \in \RR\)</span> is a scalar. Unconstrained minimization of a linear function is a trivial problem, because (assuming <span class="math inline">\(\c \neq 0\)</span>) it is possible to make the objective function arbitrarily negative. To see this, take any vector <span class="math inline">\(\w\)</span> with negative inner product <span class="math inline">\(\c^T\w &lt; 0\)</span> with <span class="math inline">\(\c\)</span> (such as <span class="math inline">\(-\c\)</span>) and consider points of the form <span class="math inline">\(\beta \w\)</span> for <span class="math inline">\(\beta\)</span> arbitrarily large. Suppose you are currently at a point <span class="math inline">\(\w \in \RR^n\)</span>, and you are allowed to move at most one unit of Euclidean distance in whatever direction you want. Where should you go to decrease the function <span class="math inline">\(f\)</span> in  as much as possible, and how much will the function decrease? To answer this, let <span class="math inline">\(\u \in \RR^n\)</span> be a unit vector; moving from <span class="math inline">\(\w\)</span> one unit of distance in the direction <span class="math inline">\(\u\)</span> changes the objective function as follows: <span class="math display">\[\begin{aligned}
\label{eq:linear2}
\c^T\w + b &amp; \mapsto &amp; \c^T(\w+\u) + b\\
&amp; = &amp; \c^T\w + b + \c^T\u\\ \label{eq:linear3}
&amp; = &amp; \underbrace{\c^T\w+b}_{\text{independent of $\u$}} 
+ \|\c||_2\underbrace{\|\u\|_2}_{=1} \cos \theta,\end{aligned}\]</span> where <span class="math inline">\(\theta\)</span> denotes the angle between the vectors <span class="math inline">\(\c\)</span> and <span class="math inline">\(\u\)</span>. To decrease <span class="math inline">\(f\)</span> as much as possible, we see that we should make <span class="math inline">\(\cos
\theta\)</span> as small as possible (i.e., -1), which we do by choosing <span class="math inline">\(\u\)</span> to point in the opposite direction of <span class="math inline">\(\c\)</span> (i.e., <span class="math inline">\(\u = -\c/\|\c\|_2\)</span>). The derivation – shows that moving one unit in this direction causes <span class="math inline">\(f\)</span> to decrease by <span class="math inline">\(\|\c\|_2\)</span>, so <span class="math inline">\(\|\c\|_2\)</span> is also the rate of decrease (per unit moved) in the direction <span class="math inline">\(-\|\c\|_2\)</span>. These are the things to remember about this warm-up example: <span><em>the direction of steepest descent is that of <span class="math inline">\(-\c\)</span>, for a rate of decrease of <span class="math inline">\(\|\c\|_2\)</span></em></span>.</p>
<h2 id="ss:calc">Some Calculus, Revisited</h2>
<p>What about general (differentiable) functions, the ones we really care about? The idea is to <span><em>reduce general functions to linear functions</em></span>. This might sound ridiculous, given how simple linear functions are and how weird general functions can be, but basic calculus already gives a method for doing this. What it really means for a function to be differentiable at a point is that it can be locally approximated at that point by a linear function. For a univariate differentiable function, like in Figure [f:uni], it’s clear what the linear approximation is — just use the tangent line. That is, at the point <span class="math inline">\(x\)</span>, approximate the function <span class="math inline">\(f\)</span> for <span class="math inline">\(y\)</span> near <span class="math inline">\(x\)</span> by the linear function <span class="math display">\[f(y) \approx f(x) + (y-x)f&#39;(x) = \underbrace{f(x) -
  xf&#39;(x)}_{\text{$y$-intercept}} 
+ y\underbrace{f&#39;(x)}_{\text{slope}},\]</span> where <span class="math inline">\(x\)</span> is fixed and <span class="math inline">\(y\)</span> is the variable. It’s also clear that the tangent line is only a good approximation of <span class="math inline">\(f\)</span> locally — far away from <span class="math inline">\(x\)</span>, the value of <span class="math inline">\(f\)</span> and this linear function have nothing to do with each other. Thus being differentiable means that at each point there exists a good local approximation by a linear function, with the specific linear function depending on the choice of point.</p>
<p>Another way to think about this, which has the benefit of extending to better approximations via higher-degree polynomials, is through Taylor expansions. Recall what Taylor’s Theorem says (for <span class="math inline">\(n=1\)</span>): if all of the derivatives of a function <span class="math inline">\(f\)</span> exist at a point <span class="math inline">\(x\)</span>, then for all sufficiently small <span class="math inline">\(\eps &gt; 0\)</span> we can write <span class="math display">\[\label{eq:taylor}
f(x+\eps) = \underbrace{f(x) + \eps \cdot f&#39;(x)}_{\text{linear approx.}}
+ \tfrac{\eps^2}{2!} \cdot f&#39;&#39;(x)
+ \tfrac{\eps^3}{3!} \cdot f&#39;&#39;(x)
+ \cdots.\]</span> So what? The point is that with the first two terms on the right-hand side of , we have a linear approximation of <span class="math inline">\(f\)</span> around <span class="math inline">\(x\)</span> staring us in the face (in the variable <span class="math inline">\(\eps\)</span>). This is the same as the tangent line approximation, with <span class="math inline">\(\eps\)</span> playing the role of <span class="math inline">\(y-x\)</span>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> The discussion so far has been for the <span class="math inline">\(n=1\)</span> case for simplicity, but everything we’ve said extends to an arbitrary number <span class="math inline">\(n\)</span> of dimensions. For example, the Taylor expansion  remains valid in higher dimensions, just with the derivatives replaced by their higher dimensional analogs. Since we’ll use only linear approximations, we only need to care about the higher-dimensional analog of the first derivative <span class="math inline">\(f&#39;(x)\)</span>, which is the gradient.</p>
<p>Recall that for a differentiable function <span class="math inline">\(f:\RR^n \rightarrow \RR\)</span>, the <span><em>gradient</em></span> <span class="math inline">\(\grad f(\w)\)</span> of <span class="math inline">\(f\)</span> at <span class="math inline">\(\w\)</span> is the real-valued <span class="math inline">\(n\)</span>-vector <span class="math display">\[\label{eq:grad}
\grad f(\w) = \left( \frac{\del f}{\del w_1}(\w), \frac{\del f}{\del w_2}(\w), \ldots, \frac{\del f}{\del w_n}(\w) \right)\]</span> in which the <span class="math inline">\(i\)</span>th component specifies the rate of change of <span class="math inline">\(f\)</span> as a function of <span class="math inline">\(w_i\)</span>, holding the other <span class="math inline">\(n-1\)</span> components of <span class="math inline">\(\w\)</span> fixed.</p>
<p>To relate this definition to our two-warm ups, note that if <span class="math inline">\(n=1\)</span>, then the gradient becomes the scalar <span class="math inline">\(f&#39;(x)\)</span>. If <span class="math inline">\(f(\w) = \c^T\w + b\)</span> is linear, then <span class="math inline">\(\del f/\del w_i = c_i\)</span> for every <span class="math inline">\(i\)</span> (no matter <span class="math inline">\(\w\)</span> is), so <span class="math inline">\(\grad f\)</span> is just the constant function everywhere equal to <span class="math inline">\(\c^T\)</span>.</p>
<p>For a simple but slightly less trivial example, we can consider a quadratic function <span class="math inline">\(f:\RR^n \rightarrow \RR\)</span> of the form <span class="math display">\[f(\w) = \frac{1}{2} \w^T\A\w - \b^T\w,\]</span> where <span class="math inline">\(\A\)</span> is an <span class="math inline">\(n \times n\)</span> matrix and <span class="math inline">\(\b\)</span> is an <span class="math inline">\(n\)</span>-vector. Expanding, we have <span class="math display">\[f(\w) = \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_{ij}w_iw_j - \sum_{i=1}^n b_iw_i,\]</span> and you should check that <span class="math display">\[\frac{\del f}{\del w_i}(\w) = \sum_{j=1}^n a_{ij}w_j - b_i\]</span> for each <span class="math inline">\(i=1,2,\ldots,n\)</span>. We can therefore express the gradient succinctly as <span class="math display">\[\grad f(\w) = \A\w - \b\]</span> at each point <span class="math inline">\(\w \in \RR^n\)</span>.</p>
<p>We’ll see another explicit gradient computation below, when we apply gradient descent to a linear regression problem. For more complex functions <span class="math inline">\(f\)</span>, it’s not always clear how to compute the gradient of <span class="math inline">\(f\)</span>. But as long as one can evaluate <span class="math inline">\(f\)</span>, one can estimate <span class="math inline">\(\grad f\)</span> by estimating each partial derivative in the definition  in the obvious way — changing one coordinate a little bit and seeing how much <span class="math inline">\(f\)</span> changes.</p>
<h2 id="ss:gd">Gradient Descent: The General Case</h2>
<p>Here is the general gradient descent algorithm. It has three parameters — <span class="math inline">\(\w_0\)</span>, <span class="math inline">\(\eps\)</span>, and <span class="math inline">\(\alpha\)</span> — which we’ll elaborate on shortly.</p>
<p>initialize <span class="math inline">\(\w := \w_0\)</span></p>
<p>And that’s it!</p>
<p>It’s also worth zooming in to see what the update rule  looks like in some coordinate, say the <span class="math inline">\(j\)</span>th one: <span class="math display">\[\label{eq:update2}
w_j := w_j - \alpha \cdot \frac{\del f}{\del w_j}(\w).\]</span> The update  can be thought of as <span class="math inline">\(n\)</span> updates of the form  being done in parallel (one per coordinate <span class="math inline">\(j\)</span>).</p>
<p>Conceptually, gradient descent enters the following contract with basic calculus:</p>
<ol>
<li><p>Calculus promises that, for <span class="math inline">\(\z\)</span> close to <span class="math inline">\(\w\)</span>, one can pretend that the true function <span class="math inline">\(f\)</span> is just the linear function <span class="math inline">\(f(\w) +
  \grad f(\w)^T(\z - \w)\)</span> (Section [ss:calc]). We know what to do with linear functions: move in the opposite direction of the coefficient vector — that is, in the direction <span class="math inline">\(-\grad f(\w)\)</span> — to locally decrease the function at a rate of <span class="math inline">\(\|\grad f(\w)\|_2\)</span> per unit distance (Section [ss:linear]).</p></li>
<li><p>In exchange, gradient descent promises to only take a small step (parameterized by the step size <span class="math inline">\(\alpha\)</span>) away from <span class="math inline">\(\w\)</span>. Taking a large step would violate the agreement, in that far away from <span class="math inline">\(\w\)</span> the function <span class="math inline">\(f(\w)\)</span> need not behave anything like the local linear approximation <span class="math inline">\(f(\w) +               
  \grad f(\w)^T(\z - \w)\)</span> (recall the tangent lines in Figure [f:uni]).</p></li>
</ol>
<p>The starting point <span class="math inline">\(\w_0\)</span> can be chosen arbitrarily, though as we saw in Section [ss:1], for non-convex <span class="math inline">\(f\)</span> the output of gradient descent can vary with the choice of <span class="math inline">\(\w_0\)</span>. For convex functions <span class="math inline">\(f\)</span>, gradient descent will converge toward the same point — the global minimum — no matter how the starting state is chosen.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> The choice of start state can still affect the number of iterations until convergence, however. In practice, one should choose <span class="math inline">\(\w_0\)</span> according to your best guess as to where the global minimum is likely be — generally, the closer <span class="math inline">\(\w_0\)</span> is to the global minimum of <span class="math inline">\(f\)</span>, the faster the convergence of gradient descent.</p>
<p>The parameter <span class="math inline">\(\eps\)</span> determines the stopping rule. Note that because <span class="math inline">\(\eps &gt; 0\)</span>, gradient descent generally does not halt at an actual local minimum, but rather at some kind of “approximate local minimum.”<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> Since the rate of decrease of a given step is <span class="math inline">\(\|\grad f(\w) \|_2\)</span>, at least locally, once <span class="math inline">\(\|\grad f(\w) \|_2\)</span> gets close to 0 each iteration of gradient descent makes very little progress; this is an obvious time to quit. Smaller values of <span class="math inline">\(\eps\)</span> mean more iterations before stopping but a higher-quality solution at termination. In practice, one tries various values of <span class="math inline">\(\eps\)</span> to achieve the right balance between computation time and solution quality. Alternatively, one can just run gradient descent for a fixed amount of time and use whatever point was computed in the final iteration.</p>
<p>The final parameter <span class="math inline">\(\alpha\)</span>, the “step size,” is perhaps the most important. While gradient descent is flexible enough that different <span class="math inline">\(\alpha\)</span>’s can be used in different iterations, in practice one typically uses a fixed value of <span class="math inline">\(\alpha\)</span> over all iterations.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> While there is some nice theory that gives advice on how to choose <span class="math inline">\(\alpha\)</span> as a function of the “niceness” of <span class="math inline">\(f\)</span>, in practice the “best” value of <span class="math inline">\(\alpha\)</span> is typically chosen by experimentation. The very first time you’re exploring some function <span class="math inline">\(f\)</span>, one option is a “line search,” which just means identifying by binary search the value of <span class="math inline">\(\alpha\)</span> that minimizes <span class="math inline">\(f\)</span> over the line <span class="math inline">\(\w - \alpha \cdot
\grad f(\w)\)</span>. After a few line searches, you should have a decent guess as to a good value of <span class="math inline">\(\alpha\)</span>. Alternatively, you can run the entire gradient descent algorithm with a few different choices of <span class="math inline">\(\alpha\)</span> to see which run gives you the best results.</p>
<h1 id="s:lr">Application: Linear Regression</h1>
<p>A remarkable amount of modern machine learning boils down to variants of gradient descent. This section illustrates how to apply gradient descent to one of the simplest non-trivial machine learning problems, namely linear regression.</p>
<h2 id="linear-regression">Linear Regression</h2>
<p>In linear regression, the input is <span class="math inline">\(m\)</span> data points <span class="math inline">\(\x^{(1)},\x^{(2)},\ldots,\x^{(m)} \in \RR^n\)</span>, each an <span class="math inline">\(n\)</span>-dimensional vector. Also given is a real-valued “label” <span class="math inline">\(y^{(i)} \in \RR\)</span> for each data point <span class="math inline">\(i\)</span>.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> For example, each data point <span class="math inline">\(i\)</span> could correspond to a 5th-grade student, <span class="math inline">\(y^{(i)}\)</span> could correspond to the score earned by that student on some standardized test, and <span class="math inline">\(\x^{(i)}\)</span> could represent the values of <span class="math inline">\(n\)</span> different “features” of student <span class="math inline">\(i\)</span> — the average household income in his/her neighborhood, the number of years of education earned by his/her parents, etc.</p>
<p>The goal is to compute the “best” linear relationship between the <span class="math inline">\(\x^{(i)}\)</span>’s and the <span class="math inline">\(y^{(i)}\)</span>’s. That is, we want to compute a linear function <span class="math inline">\(h:\RR^n \rightarrow \RR\)</span> such that <span class="math inline">\(h(\x^{(i)}) \approx
y^{(i)}\)</span> for every <span class="math inline">\(i\)</span>. Every such linear function <span class="math inline">\(h\)</span> can be written as <span class="math display">\[h_{\w}(\x) = w_0 + \sum_{j=1}^n w_jx_j\]</span> for real-valued coefficients <span class="math inline">\(w_0,w_1,\ldots,w_n\)</span>. We can simplify the notation by giving every data point a “dummy zeroth coordinate” equal to 1. Then, the coefficient of the dummy coordinate plays the role of the intercept <span class="math inline">\(w_0\)</span>. From now on, we assume that the data points have been preprocessed in this way, and that the coordinates are named <span class="math inline">\(\{1,2,\ldots,n\}\)</span>. We then associate <span class="math inline">\(\w \in \RR^n\)</span> with the linear function <span class="math display">\[\label{eq:h}
h_{\w}(\x) = \sum_{j=1}^n w_jx_j.\]</span></p>
<p>The two most common motivations for computing a “best-fit” linear function are prediction and data analysis. In the first scenario, one uses the given “labeled data” (the <span class="math inline">\(\x^{(i)}\)</span>’s and <span class="math inline">\(y^{(i)}\)</span>’s) to identify a linear function <span class="math inline">\(h\)</span> that, at least for these data points, does a good job of predicting the label <span class="math inline">\(y^{(i)}\)</span> from the feature values <span class="math inline">\(\x^{(i)}\)</span>. The hope is that this linear function “generalizes,” meaning that it also makes accurate predictions for other data points for which the label is not already known. There is a lot of beautiful and useful theory in statistics and machine learning about when one can and cannot expect a hypothesis to generalize, which you’ll learn about if you take courses in those areas. In the second scenario, the goal is to understand the relationship between each feature of the data points and the labels, and also the relationships between the different features. As a simple example, it’s clearly interesting to know when one of the <span class="math inline">\(n\)</span> features is much more strongly correlated with the label <span class="math inline">\(y^{(i)}\)</span> than any of the others.</p>
<h2 id="mean-squared-error-mse">Mean Squared Error (MSE)</h2>
<p>To complete the formal problem description, we need to choose a notion of “best fit.” We’ll use the most common one, that of minimizing the mean squared error (MSE) of a linear function. For a linear function <span class="math inline">\(h_{\w}\)</span> with coefficient vector <span class="math inline">\(\w \in \RR^{n}\)</span>, this is defined as <span class="math display">\[\label{eq:mse}
\MSE(\w) = \frac{1}{m} \sum_{i=1}^m E_i(\w)^2\]</span> where the “error” or “residual” <span class="math inline">\(E_i(\w)\)</span> is the difference between <span class="math inline">\(h_{\w}\)</span>’s “prediction” <span class="math inline">\(h_{\w}(\x^{(i)})\)</span> for the <span class="math inline">\(i\)</span>th data point and the “correct answer” <span class="math inline">\(y^{(i)}\)</span>: <span class="math display">\[\label{eq:ei}
E_i(\w) = h_{\w}(\x^{(i)}) - y^{(i)}.\]</span> There are a couple of reasons to choose the MSE objective function.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> One is that, as we’ll see, the function has several nice mathematical and computational properties. The function also has a satisfying Bayesian justification: if the data is such that each label <span class="math inline">\(y^{(i)}\)</span> is generated from <span class="math inline">\(\x^{(i)}\)</span> by applying a linear function <span class="math inline">\(h_{\w}\)</span> and then adding independent Gaussian noise to each data point, then minimizing the MSE is equivalent to the problem of maximizing (over linear functions) the likelihood of the data.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<p>Since we want to minimize the mean-squared error, our function <span class="math inline">\(f:\RR^{n} \rightarrow \RR\)</span> is that in .<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> In this minimization problem, the variables are the coefficients <span class="math inline">\(\w\)</span> of the linear function <span class="math inline">\(h_{\w}\)</span> — all of the data points (the <span class="math inline">\(\x^{(i)}\)</span>’s) and labels (the <span class="math inline">\(y^{(i)}\)</span>’s) are given as input and fixed forevermore.</p>
<p>One nice property of the MSE is that it is a convex function of its variables <span class="math inline">\(\w\)</span>. The rough argument is: each function <span class="math inline">\(E_i(\w)\)</span> is linear in <span class="math inline">\(\w\)</span>, and linear functions are convex; taking the square only makes these functions “more convex;” and the sum  of convex functions is again convex. In particular, the only local minimum of the MSE function is the global minimum.</p>
<h2 id="the-gradient-of-the-mse-function">The Gradient of the MSE Function</h2>
<p>One approach to computing the linear function with minimum-possible MSE is to apply gradient descent. To see what this would look like, let’s compute the gradient of the MSE function  — it turns out to be quite nice and interpretable. Recall that derivatives are linear — for example, <span class="math inline">\((g+h)&#39; = g&#39;+h&#39;\)</span>. Since  has one term per data point <span class="math inline">\(i=1,2,\ldots,m\)</span>, the gradient will also have one term per data point. This is a key point: the fact that the gradient separates over the data points is a big reason why gradient descent can scale to very machine learning problems.</p>
<p>The term for the <span class="math inline">\(i\)</span>th data point is, by the chain rule of calculus, <span class="math display">\[\grad (E_i(\w))^2
= 2E_i(\w) \cdot \grad E_i(\w).\]</span> Inspecting  and , we have <span class="math display">\[\frac{\del E_i}{\del w_j} = x^{(i)}_j\]</span> for <span class="math inline">\(j=1,2,\ldots,n\)</span>, and hence <span class="math inline">\(\grad E_i(\w) = \x\)</span>. Putting it all together, we have <span class="math display">\[\label{eq:msegrad}
\grad f(\w) = \frac{2}{m} \sum_{i=1}^m
\left( \underbrace{E_i(\w)}_{\text{scalar}} \cdot 
\underbrace{\x^{(i)}}_{\text{$n$-vector}} \right),\]</span> where <span class="math inline">\(f(\w)\)</span> denotes the MSE of the linear function <span class="math inline">\(h_{\w}\)</span>.</p>
<p>The gradient  has a natural interpretation. The <span class="math inline">\(i\)</span>th term <span class="math inline">\(E_i(\w) \cdot \x^{(i)}\)</span> can be thought of as the <span class="math inline">\(i\)</span>th data point’s “opinion” as to how the coefficients <span class="math inline">\(\w\)</span> should be updated. To see this, first note that if we move from <span class="math inline">\(\w\)</span> in the direction of <span class="math inline">\(\x^{(i)}\)</span>, then the prediction of the current linear function <span class="math display">\[h_{\w}(\x^{(i)}) = \w^T\x^{(i)}\]</span> increases at rate <span class="math inline">\(\|\x^{(i)}\|^2_2\)</span>.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> Similarly, if we move in the direction of <span class="math inline">\(-\x^{(i)}\)</span>, then the prediction of the current linear function on <span class="math inline">\(\x^{(i)}\)</span> decreases at this rate. Thus, if <span class="math inline">\(E_i(\w) &lt; 0\)</span>, so that the prediction <span class="math inline">\(h_{\w}(\x^{(i)})\)</span> of the current linear function underestimates the correct value <span class="math inline">\(y^{(i)}\)</span>, then data point <span class="math inline">\(i\)</span>’s “vote” is to move in the direction of <span class="math inline">\(\x^{(i)}\)</span> (increasing <span class="math inline">\(h_{\w}\)</span>’s prediction), at a rate proportional to the magnitude <span class="math inline">\(|E_i(\w)|\)</span> of the current error. Similarly, if <span class="math inline">\(E_i(\w) &gt; 0\)</span>, then data point <span class="math inline">\(i\)</span> votes for changing <span class="math inline">\(\w\)</span> in the direction that would decrease <span class="math inline">\(h_{\w}\)</span>’s prediction for <span class="math inline">\(\x^{(i)}\)</span> as rapidly as possible. Every data point has its own opinion, and the gradient  just averages these opinions. In addition to be conceptually transparent, computing this gradient is straightforward, requiring <span class="math inline">\(O(mn)\)</span> time. And since  is a just a sum of terms, one per data point, the computation is easy to parallelize. The data set can be spread over however many machines or cores are available, the summands can be computed independently, and then the results are aggregated together.</p>
<h1 id="s:sum">Lecture Take-Aways</h1>
<p>After the course, the following would be a good list of things to remember about gradient descent.</p>
<ol>
<li><p>The goal of gradient descent is to minimize a function via greedy local search.</p></li>
<li><p>Gradient descent scales well to large data sets, especially with some tweaks (covered next lecture) and if an approximately optimal solution is good enough. For example, the algorithm doesn’t even need to multiply matrices. This is the primary reason for the algorithm’s renaissance in the 21st century, driven by large-scale machine learning applications.</p></li>
<li><p>Gradient descent provably solves many convex problems. (Some problems of interest are convex, like linear regression, while others are not.)</p></li>
<li><p>Gradient descent can be an unreasonably good heuristic for the approximate solution of non-convex problems; this is one of the main points of Mini-Project #3.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Gradient descent requires gradients! Well, actually it doesn’t — there are extensions of gradient descent that relax the differentiability assumption, but we won’t have time to discuss them.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Your math classes may or may not have emphasized the central importance of convexity. But a good rule of thumb, especially in optimization, is to equate convexity with “niceness,” including efficient solvability.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Modulo any approximation error from stopping before the derivative is exactly zero; see Section [ss:gd] for details.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Note the analogy with some of our “lossy compression” solutions — we’re throwing out as much information as possible subject to some type of approximation guarantee.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Strictly speaking, this is true only for functions that are “strictly convex” is some sense. We’ll gloss over this distinction in this lecture.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>If the function <span class="math inline">\(f\)</span> is sufficiently nice — “strongly convex” is most common sufficient condition — then gradient descent provably terminates at a point very close to a global minimum.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>For example, one could imagine decreasing <span class="math inline">\(\alpha\)</span> over the course of the algorithm, a la simulated annealing. But in the common case where the norm <span class="math inline">\(\|\grad f\|_2\)</span> of <span class="math inline">\(f\)</span> decreases each iteration, then even with a fixed <span class="math inline">\(\alpha\)</span>, the distance traveled by gradient descent each iteration is already decreasing.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>This is an example of <span><em>supervised</em></span> learning, in that the input includes the “correct answers” <span class="math inline">\(y^{(1)},\ldots,y^{(m)}\)</span> for the data points <span class="math inline">\(\x^{(1)},\ldots,\x^{(m)}\)</span>, as opposed to just the data points alone (which would be an <span><em>unsupervised</em></span> learning problem).<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Next lecture we’ll look at some important variations.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Even though reality may not conform to the precise assumption of independent Gaussian noise, a result like this provides evidence that this approach should give good results quite generally.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>Omitting the normalizing term <span class="math inline">\(1/m\)</span> in  results in an equivalent optimization problem. Note that the best step size to use in gradient descent will depend on whether or not this term in included.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>In more detail, going from <span class="math inline">\(\w\)</span> to <span class="math inline">\(\w + \gamma \x^{(i)}\)</span> means we go from <span class="math inline">\(h_{\w}(\x^{(i)}) =  \w^T\x^{(i)}\)</span> to <span class="math inline">\(h_{(\w+\gamma \x^{(i)})}(\x^{(i)}) =  (\w+\gamma \x^{(i)})^T\x^{(i)} =
\w^T\x^{(i)} + \gamma (\x^{(i)})^T \x^{(i)}
= \w^T\x^{(i)} + \gamma \|\x^{(i)}\|_2^2\)</span>.<a href="#fnref12">↩</a></p></li>
</ol>
</div>

{% endraw %}
