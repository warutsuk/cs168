---
layout: post
title: Lecture 5
author: Tim
---
{% raw %}

<h1 id="data-sets-as-random-samples">Data Sets as Random Samples</h1>
<p>When analyzing a data set, you can be in one of two different modes. In the first mode, you really care about understanding the data set at hand. For example, in sociology, often the main point of a research project is to understand deeply the peculiarities of a single data set (e.g., the friendship structure among the 34 members of a karate club <span class="citation"></span>).</p>
<p>In the second mode, you care about the data set at hand only inasmuch as it allows you to learn something more general—to extrapolate accurately to additional data. For example, think about the results of a medical trial (“treatment X helped 37% of the subjects while a placebo helped 25%”). The implicit assumption (or hope) behind any such trial is that the results for the trial subjects are representative of what would happen for the entire population. In this and the next lecture, we’ll be solidly in this second mode.</p>
<ul>
<li><p><strong>Today’s Viewpoint:</strong> data points are random samples from some underlying population (i.e., distribution).</p></li>
</ul>
<p>In the medical trial example, the distribution would be the uniform distribution over possible trial subjects (from which the actual subjects are assumed to be random samples).</p>
<p>For another example you might have seen before, consider the problem of spam detection—given an email, label it as spam or not. You might have done the following project in a different class: given “labeled training data,” meaning emails classified (by hand) as either spam or not, use an algorithm (a “learning” or “training” algorithm) to compute a “prediction function,” whose job is to offer an opinion as to whether or not an arbitrary (perhaps never-before-seen) email is spam. Again, the reason you spend time with the data set of labeled emails is not because you want to understand them per se, but rather to learn how to extrapolate correctly to emails not in data set.</p>
<p>In this course, you’ll see plenty of lectures in each mode. We’ll return to the distributional viewpoint in future lectures on techniques for sampling and estimation. Last week (e.g., with k-d trees or dimensionality reduction), and the upcoming weeks on linear algebraic techniques, are more in the first mode.</p>
<h1 id="binary-classification">Binary Classification</h1>
<p>There are many different types of learning problems. For concreteness, we’ll illustrate our points using <span><em>binary classification problems</em></span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Here are the ingredients of the model:</p>
<ol>
<li><p>Data points correspond to points in <span class="math inline">\(d\)</span>-dimensional Euclidean space <span class="math inline">\(\RR^d\)</span>. (For example, the vector of word frequencies in an email, for <span class="math inline">\(d\)</span> different words that you’re keeping track of.)</p></li>
<li><p>There is a “ground truth” function <span class="math inline">\(f:\RR^d \rightarrow
  \{0,1\}\)</span> specifying the correct classification of every possible data point. (E.g., whether or not each possible email is actually spam, or not.)<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> <span><em>The function <span class="math inline">\(f\)</span> is initially unknown, and is what we want to learn.</em></span></p></li>
<li><p>There is an unknown distribution <span class="math inline">\(D\)</span> on <span class="math inline">\(\RR^d\)</span> (the origin of our training data, see below). For example, you can think of <span class="math inline">\(D\)</span> as the uniform distribution over a very large finite subset of <span class="math inline">\(\RR^d\)</span> (e.g., all emails with <span class="math inline">\(\le 10,000\)</span> characters that could conceivably be received).</p></li>
</ol>
<p>Here’s the problem solved by a learning algorithm:<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<ul>
<li><p><strong>Input:</strong> <span class="math inline">\(n\)</span> data points <span class="math inline">\(\x_1,\ldots,\x_n \in
  \RR^d\)</span>, with each <span class="math inline">\(\x_i\)</span> drawn independently and identically (“i.i.d.”) from the distribution <span class="math inline">\(D\)</span>, and the corresponding ground truth labels <span class="math inline">\(f(\x_1),\ldots,f(\x_n) \in \{0,1\}\)</span>. The <span class="math inline">\(\x_i\)</span>’s (and their labels) constitute the <span><em>training data</em></span>. For examples, the <span class="math inline">\(\x_i\)</span>’s could be emails, with each email correctly labeled by a human as spam or not.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p></li>
<li><p><strong>Output:</strong> The job of the learning algorithm is to output a <span><em>prediction function</em></span> <span class="math inline">\(g:\RR^d \rightarrow \zo\)</span>, its “best guess” as to what the ground truth <span class="math inline">\(f\)</span> is.</p></li>
<li><p><strong>Success Criterion:</strong> the prediction function <span class="math inline">\(g\)</span> is identical to the ground truth function <span class="math inline">\(f\)</span>. (Or later in lecture, <span class="math inline">\(g\)</span> is at least “close” to <span class="math inline">\(f\)</span>.)</p></li>
</ul>
<p>Figure [f:ex] shows an example with <span class="math inline">\(d=2\)</span>, with <span class="math inline">\(n\)</span> data points labeled as “+” or “-.” The learning algorithm needs to label the entire plane with “+”s and “-”s, ideally with 100% accuracy (with respect to the ground truth <span class="math inline">\(f\)</span>). Figure [f:ex] also shows a simple example of what a ground truth function <span class="math inline">\(f\)</span> might look like—a line through the origin (or hyperplane in higher dimensions), with all “+”s on one side and all “-”s on the other. (We’ll return to such “linear classifiers” later in the lecture, see Section [s:lc].)</p>
<p>Again, we emphasize that <span class="math inline">\(f\)</span> is initially unknown, with the learning algorithm acting as a detective who is trying to reconstruct <span class="math inline">\(f\)</span>. The learning algorithm does receive some clues about <span class="math inline">\(f\)</span>, specifically its evaluation at <span class="math inline">\(n\)</span> different data points (the <span class="math inline">\(\x_i\)</span>’s). The prediction function <span class="math inline">\(g\)</span> is the algorithm’s extrapolation of <span class="math inline">\(f\)</span> from these <span class="math inline">\(n\)</span> data points to all of the data points (included the never-before-seen ones). “Learning” means extrapolating correctly, in that <span class="math inline">\(g\)</span> should coincide with <span class="math inline">\(f\)</span>.</p>
<p>Are there any learning algorithms that actually achieve this goal? It depends, with two parameters of the application being crucial.</p>
<ul>
<li><p><span><em>The amount of data</em></span> (i.e., <span class="math inline">\(n\)</span>). The more data you have, the more you know about the ground truth function <span class="math inline">\(f\)</span>, and the better your prospects for figuring it out.</p></li>
<li><p><span><em>The number of possible functions that might be the ground truth.</em></span> The fewer functions that you’re trying to distinguish between, the easier the learning problem.</p></li>
</ul>
<h1 id="training-error-test-error-and-generalization">Training Error, Test Error, and Generalization</h1>
<p>How do we assess the “goodness” of a prediction function <span class="math inline">\(g\)</span>? What we really care about is the <span><em>test error</em></span>, defined as the probability that <span class="math inline">\(g\)</span> disagrees with the ground truth <span class="math inline">\(f\)</span> on the label of a random sample (from the underlying distribution <span class="math inline">\(D\)</span>): <span class="math display">\[\text{test error($g$)} = {\text{\bf Pr}\ifthenelse{\not\equal{}{\x \sim D}}{_{\x \sim D}}{}\!\left[g(\x) \neq f(\x)\right]}.\]</span> Our learning goal can be rephrased as identifying a function <span class="math inline">\(g\)</span> with 0% test error (or later in lecture, with very small test error).</p>
<p>We’re not in a position to evaluate the test error of a prediction function, as we know neither the ground truth <span class="math inline">\(f\)</span> nor the distribution <span class="math inline">\(D\)</span>. What we do know is <span class="math inline">\(n\)</span> sample points and their ground truth labels, and it’s natural to use <span class="math inline">\(g\)</span>’s performance on these as a proxy for <span class="math inline">\(g\)</span>’s test error. This proxy is called the <span><em>training error</em></span> of <span class="math inline">\(g\)</span> (with respect to a sample <span class="math inline">\(\x_1,\ldots,\x_n\)</span> with ground truth labels <span class="math inline">\(f(\x_1),\ldots,f(\x_n)\)</span>): <span class="math display">\[\text{training error($g$)} = \frac{1}{n} \cdot [\text{number of
  $\x_i$&#39;s with $g(\x_i) \neq f(\x_i)$}].\]</span></p>
<p>For any given prediction function <span class="math inline">\(g\)</span>, its expected training error (over the random sample <span class="math inline">\(\x_1,\ldots,\x_n\)</span>) is exactly its test error.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> But is the training error of <span class="math inline">\(g\)</span> on a random sample very likely to be close to its test error, or might there be a big discrepancy? As a special case of this, is a function <span class="math inline">\(g\)</span> with 0% training error likely to also have 0% test error?</p>
<p>Such questions are usually phrased as: does <span class="math inline">\(g\)</span> <span><em>generalize</em></span>? The answer depends, at the least on the amount of training data <span class="math inline">\(n\)</span> (with more being better), and in some cases also on the learning algorithm used to come up with the prediction function <span class="math inline">\(g\)</span> (see Mini-Project #3). When a learning algorithm outputs a prediction function <span class="math inline">\(g\)</span> with test error much higher than its training error, then it is said to have <span><em>overfit</em></span> the training data. In this case, the prediction function learned is largely an artifact of the particular sample, and does not capture the more fundamental patterns in the data distribution.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<h1 id="s:warmup">Analysis: The Well-Separated Finite Case</h1>
<h2 id="assumptions">Assumptions</h2>
<p>To develop our intuition, we first consider the learning problem with two additional assumptions:</p>
<ul>
<li><p><span><em>(Finite)</em></span> The ground truth function <span class="math inline">\(f\)</span> belongs to a known set <span class="math inline">\(\{ f_1,\ldots,f_h\}\)</span> of <span class="math inline">\(h\)</span> different functions. That is, there are only <span class="math inline">\(h\)</span> different possibilities for what <span class="math inline">\(f\)</span> might be, and we know the options up front.</p></li>
<li><p><span><em>(Well-separated)</em></span> No function (other than <span class="math inline">\(f\)</span>) is extremely close to <span class="math inline">\(f\)</span>, meaning that every candidate <span class="math inline">\(f_j \neq f\)</span> has test error at least <span class="math inline">\(\eps\)</span>. (Here <span class="math inline">\(\eps\)</span> is a parameter in our assumption, which would be e.g. 1% or 10%.)</p></li>
</ul>
<p>Later this lecture we will relax both of these assumptions.</p>
<h2 id="our-first-learning-algorithm">Our First Learning Algorithm</h2>
<p>Recall that our goal is to design a learning algorithm whose output <span class="math inline">\(g\)</span> is the same as the a priori unknown ground truth <span class="math inline">\(f\)</span> (with high probability over the sampled data points), i.e., a function <span class="math inline">\(g\)</span> with 0% test error. A simple but important point is that, if we ever identify a data point <span class="math inline">\(\x\)</span> such that <span class="math inline">\(g(\x) \neq f(\x)\)</span>, then we can rule out <span class="math inline">\(g\)</span> from future consideration (it can’t possibly be the same as <span class="math inline">\(f\)</span>). In particular, a necessary condition for <span class="math inline">\(g\)</span> to have 0% test error is that it has 0% training error (otherwise there is a training sample that proves that <span class="math inline">\(g \neq f\)</span>). This leads us to the first version of our basic learning algorithm:</p>
<p>Output an arbitrary function <span class="math inline">\(g \in \{f_1,\ldots,f_h\}\)</span> that has 0% training error.</p>
<p>How well does this algorithm work? Does its output <span class="math inline">\(g\)</span> generalize, in the sense of also having 0% test error? The answer depends on <span class="math inline">\(n\)</span>. For example, if <span class="math inline">\(n=1\)</span>, there are presumably lot of choices of <span class="math inline">\(f_j\)</span> that are correct on this one data point, and all but one of them have non-zero test error. So our goal is to identify a sufficient condition on the data set size <span class="math inline">\(n\)</span> such that the prediction function output by the learning algorithm generalizes (i.e., is always correct even on not-yet-seen data points).</p>
<h2 id="ss:single">Getting Tricked by a Single Function</h2>
<p>For the analysis, first consider a fixed function <span class="math inline">\(f_j\)</span> different from the true <span class="math inline">\(f\)</span>. We’re concerned about getting “tricked” by <span class="math inline">\(f_j\)</span>, meaning that it winds up having 0% training error (despite having test error at least <span class="math inline">\(\eps\)</span>, by (A2)). The probability of this bad event is <span class="math display">\[\begin{aligned}
{\text{\bf Pr}\ifthenelse{\not\equal{}{\x_1,\ldots,\x_n \sim D}}{_{\x_1,\ldots,\x_n \sim D}}{}\!\left[f_j(\x_i) = f(\x_i) \text{ for all
  $i=1,2,\ldots,n$}\right]}
&amp; = \prod_{i=1}^n {\text{\bf Pr}\ifthenelse{\not\equal{}{\x_i \sim D}}{_{\x_i \sim D}}{}\!\left[f_j(\x_i) = f(\x_i)\right]}\\
&amp; \le (1-\eps)^n\\
&amp; \le e^{-\eps n},\end{aligned}\]</span> where the equation follows from our assumption that the <span class="math inline">\(\x_i\)</span>’s are independent samples from <span class="math inline">\(D\)</span>, the first inequality follows from assumption (A2), and the last inequality follows because <span class="math inline">\(1+x \le
e^{x}\)</span> for all <span class="math inline">\(x \in \RR\)</span> (see Figure [f:exp], we’re taking <span class="math inline">\(x=-\eps\)</span>). As we hoped, this probability is decreasing (exponentially, in fact) with the number of samples <span class="math inline">\(n\)</span>.</p>
<div class="figure">
<embed src="exp2.pdf" />
<p class="caption">The inequality <span class="math inline">\(1+x \le e^x\)</span> holds for all real-valued <span class="math inline">\(x\)</span>.<span data-label="f:exp"></span></p>
</div>
<h2 id="the-union-bound">The Union Bound</h2>
<p>The computation above was just for a fixed incorrect prediction function <span class="math inline">\(f_j \neq f\)</span>, and we have <span class="math inline">\(h-1\)</span> such functions to worry about. To deal with this, recall the Union Bound, which should be familiar from CS109. The Union Bound just says that for events <span class="math inline">\(A_1,\ldots,A_h\)</span>, <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[\text{at least one of the $A_i$&#39;s occurs}\right]} \le \sum_{i=1}^h {\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[A_i\right]}.\]</span> Importantly, the events are completely arbitrary, and do not need to be independent. The proof is a one-liner. In terms of Figure [f:union], the union bound just says that the area (i.e., probability mass) in the union is bounded above by the sum of the areas of the circles. The bound is tight if the events are disjoint; otherwise the right-hand side is larger, due to double-counting the overlaps. In most applications, including the present one, the events <span class="math inline">\(A_1,\ldots,A_h\)</span> represent “bad events” that we’re hoping don’t happen; the union bound says that as long as each event occurs with low probability and there aren’t too many events, then with high probability none of them occur.</p>
<div class="figure">
<img src="f_union.png" alt="Union Bound: area of the union is bounded by the sum of areas of the circles." width="264" />
<p class="caption">Union Bound: area of the union is bounded by the sum of areas of the circles.<span data-label="f:union"></span></p>
</div>
<h2 id="completing-the-analysis">Completing the Analysis</h2>
<p>Let <span class="math inline">\(A_j\)</span> denote the event that an incorrect function <span class="math inline">\(f_j \neq f\)</span> has 0% training error on <span class="math inline">\(n\)</span> samples, despite having test error at least <span class="math inline">\(\eps\)</span>. From the derivation in Section [ss:single], we know that <span class="math inline">\({\text{\bf Pr}\ifthenelse{\not\equal{}{}}{_{}}{}\!\left[A_j\right]} \le e^{-\eps n}\)</span> for every <span class="math inline">\(j\)</span>. Since there are only <span class="math inline">\(h\)</span> different values of <span class="math inline">\(j\)</span> to consider (by assumption (A1)), the Union Bound of the previous section implies that <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{\x_1,\ldots,\x_n \sim D}}{_{\x_1,\ldots,\x_n \sim D}}{}\!\left[\text{some $f_j \neq f$ has 0\    training error}\right]} \le \sum_{j=1}^h e^{-\eps n} = h \cdot e^{-\eps n}.\]</span> Also, because our learning algorithm can only fail to output the ground truth <span class="math inline">\(f\)</span> when there is a function <span class="math inline">\(f_j \neq f\)</span> with 0% training error (otherwise the only remaining candidate is the correct answer <span class="math inline">\(f\)</span>), we have <span class="math display">\[{\text{\bf Pr}\ifthenelse{\not\equal{}{\x_1,\ldots,\x_n \sim D}}{_{\x_1,\ldots,\x_n \sim D}}{}\!\left[\text{output of learning algorithm
    is~$f$}\right]} \ge 1 - he^{-\eps n}.\]</span> That is, <span class="math inline">\(he^{-\eps n}\)</span> is an upper bound on the failure probability of our learning algorithm. This upper bound increases linearly with the number of possible functions (remember the learning problem is harder as you’re trying to differentiate between more functions) but decreases exponentially with the size of training data set.</p>
<p>So suppose you want a failure probability of at most <span class="math inline">\(\delta\)</span> (say, <span class="math inline">\(\delta=1\%\)</span>).<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> How much data is required? Setting <span class="math inline">\(he^{-\eps n} =
\delta\)</span> and solving for <span class="math inline">\(n\)</span>, we get the following sufficient condition for generalization.</p>
<p>[t:finite] Suppose assumptions (A1) and (A2) hold, and assume that <span class="math display">\[\label{eq:sc}
n \ge \frac{1}{\eps}\left( \ln h + \ln \frac{1}{\delta} \right).\]</span> Then with probability at least <span class="math inline">\(1-\delta\)</span> over the samples <span class="math inline">\(\x_1,\ldots,\x_n \sim D\)</span>, the output of the learning algorithm is the ground truth function <span class="math inline">\(f\)</span>.</p>
<p>The <span><em>sample complexity</em></span> of a learning task is the minimum number of i.i.d. samples necessary to accomplish it. Thus Theorem [t:finite] states that the right-hand side of  is an upper bound on the sample complexity of learning an unknown ground truth function with probability at least <span class="math inline">\(1-\delta\)</span> (under assumptions (A1) and (A2)). Let’s inspect this upper bound more closely. The not-so-good news is that the dependence on <span class="math inline">\(\tfrac{1}{\eps}\)</span> is linear. So to reduce <span class="math inline">\(\eps\)</span> from 10% to 1%, you need 10 times as much data (at least in theory). The much better news is that the dependence on <span class="math inline">\(h\)</span> and <span class="math inline">\(\tfrac{1}{\delta}\)</span> is only logarithmic, so there’s generally no problem with taking <span class="math inline">\(\delta\)</span> to be quite small (1% or even less), and large (even exponential-size) values of <span class="math inline">\(h\)</span> can be accommodated. For example, if we take <span class="math inline">\(\eps = 5\%\)</span>, <span class="math inline">\(\delta=1\%\)</span>, and <span class="math inline">\(h=1000\)</span>, then our bound on the sample complexity is something like 230. If we reduce <span class="math inline">\(\eps\)</span> to 1% then the bound shoots up past 1000, but if we reduce <span class="math inline">\(\delta\)</span> to 0.1% then the bound remains under 300.</p>
<h1 id="s:finite">PAC Guarantees: The Finite Case</h1>
<p>Now we extend our sample complexity bounds to hold under weaker assumptions. For starters, let’s drop assumption (A2) entirely while keeping assumption (A1) (i.e., a finite number of possible <span class="math inline">\(f\)</span>’s). The obstacle that then arises is functions <span class="math inline">\(f_j\)</span> with test error that is strictly positive but very close to 0. In this case, the learning algorithm is unlikely to ever see a data point to differentiate such an incorrect function <span class="math inline">\(f_j\)</span> from the ground truth <span class="math inline">\(f\)</span>, and might get tricked into outputting <span class="math inline">\(f_j\)</span> rather than <span class="math inline">\(f\)</span>. But if <span class="math inline">\(f_j\)</span> is almost the same as <span class="math inline">\(f\)</span> anyway, shouldn’t that be good enough for our purposes?</p>
<p>More formally, keep the learning algorithm exactly the same as in the previous section. Theorem [t:finite] then translates, without any change to the proof, to the following guarantee.</p>
<p>[t:pac] Suppose assumption (A1) holds, and assume that <span class="math display">\[n \ge \frac{1}{\eps}\left( \ln h + \ln \frac{1}{\delta} \right).\]</span> Then with probability at least <span class="math inline">\(1-\delta\)</span> over the samples <span class="math inline">\(\x_1,\ldots,\x_n \sim D\)</span>, the output of the learning algorithm is a function <span class="math inline">\(f_j\)</span> with test error less than <span class="math inline">\(\eps\)</span>.</p>
<p>Several comments. First, note the differences between the statements in Theorems [t:finite] and [t:pac]: the latter has one less assumption, but compromises by allowing the learning algorithm to output a function with small (but non-zero) test error. Note that the semantics of the parameter <span class="math inline">\(\eps\)</span> have changed: in Theorem [t:finite], <span class="math inline">\(\eps\)</span> depended on the <span class="math inline">\(f_j\)</span>’s (and <span class="math inline">\(D\)</span>) and was outside anyone’s control, while in Theorem [t:pac], <span class="math inline">\(\eps\)</span> is a user-specified parameter (in the same sense as <span class="math inline">\(\delta\)</span>), controlling the trade-off between the sample complexity and the error of the learned prediction function. The type of guarantee in Theorem [t:pac] is often called a <span><em>PAC</em></span> guarantee, which stands for “probably approximately correct” <span class="citation"></span>. (The “probably” refers to the probability <span class="math inline">\(\delta\)</span> of failure over the samples, and the “approximately” refers to the possible test error of <span class="math inline">\(\eps\)</span>.) Finally, note that the proof of Theorem [t:finite] also immediately implies Theorem [t:pac]: the analysis in Section [s:warmup] continues to apply to the functions <span class="math inline">\(f_j\)</span> with test error at least <span class="math inline">\(\eps\)</span>, so with probability at least <span class="math inline">\(1-\delta\)</span>, the only hypotheses eligible for having 0% training error are those with test error less than <span class="math inline">\(\eps\)</span>. Our learning algorithm then outputs an arbitrary such hypothesis.</p>
<h1 id="s:lc">PAC Guarantees: Linear Classifiers</h1>
<h2 id="the-need-for-assumptions">The Need for Assumptions</h2>
<p>We now consider relaxing the assumption that <span class="math inline">\(f\)</span> is one of a finite number of possibilities (assumption (A1)). It is important to note that this assumption cannot be dropped entirely. To see this, suppose the ground truth <span class="math inline">\(f\)</span> could be any function whatsoever from <span class="math inline">\(\RR^d\)</span> to <span class="math inline">\(\{0,1\}\)</span>. A learning algorithm gets no information about <span class="math inline">\(f\)</span> other than its value on <span class="math inline">\(n\)</span> sample points. If <span class="math inline">\(f\)</span> is unrestricted, then the labels of unseen points could be <span><em>literally anything</em></span>, and the learning algorithm cannot extrapolate at all from the training data to any other data points. In effect, the learning algorithm outputs a function that has memorized the training data (0% training error) and doesn’t generalize at all (large test error, say 50%). This is an extreme case of overfitting.</p>
<h2 id="linear-classifiers">Linear Classifiers</h2>
<p>So to move forward, we insist that the ground truth function <span class="math inline">\(f\)</span> is “structured,” meaning it takes on a particular form. In this lecture, we’ll focus on <span><em>linear classifiers</em></span>.</p>
<p>[d:lc] A <span><em>linear classifier in <span class="math inline">\(\RR^d\)</span></em></span> is specified by a <span class="math inline">\(d\)</span>-vector <span class="math inline">\(\a=(a_1,\ldots,a_d) \in \RR^d\)</span> of real coefficients, and is defined as the function <span class="math inline">\(f_{\a}:\RR^d \rightarrow \{0,1\}\)</span> with <span class="math display">\[\label{eq:lc}
f_{\a}((x_1,\ldots,x_d)) =
\left\{
\begin{array}{cl}
1 &amp; \text{if $\sum_{i=1}^d a_ix_i \ge 0$}\\
0 &amp; \text{if $\sum_{i=1}^d a_ix_i &lt; 0$.}
\end{array}
\right.\]</span></p>
<p>Note that a linear classifier in <span class="math inline">\(d\)</span> dimensions has <span class="math inline">\(d\)</span> degrees of freedom (the <span class="math inline">\(a_i\)</span>’s). Geometrically, a linear classifier corresponds to a hyperplane through the origin (with normal vector <span class="math inline">\(\a\)</span>), with all points on the same side of the hyperplane as the normal vector labeled “1” and all points on the other side labeled “0”. See Figure [f:lc]. Linear classifiers may feel like a toy example, but they’re not—“support vector machines (SVMs),” which are one of the most commonly used models in machine learning, are basically the same as linear classifiers.</p>
<h2 id="from-the-curse-of-dimensionality-to-generalization">From the Curse of Dimensionality to Generalization</h2>
<p>There are an infinite number of linear classifiers (even when <span class="math inline">\(d=1\)</span>), so Theorem [t:pac] does not directly apply to the problem of learning a good linear classifier from labeled training data. However, recall the discussion last week (Lecture #4) about the “curse of dimensionality” and kissing numbers of spheres. We saw that the number of “distinct directions” in <span class="math inline">\(\RR^d\)</span> grows exponentially (but not more) with <span class="math inline">\(d\)</span>.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> Last lecture, we viewed this as a negative result, which explained why many problems we care about (like computing nearest neighbors) become difficult in high dimensions. Today, we’re actually going to use this exponential dependence for a <span><em>positive</em></span> result. Namely, recall that our sample complexity bound in  for the finite case depended only <span> <em>logarithmically</em></span> on the number of relevant functions <span class="math inline">\(f_j\)</span>. So if all linear classifiers can be well approximated by a finite set of linear classifiers with size “only” exponential in <span class="math inline">\(d\)</span>, then we might hope that the sample complexity of learning a good classifier is only linear in <span class="math inline">\(d\)</span>. And this is in fact the case.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>[t:lc] Suppose <span class="math inline">\(f\)</span> is a linear classifier of the form in , and assume that <span class="math display">\[n \ge \frac{c}{\eps}\left( d + \ln \frac{1}{\delta} \right),\]</span> where <span class="math inline">\(c\)</span> is a sufficiently large constant. Then with probability at least <span class="math inline">\(1-\delta\)</span> over the samples <span class="math inline">\(\x_1,\ldots,\x_n \sim D\)</span>, the output of the learning algorithm is a linear classifier with test error less than <span class="math inline">\(\eps\)</span>.</p>
<p>The algorithm in Theorem [t:lc] is the same as before—just output an arbitrary linear classifier that has 0% training error. Current proofs of Theorem [t:lc] require the constant <span class="math inline">\(c\)</span> to be larger than one would like. In practice, a reasonable guideline is to think of <span class="math inline">\(c\)</span> as 1.</p>
<h2 id="a-rule-of-thumb">A Rule of Thumb</h2>
<p>Theorem [t:lc] leads to a surprisingly robust rule of thumb, which will serve you well in your future endeavors.</p>
<ul>
<li><p><span><strong>Upshot:</strong> </span> to guarantee generalization, make sure that your training data set size <span class="math inline">\(n\)</span> is at least linear in the number <span class="math inline">\(d\)</span> of free parameters in the function that you’re trying to learn.</p></li>
</ul>
<p>Mini-Project #3 will drive home this guideline—you’ll observe empirically that learned prediction functions tend to generalize when <span class="math inline">\(n \gg d\)</span> but not when <span class="math inline">\(n \ll d\)</span>.</p>
<p>Theorem [t:lc] makes the rule of thumb above precise for the problem of learning a linear classifier. For most other types of prediction functions that you might want to learn (e.g., in linear or logistic regression, or even with neural nets) there is a sensible notion of “number of parameters,” and the guideline above is usually pretty accurate.</p>
<h2 id="faq">FAQ</h2>
<p>Three questions you might have at this point are:</p>
<ol>
<li><p>How do you actually implement the basic learning algorithm?</p></li>
<li><p>What if, contrary to our standing assumption, <span><em>no</em></span> function under consideration has 0% training error?</p></li>
<li><p>What should you do if <span class="math inline">\(n \ll d\)</span>?</p></li>
</ol>
<p>The next two sections answer the first two questions; next lecture is devoted to answering the third.</p>
<h1 id="s:complexity">Computational Considerations</h1>
<p>How do we actually implement the basic learning algorithm? That is, given a bunch of correctly labeled data points, how do we compute a candidate function <span class="math inline">\(\hat{f}\)</span> with 0% training error. If there are only a finite number of candidates (as in Sections [s:warmup]–[s:finite]), then if nothing else we can resort to exhaustive search. (For some types of functions, there will also be faster algorithms.) But we can’t very well try each of the infinitely many linear classifiers, can we?</p>
<p>More formally, the relevant algorithmic problem for learning a good linear classifier is: given a number of “+”s and “-”s in <span class="math inline">\(\RR^d\)</span>, compute a hyperplane such that all of the “+”s are on one side and all of the “-” on the other (see Figure [f:separation]).<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<div class="figure">
<img src="f_separation.png" alt="We want to find a hyperplane that separates the positive points (plus signs) from the negative points (minus signs)" width="377" />
<p class="caption">We want to find a hyperplane that separates the positive points (plus signs) from the negative points (minus signs)<span data-label="f:separation"></span></p>
</div>
<p>One way to solve the problem is via linear programming, which we’ll discuss in Lecture #18. With this approach you can even maximize the “margin,” meaning the smallest Euclidean distance between a training point and the hyperplane. (This requires convex programming, which is still tractable and again will be discussed in Lecture #18.) This is one of the key ideas behind support vector machines (SVMs). Alternatively, one can apply iterative methods (like stochastic gradient descent, the perceptron algorithm, etc.).</p>
<h1 id="non-zero-training-error-and-the-erm-algorithm">Non-Zero Training Error and the ERM Algorithm</h1>
<p>What if no function <span class="math inline">\(\hat{f}\)</span> under consideration has 0% training error? For example, what if you restrict yourself to learning a linear classifier, but the ground truth is not actually a linear classifier?</p>
<h2 id="generalization-guarantees-for-the-erm-algorithm">Generalization Guarantees for the ERM Algorithm</h2>
<p>One solution is to extend the learning algorithm in the obvious way, to output the best of the available functions.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p>
<p>Output the function <span class="math inline">\(\hat{f}\)</span> (from the set of functions under consideration) that has the smallest training error, breaking ties arbitrarily.</p>
<p>This algorithm is often called the <span><em>ERM algorithm</em></span>, for “empirical risk minimization.” (Sounds fancy, but it’s really just the one-line algorithm above.)</p>
<p>The ERM algorithm has a PAC guarantee analogous to that of the basic learning algorithm, although with somewhat larger sample complexity. We state the guarantee for linear classifiers; the same result holds for arbitrary finite sets of functions (as before with the <span class="math inline">\(d\)</span> replaced by <span class="math inline">\(\ln h\)</span>, where <span class="math inline">\(h\)</span> is the number of candidate functions). The key behind the ERM’s generalization guarantee is the following theorem, which states that, with high probability after sufficiently many samples, the test error of <span><em>every</em></span> linear classifier is very close to its training error.</p>
<p>[t:erm] Assume that <span class="math display">\[\label{eq:sample}
n \ge \frac{c}{\eps^2}\left( d + \ln \frac{1}{\delta} \right),\]</span> where <span class="math inline">\(c\)</span> is a sufficiently large constant. Then with probability at least <span class="math inline">\(1-\delta\)</span> over the samples <span class="math inline">\(\x_1,\ldots,\x_n \sim D\)</span>, for every linear classifier <span class="math inline">\(\hat{f}\)</span>, <span class="math display">\[\label{eq:uc}
\text{test error of $\hat{f}$} \in \text{training error of $\hat{f}$}\pm\eps.\]</span></p>
<p>Note that Theorem [t:erm] does not assume that the ground truth <span class="math inline">\(f\)</span> is given by a linear classifier—<span class="math inline">\(f\)</span> could be anything. Of course, if <span class="math inline">\(f\)</span> looks nothing like any linear classifier, then all linear classifiers will have large test error and the guarantee in Theorem [t:erm] is not very relevant.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<p>A special case of Theorem [t:erm] is: if <span class="math inline">\(\hat{f}\)</span> has 0% training error, then it has test error at most <span class="math inline">\(\eps\)</span>. So Theorem [t:erm] generalizes Theorem [t:lc], with the caveat that its sample complexity larger by a <span class="math inline">\(\tfrac{1}{\eps}\)</span> factor.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a></p>
<p>Theorem [t:erm] is a sufficient condition for generalization, in the following sense.</p>
<p>[cor:erm] In the setting of Theorem [t:erm], let <span class="math inline">\(\tau \in [0,1]\)</span> denote the minimum test error of any linear classifier. Assume that <span class="math display">\[n \ge \frac{c}{\eps^2}\left( d + \ln \frac{1}{\delta} \right),\]</span> where <span class="math inline">\(c\)</span> is a sufficiently large constant. Then with probability at least <span class="math inline">\(1-\delta\)</span> over the samples <span class="math inline">\(\x_1,\ldots,\x_n \sim D\)</span>, the output <span class="math inline">\(\hat{f}\)</span> of the ERM algorithm satisfies <span class="math display">\[\text{test error of $\hat{f}$} \le \tau + 2\eps.\]</span></p>
<p>Thus with high probability, the ERM algorithm learns a linear classifier that is almost as accurate as the best linear classifier.</p>
<p><span>Corollary</span><span>cor:erm</span> Let <span class="math inline">\(f^*\)</span> denote the most accurate linear classifier, with test error <span class="math inline">\(\tau\)</span>. Let <span class="math inline">\(\hat{f}\)</span> denote the linear classifier returned by the ERM algorithm. Theorem [t:erm] implies that, with high probability, <span class="math display">\[\text{training error of $f^*$} \le \tau + \eps\]</span> and also <span class="math display">\[\text{training error of $\hat{f}$} \ge \text{test error of $\hat{f}$}-\eps.\]</span> Since the ERM algorithm picked <span class="math inline">\(\hat{f}\)</span> over <span class="math inline">\(f^*\)</span>, we must have <span class="math display">\[\text{training error of $\hat{f}$} \le \text{training error of $f^*$}.\]</span> Chaining together the three inequalities proves the corollary.</p>
<h2 id="sample-complexity-considerations">Sample Complexity Considerations</h2>
<p>There are two things that get worse when passing from the case where some function has 0% training error (called the <span><em>realizable case</em></span>) and the general case, however. First, the sample complexity bound in  is worse than that in  by a factor of <span class="math inline">\(\tfrac{1}{\eps}\)</span>. This gap is necessary, at least in theory. One way to develop intuition about the difference between the two scenarios is to think about two different experiments that involve repeatedly flipping a coin with unknown bias <span class="math inline">\(p\)</span>:</p>
<ol>
<li><p>Suppose I promise you that the bias is either a 0% chance of heads, or a 1% chance. If any of your coin flips come up heads, then you know you’re in the second case. What if you only observe tails, say <span class="math inline">\(k\)</span> in a row? What are the chances that the bias actually is 1%? If you observe 300 heads in a row, then you can be pretty confident that the bias is actually 0 (there’s only a roughly 5% chance of getting 300 consecutive tails when the bias is 1%). This thought experiment corresponds to original realizable setting, and a <span class="math inline">\(\tfrac{1}{\eps}\)</span> dependence in the sample complexity (where <span class="math inline">\(\eps\)</span> corresponds to 1%).</p></li>
<li><p>Now suppose you want to estimate the unknown bias <span class="math inline">\(p\)</span>, up <span class="math inline">\(\pm
  1\%\)</span>. Suppose you toss the coin 300 times, and see exactly 150 heads and 150 tails. Can you confidently conclude that <span class="math inline">\(p = 50\%
  \pm 1\%\)</span>? No — if <span class="math inline">\(p=52\%\)</span>, say, there’s a perfectly reasonable chance of seeing an equal split of heads and trails in 300 trials.<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> About all you can say is that <span class="math inline">\(p\)</span> is very likely to be between 40% and 60%. You would need more like 30,000 coin flips to be very confident about <span class="math inline">\(p\)</span> up to <span class="math inline">\(\pm 1\%\)</span>. This thought experiment corresponds to our general learning problem, and a sample complexity dependence on <span class="math inline">\(\tfrac{1}{\eps^2}\)</span>.</p></li>
</ol>
<p>Another way to think about the difference is the following. In the first experiment, there is “one-sided error;” you might mistake a 1%-bias coin for a 0%-bias coin (from a long run of tails), but never vice versa (once you see a heads you know with certainty that you’re correct). The second problem, of estimating the bias, has “two-sided error”—no matter what bias you guess, there’s a chance that your guess is too high, and also that it’s too low. Needing to hedge between two different types of error causes the sample complexity to jump by <span class="math inline">\(\tfrac{1}{\eps}\)</span>.</p>
<p>Despite all this, the dependence on <span class="math inline">\(d\)</span> in  remains linear, and our key take-away from before remains valid:</p>
<ul>
<li><p><span><strong>Upshot:</strong> </span> to guarantee generalization, make sure your training data set size <span class="math inline">\(n\)</span> is at least linear in the number <span class="math inline">\(d\)</span> of free parameters in the function that you’re trying to learn.</p></li>
</ul>
<p>Again, in practice, one usually sees pretty good generalization when <span class="math inline">\(n\)</span> is equal to or a small multiple of <span class="math inline">\(d\)</span>; see Mini-Project #3.</p>
<h2 id="computational-considerations">Computational Considerations</h2>
<p>The second complication in the general case, relative to the realizable case, concerns the complexity of implementing the ERM algorithm. In Section [s:complexity], we noted that there are various computationally efficient methods for checking if there is a linear classifier with 0% training error. The more general problem that the ERM algorithm needs to solve, of identifying the linear classifier with the minimum (possibly non-zero) training error, is unfortunately an <span class="math inline">\(NP\)</span>-hard problem. Because of this computational intractability (in both theory and practice), one has to resorts to heuristics. Many of these heuristics are based on the methods already mentioned for the realizable case (linear programming, stochastic gradient descent, etc.).</p>
<h1 id="recap-of-key-points">Recap of Key Points</h1>
<p>Here are some of the high-order bits to take away from this lecture:</p>
<ol>
<li><p>Data can be fruitfully thought of as samples from some underlying population or distribution.</p></li>
<li><p><span><em>Learning</em></span> means successfully extrapolating from labeled training data to as-yet-unseen data points (i.e., learning a prediction function with low test error).</p></li>
<li><p><span><em>Generalization</em></span> means that the observed training error of a prediction function is an accurate proxy for the quantity you really care about, its test error. With generalization guarantees, learning reduces to the problem of computing a prediction function with small training error.</p></li>
<li><p>A sufficient condition for generalization is for the data set size to exceed the number of free parameters in the prediction function being learned. We stated this formally for the case of linear classifiers, but this rule of thump is surprisingly robust across learning problems (e.g., linear and logistic regression, and even in many cases for neural nets).</p></li>
<li><p>In practice, you might be stuck with <span class="math inline">\(n\)</span> much smaller than <span class="math inline">\(d\)</span>. Next lecture discusses additional techniques that are useful for this case, including regularization.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The lessons learned in this lecture carry over to many other learning problems, such as linear and logistic regression.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Again, one can consider variations, with <span class="math inline">\(\{0,1\}\)</span> replaced by <span class="math inline">\([0,1]\)</span> (allowing “soft” classifications) or <span class="math inline">\(\RR\)</span>—the main points and results of this lecture remain the same.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This type of learning problem is called “batch” or “offline” learning, because all of the training data is available up front in a batch.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This type of problem, where the learning algorithm is given labeled examples, is called a <span><em>supervised</em></span> learning problem. Also important are <span><em>unsupervised</em></span> learning problems, where an algorithm is given unlabeled data and is responsible for identifying “interesting patterns.” We’ll talk more about unsupervised learning next week, when we discuss principal components analysis (PCA).<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>We have <span class="math inline">\({\text{\bf E}\ifthenelse{\not\equal{}{\x_1,\ldots,\x_n \sim D}}{_{\x_1,\ldots,\x_n \sim D}}{}\!\left[\text{training error($g$) on $\x_1,\ldots,\x_n$}\right]} =
  \tfrac{1}{n} \sum_{i=1}^n {\text{\bf Pr}\ifthenelse{\not\equal{}{\x_i \sim D}}{_{\x_i \sim D}}{}\!\left[g(\x_i) \neq f(\x_i)\right]} =
  \text{test error($g$)}\)</span>, where in the first inequality we’re using the linearity of expectation.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>In practice, one checks for generalization/overfitting by dividing the available data set into two parts, the “training set” and the “test set.” The learning algorithm is given the training set only, and the test error of the computed prediction function <span class="math inline">\(\hat{f}\)</span> is then defined as the fraction of test set data points that <span class="math inline">\(\hat{f}\)</span> labels incorrectly. This exercise more or less corresponds to the case where the distribution <span class="math inline">\(D\)</span> is uniform over the points in the data set.</p>
<p>If there is a significant difference between the training and test error of the output of the learning algorithm (generally with the latter much bigger than the former), then you’ve got a problem, and should consider gathering more data and/or using some of the techniques discussed in the next lecture to learn a better prediction function.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Note that there’s no hope of having failure probability 0. There’s always some chance, however remote, that you get a totally uninformative sample (e.g., <span class="math inline">\(n\)</span> copies of the same point).<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>For example, if you take <span class="math inline">\(\alpha^d\)</span> random points <span class="math inline">\(R\)</span> on the unit sphere in <span class="math inline">\(\RR^d\)</span>, where <span class="math inline">\(\alpha\)</span> is a sufficiently large constant, then with high probability, <span><em>every</em></span> unit vector will be in roughly the same direction as some point of <span class="math inline">\(R\)</span> (meaning the angle between them is less than <span class="math inline">\(1^{\circ}\)</span>). This fact can be proved using the formulas for the volume and surface area of a sphere in <span class="math inline">\(d\)</span>-dimensions—every random point “covers” a small but non-trivial sphere of directions around it, and eventually the whole unit sphere is covered.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>The proof sketch in the previous footnote leads to a slightly weaker bound that depends on <span class="math inline">\(d \log \tfrac{1}{\eps}\)</span> rather than <span class="math inline">\(d\)</span>. A more clever argument gets rid of the <span class="math inline">\(\log \tfrac{1}{\eps}\)</span> factor, see e.g. <span class="citation"></span>.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>We’re glossing over the distinction of whether or not the hyperplane has to go through the origin. It really doesn’t matter—hyperplanes in <span class="math inline">\(d\)</span> dimensions can be simulated by hyperplanes through the origin in <span class="math inline">\(d+1\)</span> dimensions (by adding a dummy coordinate with value “1” for every data point). More on this next lecture.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>A second solution (compatible also with the first) is to enrich your class of functions so that there <span><em>is</em></span> a function with 0% training error (or at least smaller than before). One systematic way of doing this, discussed in detail next lecture, is to use linear classifiers in a higher-dimensional space (which can correspond to non-linear classifiers in the original space). There are various ways of adding new dimensions, such as appending to each data point <span class="math inline">\((x_1,\ldots,x_d)\)</span> new coordinates containing quadratic terms in the original coordinates (<span class="math inline">\(x^2_1,\ldots,x_d^2,x_1x_2,x_1x_3,\ldots\)</span>).<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>This highlights two different types of errors in learning. The first type of error is <span><em>representation error</em></span>, where discrepancies between the ground truth and the set of allowed prediction functions cause all available functions to have large test error. No amount of algorithmic cleverness or data can reduce representation error; the only solution is to enrich the set of allowable prediction functions (see also the previous footnote and next lecture). The second type of error, which is the focus of this lecture, is <span><em>generalization error</em></span>, which comes from choosing a prediction function with training error much less than its test error (due to an overly small or unlucky sample).</p>
<p>These two types of errors connect to the “bias-variance” trade-off that you might hear about in a statistics course; enlarging the set of possible prediction functions tends to decrease the representation error (reducing “bias”) but increase generalization error (enlarging the “variance).”<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>The guarantee in  is sometimes called a “uniform convergence” result. “Convergence” refers to the fact that the training error of every classifier <span class="math inline">\(\hat{f}\)</span> approaches the test error (with high probability), while “uniform” refers to the fact that a single finite set of samples (with size as in ) suffices to give the guarantee in  simultaneously for all infinitely many linear classifiers.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>This can be verified by computing some binomial coefficients.<a href="#fnref14">↩</a></p></li>
</ol>
</div>

{% endraw %}
