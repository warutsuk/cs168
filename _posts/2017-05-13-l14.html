---
layout: post
title: "Lecture 14 : Markov Chain Monte Carlo"
author: Tim
---
{% raw %}

<p>The previous lecture covered several tools for inferring properties of the distribution that underlies a random sample. In this lecture we will see how to design distributions and sampling schemes that will allow us to solve problems we care about. In some instances, the goal will be to understand an existing random process, and in other instances, the problem we hope to solve has no intrinsic randomness, yet we will design a random process that will allow us to understand the problem.</p>
<h1 id="the-monte-carlo-methodsimulation">The Monte Carlo Method/Simulation</h1>
<p>The Monte Carlo Method is simply the approach of learning about a distribution, process, or system, via random sampling (i.e. via simulating the process). Especially today, this seems like a completely obvious idea. In part, this is because the Monte Carlo method is fundamentally intertwined with computers: we take fast simulation/computation for granted—without computers, the Monte Carlo method might not make much sense.</p>
<h2 id="extremely-brief-history">Extremely Brief History</h2>
<p>Historically, the rise of the Monte Carlo method closely paralleled the availability of computing resources.</p>
<ul>
<li><p>The first real example of the Monte Carlo method is usually attributed to the Compte de Buffon, who described what is now known as “Buffon’s needle&quot;. He proposed that <span class="math inline">\(\pi \approx 3.1415\ldots\)</span> could be approximated by tossing a large number of needles onto a floor that is covered in parallel floor panels. The probability that a randomly tossed needle (assuming a random location and random angle of rotation) lies entirely in a single floor panel can be computed as a simple expression involving <span class="math inline">\(\pi\)</span>, and the ratio of the needle length to the width of each floor panel. Hence by counting the ratio of needles that lie entirely within one floor panel (versus those that lie across two panels), Buffon suggested that <span class="math inline">\(\pi\)</span> can be accurately estimates. Indeed, using Chebyshev’s inequality from last lecture, and the bounds on the sum of independent <span class="math inline">\(0/1\)</span> random variables, given <span class="math inline">\(n\)</span> needles, the error in the ratio will be accurate to <span class="math inline">\(O(1/\sqrt{n}).\)</span></p></li>
<li><p>The first real appreciation for the potential of Monte Carlo simulations occurred in the context of the Manhattan Project during the 1940’s (and then later during the development of the hydrogen bomb). Many of the models of physics—particularly at the atomic level—are intrinsically probabilistic. Rather than trying to explicitly calculate the aggregate large-scale properties that result from the probabilistic behaviors of many particles, Ullam (soon joined by Von Neumann) realized that the newly available computers offered an alternative to trudging through the extremely difficult math: the Monte Carlo method. (The name “Monte Carlo Method” is attributed to Ullam, in reference to Ullam’s gambling-addict uncle….)</p></li>
</ul>
<h1 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h1>
<p>The Markov Chain Monte Carlo approach is simply the Monte Carlo approach applied to <em>Markov Processes</em>—namely, it is sampling from a distribution defined via a stochastic process known as a Markov Process.</p>
<p>A <em>Markov Process</em> has two parts: a set of <em>states</em> <span class="math inline">\(S=\{s_1,s_2,\ldots\}\)</span>, and a transition function <span class="math inline">\(P: S \times S \rightarrow [0,1],\)</span> where <span class="math inline">\(P(s_i,s_j)\)</span> is interpreted as the probability that the process transitions to state <span class="math inline">\(s_j\)</span> given that it is in state <span class="math inline">\(s_j\)</span>. Hence for any <span class="math inline">\(s_i\)</span>, <span class="math inline">\(\sum_j P(s_i,s_j)=1.\)</span></p>
<p>Given a Markov process defined by <span class="math inline">\(S\)</span> and <span class="math inline">\(P\)</span>, and an <em>initial state</em> <span class="math inline">\(X_0 \in S\)</span>, the corresponding stochastic process <span class="math inline">\(X_0,X_1,\ldots\)</span> corresponds to, for all <span class="math inline">\(i \ge 1\)</span>, selecting <span class="math inline">\(X_i\)</span> from the distribution defined by <span class="math inline">\(P(X_i,\cdot)\)</span>. The assumption that the distribution of <span class="math inline">\(X_i\)</span> depends only on <span class="math inline">\(X_{i-1}\)</span> is known as the <em>Markov Property</em>—namely that to predict the next step in the chain, <span class="math inline">\(X_i\)</span>, all the relevant history in the entire sequence <span class="math inline">\(X_0,\ldots,X_{i-1}\)</span> is encapsulated in <span class="math inline">\(X_{i-1}.\)</span> This is also sometimes referred to as the “memory-less property”.</p>
<p>Note that we can also think of the transition probabilities, <span class="math inline">\(P\)</span>, as a matrix, where there <span class="math inline">\(i,j\)</span>th entry is <span class="math inline">\(P(s_i,s_j).\)</span></p>
<p>To give a toy example, consider modeling the weather as a Markov process. (Most weather models are very complicated Markov processes, including the models that estimate the probability of El Nino winters.) Suppose the states <span class="math inline">\(S=\{sunny, cloudy, rainy\}\)</span>, and the transitions are defined by <span class="math display">\[P = \left( \begin{array}{ccc}
0.7 &amp; 0.2 &amp; 0.1 \\
0.6 &amp; 0.2 &amp; 0.2 \\
0.5 &amp; 0.2 &amp; 0.3 \end{array} \right),\]</span> where the top row represents <span class="math inline">\(P(sunny,sunny)=0.7, P(sunny,cloudy)=0.1, P(sunny,rainy)=0.1,\)</span> and the second row represents <span class="math inline">\(P(cloudy,sunny)=0.6, P(cloudy,cloudy)=0.2,\)</span> etc.</p>
<p>Given a Markov model, we can calculate things like <span class="math inline">\(\Pr[X_1 = rain | X_0 = sunny]\)</span>, or, more generally, calculate the distribution of the state after <span class="math inline">\(t\)</span> timesteps, <span class="math inline">\(X_t,\)</span> given that <span class="math inline">\(X_0 = s\)</span>.</p>
<h2 id="calculating-probabilities-in-markov-models">Calculating Probabilities in Markov Models</h2>
<p>Given a Markov Process, let <span class="math inline">\(D(t,s)\)</span> represent the distribution over values of <span class="math inline">\(X_t\)</span>, given that <span class="math inline">\(X_0 = s\)</span>. Note that we can represent <span class="math inline">\(D(t,s)\)</span> as a vector, whose <span class="math inline">\(i\)</span>th entry represents <span class="math inline">\(\Pr[X_t = s_i |X_0 = s].\)</span></p>
<p>There are two ways to estimate <span class="math inline">\(D(t,s):\)</span> direct computation, and via simulation (i.e. Markov Chain Monte Carlo).</p>
<p><strong>Direct Computation:</strong> One can always directly calculate <span class="math inline">\(D(t,s)\)</span> from the matrix of transitions, <span class="math inline">\(P\)</span>. To do this, note that <span class="math inline">\(D(0,s) = [0\ldots 0,1,0,\ldots],\)</span> where this is the indicator vector of state <span class="math inline">\(s\)</span>. For any <span class="math inline">\(t \ge 1,\)</span> <span class="math display">\[D(t,s) = D(t-1,s) \cdot P = D(0,s)\cdot P^t.\]</span></p>
<p>For example, revisiting the above example, we can compute <span class="math display">\[\Pr[X_{10} = rain | X_0 = sunny] = [1 \quad 0 \quad 0] \cdot P^{10} = [.65, .2, .15].\]</span></p>
<p>This calculation of <span class="math inline">\(P^t\)</span> involves <span class="math inline">\(O(\log t)\)</span> matrix multiplications, by the “repeated squaring” trick of first calculating <span class="math inline">\(P^2, P^4, P^8, P^{16}, \ldots,P^{2^{\lfloor \log t \rfloor}}\)</span> and then multiplying together the subset of these that make <span class="math inline">\(P^t\)</span>. If the number of states is small, and <span class="math inline">\(P\)</span> can be easily stored in memory, this calculation is rather efficient.</p>
<p><strong>Markov Chain Monte Carlo [MCMC]:</strong> The second approach to estimating <span class="math inline">\(D(t,s)\)</span> is via Monte Carlo simulation. Simply start with <span class="math inline">\(X_0 = s,\)</span> and for all <span class="math inline">\(i = 1,\ldots,t,\)</span> simulate the selection of <span class="math inline">\(X_i\)</span> according to the distribution defined by <span class="math inline">\(P\)</span> and <span class="math inline">\(X_{i-1}\)</span>. If we do this <span class="math inline">\(k\)</span> times, we have obtained <span class="math inline">\(k\)</span> samples drawn from the distribution <span class="math inline">\(D(t,s)\)</span>. This will take time <span class="math inline">\(k\cdot t\cdot updateT,\)</span> where <span class="math inline">\(updateT\)</span> is the time to sample from the distribution defined by a row of <span class="math inline">\(P\)</span>. For Markov chains that have a huge number of states, (and hence storing <span class="math inline">\(P\)</span> in memory is prohibitive), often <span class="math inline">\(updateT\)</span> is still relatively small, and hence Markov Chain Monte Carlo is a reasonable way to estimate <span class="math inline">\(D(t,s).\)</span></p>
<p>One example of a powerful and surprising application of MCMC is in the development of computer programs for playing “Go”. This game is played on a <span class="math inline">\(19\times19\)</span> board, and two players alternate placing black and white pebbles. One can place a pebble in any empty square, and the goal is to “capture” as many squares as possible, given a simple set of rules for defining what this means. Up until early 2016 when Google’s Go player beat the current human Go champion, Go was viewed as a notoriously difficult game for computers, partially because there are so many possible moves, and hence it is hard to do anything resembling a deep and reasonably complete search over the game tree. Given this, it becomes essential that a go player (or computerized player) be able to accurately estimate the quality of different potential moves.</p>
<p>A relatively recent breakthrough in computer go was the following surprising observation: to evaluate the quality of a move, one can simply consider the probability that a given player will win <em>if both players play the rest of the game RANDOMLY—by simply alternating random moves</em>. This sequence of random plays corresponds to a Markov process. The set of states is the set of board configurations, and the transition function simply captures the random plays: at each time, a player randomly selects an empty square and puts his/her pebble there. When all the squares are filled, the game ends, and the Markov process enters either the ‘win’ state or the ‘lose’ state.</p>
<p>It is obviously impossible to explicitly calculate this probability of winning/losing, since the number of states of the chain is at least <span class="math inline">\(2^{19^2}.\)</span> Nevertheless, one can estimate these probabilities via MCMC (by simply randomly playing out the game many times, and looking at the fraction of the times a given player wins in the simulation). Note that if we are trying to estimate this probability to <span class="math inline">\(\pm \eps,\)</span> it suffices to take <span class="math inline">\(\approx 1/\eps^2\)</span> random playouts (this follows from Chebyshev’s inequality, together with a bound on the variance of a sum of independent 0/1 random variables, since each of the random playouts is independent).</p>
<p>The apparent reality that this is a good way to evaluate go moves means something special about go. My interpretation is the following: this means that go positions are either good or bad, and that the quality of the position is independent of the skill of the player who is playing the game. A good position is good irrespective of whether it is two skilled players playing eachother, or two idiots (i.e. random players) playing eachother. Of course, a good player will beat a bad player, but the configurations that are desirable for a player does not depend on the player’s skill.</p>
<p>PAGERANK: The single idea that put google on the map, originally, was the idea of using a MCMC to evaluate the relevance of a given webpage. The Markov Chain crudely models a person surfing the web. The states of the Markov Chain is the set of all web pages. The transitions are defined as follows: with probability <span class="math inline">\(p =0.8,\)</span> transition from the current web page to a random outgoing link (i.e. click on a random link), and with probability <span class="math inline">\(0.2\)</span>, transition to a random web page (i.e. close the window and open a new web browser, and go to a random page). The claim is that for large <span class="math inline">\(t\)</span>, <span class="math inline">\(D(t,s)\)</span> is the distribution of web pages where probabilities correspond to the relevance of webpages. As we will see below, for large enough <span class="math inline">\(t\)</span>, <span class="math inline">\(D(t,s)\)</span> is essentially independent of the initial state, <span class="math inline">\(s\)</span>.</p>
<p>As with the MCMC for evaluating ‘go’ configurations, the above idea seems extremely simple/basic. This is the power of MCMC—one can often set up an extremely simple model of something, run MCMC, and end up with surprisingly accurate/informative predictions.</p>
<h1 id="the-stationary-distribution">The Stationary Distribution</h1>
<p>To what extent does <span class="math inline">\(D(t,s)\)</span> depend on the initial state, <span class="math inline">\(s\)</span>? The intuition that a random walk will eventually “forget” where it started, holds in many cases (for example, the Pagerank algorithm—after all how does one begin on the cs168 webpage, and end up on some bieber fan-site 45 minutes later?). The following theorem, known as the fundamental theorem of Markov chains, gives extremely simple conditions for this to hold:</p>
<p>Consider a Markov Chain that satisfies the following two conditions:</p>
<ol>
<li><p>For all pairs of states, <span class="math inline">\(s_i,s_j\)</span>, it is possible to eventually get to state <span class="math inline">\(s_j\)</span> if one starts in state <span class="math inline">\(s_i.\)</span></p></li>
<li><p>The chain is <em>aperiodic</em>: for a pair of states, <span class="math inline">\(s_i,s_j\)</span>, consider the set of times <span class="math inline">\(\{t_1,t_2,\ldots\}\)</span> consisting of all <span class="math inline">\(t \)</span> for which <span class="math inline">\(\Pr[X_t = s_j|X_0 = s_i] &gt; 0.\)</span> A chain is aperiodic if <span class="math inline">\(gcd(\{t_1,\ldots\}) = 1.\)</span> (Basically, as long as the chain is not a directed cycle, or a bipartite graph, etc.)</p></li>
</ol>
<p>Then for all states <span class="math inline">\(s\)</span>, <span class="math display">\[\lim_{t \rightarrow \infty}D(t,s) \rightarrow \pi,\]</span> where the distribution <span class="math inline">\(\pi\)</span> is called the <em>stationary distribution</em> of the chain, and is independent of <span class="math inline">\(t\)</span> and the initial state.</p>
<p>The motivation for calling <span class="math inline">\(\pi\)</span> the stationary distribution is that if <span class="math inline">\(\pi\)</span> is the limiting distribution for a chain defined by transition matrix <span class="math inline">\(P\)</span>, it satisfies <span class="math inline">\(\pi P = \pi\)</span>; hence it is stationary over time. Another immediate consequence of the fact that <span class="math inline">\(\pi P = \pi\)</span> is that the stationary distribution, <span class="math inline">\(\pi\)</span>, is is a (left) eigenvector of the transition matrix of the chain, <span class="math inline">\(P\)</span>, with eigenvalue 1.</p>
<p>Obviously the MCMC for evaluating Go moves does not satisfy these conditions—since there is no way of transitioning from a winning final state to a losing final state. The pagerank MCMC does satisfy this, because of the small probability of jumping to a random webpage. The weather example also satisfies this, since there is some probability of eventually having any weather, no matter the weather today.</p>
<h1 id="using-mcmc-to-solve-hard-problems">Using MCMC to Solve Hard Problems</h1>
<p>Up to now, we have been considering Markov Chains that model natural processes. We will now see how to use MCMCs to solve hard problems that do not inherently have any randomness. We will design our own probabilistic process to help us solve the problem, even though the problem does not necessarily have any inherent stochastic process.</p>
<p><strong>RECIPE FOR SUCCESS:</strong> Given a hard search problem over a HUGE space:</p>
<ol>
<li><p>Define a random walk/Markov Chain over the search space, such that the transition function makes the chain have a stationary distribution, <span class="math inline">\(\pi\)</span>, that places a higher probability on “good” solutions to our search problem.</p></li>
<li><p>Run MCMC.</p></li>
<li><p>Wait/hope that the MCMC finds a good solution.</p></li>
</ol>
<p>In the homework, we see a concrete realization of this recipe of success for the hard problem of finding a short traveling salesman tour of the national parks in the US.</p>
<p>Below we see another extremely cute concrete realization, due to Persi Diaconis (see the links on the course webpage for his much more thorough/eloquent description of this):</p>
<p><strong>Decrypting Substitution Ciphers:</strong> Suppose we are given a document that has been encrypted by substituting each letter for a strange symbol. We will apply the above recipe to reveal this mapping. The states of the markov chain will correspond to mappings between the symbols, and the letters. To define the transition function, we need some sense of how to score a given mapping is. Almost anything will work (e.g. defining the score to be the fraction of translated/decrypted words that appear in an english dictionary, or defining the score to be the probability of seeing the given sequence of decrypted characters, where this is calculated as the product of all consecutive PAIRS of characters, using the pair-wise probabilities based on real text—for example, if the current mapping gives a translation of ‘bakjb ska en…’, we compute the score as <span class="math inline">\(\Pr[ba]\cdot  \Pr[ak]\cdot\Pr[ki]\cdot\Pr[ib]\ldots,\)</span> where <span class="math inline">\(\Pr[ba]\)</span> is an estimate of the probability that <span class="math inline">\(a\)</span> follows <span class="math inline">\(b\)</span> in typical text). Given a score function, we can define transitions as follows: given a mapping, consider switching the image of 2 random symbols: compute the score of the new mapping and the old one: if the new mapping has a better score, keep it, otherwise, keep the new score with some (small) probability, proportional to how much worse the score is. This transition function ensures that the stationary distribution of the chain puts much higher weight on better mappings.</p>
<p>Note that we need to sometimes make ‘backwards progress’—i.e. keep a worse scoring mapping, in order to ensure that we satisfy the first condition of the Theorem; if we don’t do this, our chain might get stuck at a local optima. Amazingly, the above chain seems to decrypt this sort of cipher in a few thousand iterations.</p>
<h1 id="mixing-time">Mixing Time</h1>
<p>MCMC is magical and solves many problems very well. The main issue, which might be obvious at this point, is that it might not be clear how long we need to wait until we are close to the stationary distribution: ie how long do we need to run the MCMC before we think we are drawing a sample from something resembling the stationary distribution. Namely how large does <span class="math inline">\(t\)</span> need to be to ensure that <span class="math inline">\(D(t,s) \approx \pi\)</span>?</p>
<p>Mathematically, the “time until we are close to <span class="math inline">\(\pi\)</span>” is known as the <em>mixing time</em> of the chain, and is defined as the minimum time <span class="math inline">\(t\)</span> s.t. no matter where we start (i.e. for all states <span class="math inline">\(s\)</span>), the distance between <span class="math inline">\(D(t,s)\)</span> and <span class="math inline">\(\pi\)</span> is at most <span class="math inline">\(1/4\)</span>. Formally: <span class="math display">\[mixingTime = \min \{t: \max_s \|D(t,s) - \pi\|_{TV} \le 1/4 \}.\]</span> [Note: one sometimes sees this definition with the constant <span class="math inline">\(1/4\)</span> replaced by <span class="math inline">\(1/e\)</span>.]</p>
<h2 id="eigenvalues-and-power-iteration">Eigenvalues and Power Iteration</h2>
<p>It is often hard to estimate the mixing time of a given chain—particularly for those chains whose transition matrix is too large to write out completely. Nevertheless, it is both practically and conceptually useful to relate the mixing time of a Markov process to several concepts that we have already seen.</p>
<p>Consider the distributions of a Markov chain with transition matrix <span class="math inline">\(P\)</span> after <span class="math inline">\(0,1,2,\ldots\)</span> timesteps, given an initial state (or distribution) represented by the vector <span class="math inline">\(v\)</span>: namely, we have <span class="math inline">\(v, v P, v P^2, v P^3,\ldots.\)</span> This sequence looks exactly like the sequence of calculations that we performed in the context of the “Power Iteration Algorithm” for computing the maximum eigenvalue of a matrix that we saw in lecture 8. The only difference is that for this sequence, we are given an explicit starting vector, <span class="math inline">\(v\)</span>; in contrast, in the Power Iteration Algorithm, we selected a vector uniformly at random, and then repeatedly multiplied it by the matrix in question.</p>
<p>In the analysis of the power iteration algorithm, we saw that the ratio of the second largest eigenvalue of a matrix, <span class="math inline">\(\lambda_{max-1}\)</span> to the largest, <span class="math inline">\(\lambda_{max}\)</span>, determined the rate at which we could expect the vector to converge to the largest eigenvector. Similarly, we could hope that in most settings, the ratio between the largest eigenvalue of a transition matrix of a Markov chain (which will be 1), and the second largest eigenvalue, will roughly correspond to the rate at which <span class="math inline">\(v P^t\)</span> converges to <span class="math inline">\(\pi\)</span>, i.e. the mixing time. While we can’t argue that this correspondence exactly holds, because the initial distribution <span class="math inline">\(v\)</span> is not chosen randomly—in many special cases we can relate this ratio of eigenvalues to the mixing time. (You will explore this connection in this week’s mini-project….)</p>
<p>Maybe add an explicit example of random walk on undirected <span class="math inline">\(d\)</span>-regular graph, where one can explicitly bound mixing time via <span class="math inline">\(1/\lambda_{max-1}\)</span>.</p>

{% endraw %}
